{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ca0b7ce",
   "metadata": {},
   "source": [
    "\n",
    "# Career Agent – Integrated (Prod Informa + Dev Profiles) with Robust Streaming\n",
    "\n",
    "This notebook:\n",
    "- Implements **robust Bedrock streaming** (converse_stream + fallback to invoke_model_with_response_stream).\n",
    "- Adds a **Prod retriever** for `internal_curated_informa_vectorstore` (Informa internal knowledge).\n",
    "- Adds a **Dev retriever** (via `PG_DSN`) for the `internal_private_employee_profiles_vectorstore` (employee profiles).\n",
    "- Provides a **unified `run_workflow()`** that pulls from profile + existing tools (if defined) + Informa Prod snippets, then streams the answer.\n",
    "\n",
    "> You can keep using your previous helpers if you have them defined elsewhere (e.g., `job_tool`, `courses_tool`, `choose_candidates`, `synthesize_answer_llm`). The code checks for them and uses them when present.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8262e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Setup & Environment ---\n",
    "import os, json, time, urllib.parse, re\n",
    "from typing import Iterator, List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "AWS_BEDROCK_REGION = os.getenv(\"AWS_BEDROCK_REGION\") or os.getenv(\"AWS_REGION\") or \"us-east-1\"\n",
    "PRIMARY_MODEL_ID = os.getenv(\"PRIMARY_LLM_MODEL_NAME\", \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\")\n",
    "\n",
    "print(\"Using region:\", AWS_BEDROCK_REGION)\n",
    "print(\"Primary model:\", PRIMARY_MODEL_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979a1793",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Robust Bedrock Streaming Utilities ---\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "def _normalize_bedrock_model_id(model_id: str) -> str:\n",
    "    if not model_id:\n",
    "        return \"anthropic.claude-3-7-sonnet-20250219-v1:0\"\n",
    "    if model_id.startswith((\"us.\", \"eu.\")):\n",
    "        model_id = model_id.split(\".\", 1)[1]\n",
    "    return model_id\n",
    "\n",
    "def _bedrock_client():\n",
    "    region = os.getenv(\"AWS_BEDROCK_REGION\") or os.getenv(\"AWS_REGION\") or \"us-east-1\"\n",
    "    endpoint = os.getenv(\"AWS_BEDROCK_ENDPOINT\")  # optional\n",
    "    kwargs = {\"region_name\": region}\n",
    "    if endpoint:\n",
    "        kwargs[\"endpoint_url\"] = endpoint\n",
    "    return boto3.client(\"bedrock-runtime\", **kwargs)\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are Informa’s internal career advisor. \"\n",
    "    \"Write naturally and concisely, tailored to the employee’s background and the question. \"\n",
    "    \"Explain tradeoffs, propose bridge steps if the target domain differs from the profile. \"\n",
    "    \"Use only facts provided; do not invent links or data.\"\n",
    ")\n",
    "\n",
    "def _make_messages_body(user_text: str, intents: List[str], is_manager: bool, profile_fields: dict, sections: dict) -> dict:\n",
    "    payload = {\n",
    "        \"query\": user_text,\n",
    "        \"intents\": intents,\n",
    "        \"persona\": {\"is_manager\": bool(is_manager)},\n",
    "        \"profile\": {\n",
    "            \"name\": profile_fields.get(\"name\"),\n",
    "            \"title\": profile_fields.get(\"title\"),\n",
    "            \"skills\": profile_fields.get(\"skills\") or [],\n",
    "            \"topics\": profile_fields.get(\"topics\") or [],\n",
    "        },\n",
    "        \"retrieval\": {\n",
    "            \"jobs\":   [{\"title\": x.get(\"title\"), \"url\": x.get(\"url\")} for x in (sections.get(\"jobs\") or [])][:8],\n",
    "            \"courses\":[{\"title\": x.get(\"title\"), \"url\": x.get(\"url\")} for x in (sections.get(\"courses\") or [])][:8],\n",
    "            \"development_plan\":   [{\"title\": x.get(\"title\") or (x.get(\"metadata\") or {}).get(\"title\",\"\")} for x in (sections.get(\"development_plan\") or [])][:6],\n",
    "            \"manager_toolkit\":    [{\"title\": x.get(\"title\") or (x.get(\"metadata\") or {}).get(\"title\",\"\")} for x in (sections.get(\"manager_toolkit\")  or [])][:6],\n",
    "            \"leadership_strategy\":[{\"title\": x.get(\"title\") or (x.get(\"metadata\") or {}).get(\"title\",\"\")} for x in (sections.get(\"leadership_strategy\") or [])][:6],\n",
    "            \"informa_strategy\":   [{\"title\": x.get(\"title\"), \"snippet\": x.get(\"snippet\")} for x in (sections.get(\"informa_strategy\") or [])][:8],\n",
    "        }\n",
    "    }\n",
    "    return {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 700,\n",
    "        \"temperature\": 0.4,\n",
    "        \"system\": SYSTEM_PROMPT,\n",
    "        \"messages\": [{\n",
    "            \"role\":\"user\",\n",
    "            \"content\":[{\"type\":\"text\",\"text\":\n",
    "                \"Using only this JSON, answer naturally. \"\n",
    "                \"Pick items that best fit the query and profile; prefer intersection/bridge when needed. \"\n",
    "                \"If info is insufficient, ask for the minimum missing detail.\\n\\n\"\n",
    "                + json.dumps(payload, ensure_ascii=False)}]\n",
    "        }]\n",
    "    }\n",
    "\n",
    "def _anthropic_stream_fallback(br, model_id: str, body: dict) -> Iterator[str]:\n",
    "    \"\"\"Fallback using invoke_model_with_response_stream (Anthropic Messages format).\"\"\"\n",
    "    resp = br.invoke_model_with_response_stream(\n",
    "        modelId=model_id,\n",
    "        body=json.dumps(body),\n",
    "        accept=\"application/json\",\n",
    "        contentType=\"application/json\",\n",
    "    )\n",
    "    for evt in resp.get(\"body\"):\n",
    "        if \"chunk\" not in evt:\n",
    "            continue\n",
    "        try:\n",
    "            payload = json.loads(evt[\"chunk\"][\"bytes\"].decode(\"utf-8\"))\n",
    "        except Exception:\n",
    "            continue\n",
    "        et = payload.get(\"type\")\n",
    "        if et == \"content_block_delta\":\n",
    "            d = payload.get(\"delta\", {})\n",
    "            if d.get(\"type\") == \"text_delta\":\n",
    "                t = d.get(\"text\", \"\")\n",
    "                if t:\n",
    "                    yield t\n",
    "        elif et == \"message_stop\":\n",
    "            break\n",
    "\n",
    "def synthesize_answer_llm_stream(\n",
    "    user_text: str,\n",
    "    intents: List[str],\n",
    "    is_manager: bool,\n",
    "    profile_fields: dict,\n",
    "    sections: dict,\n",
    "    model_id: Optional[str] = None,\n",
    ") -> Iterator[str]:\n",
    "    \"\"\"Streams text deltas (converse_stream first; falls back to invoke_model_with_response_stream).\"\"\"\n",
    "    br = _bedrock_client()\n",
    "    model_id = _normalize_bedrock_model_id(model_id or os.getenv(\"PRIMARY_LLM_MODEL_NAME\", \"anthropic.claude-3-7-sonnet-20250219-v1:0\"))\n",
    "    body = _make_messages_body(user_text, intents, is_manager, profile_fields, sections)\n",
    "\n",
    "    # Fast path: Converse API (system passed in 'system' param, NOT as a message)\n",
    "    system_prompts = [{\"text\": body.get(\"system\", SYSTEM_PROMPT)}] if body.get(\"system\") else [{\"text\": SYSTEM_PROMPT}]\n",
    "    conv_msgs = []\n",
    "    for m in body.get(\"messages\", []):\n",
    "        role = m.get(\"role\", \"user\")\n",
    "        if role not in (\"user\", \"assistant\"):\n",
    "            role = \"user\"\n",
    "        text_parts = [c.get(\"text\", \"\") for c in (m.get(\"content\") or []) if c.get(\"type\") == \"text\"]\n",
    "        conv_msgs.append({\"role\": role, \"content\": [{\"text\": \"\".join(text_parts)}]})\n",
    "\n",
    "    inference = {\"temperature\": body.get(\"temperature\", 0.4), \"maxTokens\": body.get(\"max_tokens\", 700)}\n",
    "\n",
    "    try:\n",
    "        resp = br.converse_stream(\n",
    "            modelId=model_id,\n",
    "            system=system_prompts,\n",
    "            messages=conv_msgs if conv_msgs else [{\"role\":\"user\",\"content\":[{\"text\":\"Hello\"}]}],\n",
    "            inferenceConfig=inference,\n",
    "        )\n",
    "        stream = resp.get(\"stream\")\n",
    "        if not stream:\n",
    "            # Fallback immediately if no stream\n",
    "            for t in _anthropic_stream_fallback(br, model_id, body):\n",
    "                yield t\n",
    "            return\n",
    "        with stream as events:\n",
    "            for event in events:\n",
    "                if \"contentBlockDelta\" in event:\n",
    "                    delta = event[\"contentBlockDelta\"][\"delta\"].get(\"text\")\n",
    "                    if delta:\n",
    "                        yield delta\n",
    "                elif \"messageStop\" in event:\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        # Checksumming/proxy issues -> fallback\n",
    "        for t in _anthropic_stream_fallback(br, model_id, body):\n",
    "            yield t\n",
    "\n",
    "# Simple notebook renderer\n",
    "from IPython.display import display, Markdown\n",
    "def render_stream(generator, refresh: float = 0.05, min_early_flush_chars: int = 60) -> str:\n",
    "    buf, flushed, last = [], False, 0.0\n",
    "    handle = display(Markdown(\"\"), display_id=True)\n",
    "\n",
    "    def flush():\n",
    "        handle.update(Markdown(\"\".join(buf)))\n",
    "\n",
    "    for chunk in generator:\n",
    "        buf.append(chunk)\n",
    "        if not flushed and sum(len(x) for x in buf) >= min_early_flush_chars:\n",
    "            flush(); flushed = True; last = time.time(); continue\n",
    "        now = time.time()\n",
    "        if (now - last) >= refresh:\n",
    "            flush(); last = now\n",
    "    flush()\n",
    "    return \"\".join(buf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595b1862",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- PG Retrievers: Prod Informa (Informa knowledge) + Dev Profiles (employee) ---\n",
    "import numpy as np\n",
    "import psycopg\n",
    "from psycopg.rows import dict_row\n",
    "\n",
    "# ---------- Prod Informa vector store ----------\n",
    "_PROD_PG_DSN = (\n",
    "    f\"postgresql://v_svc_usr_aidb:{urllib.parse.quote('j<pW@qNsFIc!(OR', safe='')}\"\n",
    "    f\"@elysiadb.iris.informa.com:5432/aidb?sslmode=require\"\n",
    ")\n",
    "_INFORMA_PREFILTER_SQL = '''\n",
    "SELECT e.uuid AS id, e.embedding, e.document, e.cmetadata\n",
    "FROM ai.langchain_pg_embedding e\n",
    "JOIN ai.langchain_pg_collection c ON c.uuid = e.collection_id\n",
    "WHERE c.name = %(collection)s\n",
    "  AND (\n",
    "    e.document ILIKE '%%' || %(query)s || '%%'\n",
    "    OR CAST(e.cmetadata AS TEXT) ILIKE '%%' || %(query)s || '%%'\n",
    "  )\n",
    "LIMIT %(k)s;\n",
    "'''\n",
    "\n",
    "def _pg_prod_conn():\n",
    "    return psycopg.connect(_PROD_PG_DSN, row_factory=dict_row)\n",
    "\n",
    "def _cosine(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    d = (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    if d == 0:\n",
    "        return 0.0\n",
    "    return float(np.dot(a, b) / d)\n",
    "\n",
    "def retrieve_informa_snippets(query: str, collection: str = \"internal_curated_informa_vectorstore\",\n",
    "                              k: int = 8, pre_k: int = 48, max_chars: int = 1200) -> List[Dict[str, str]]:\n",
    "    if not query:\n",
    "        return []\n",
    "    with _pg_prod_conn() as conn, conn.cursor() as cur:\n",
    "        cur.execute(_INFORMA_PREFILTER_SQL, {\"collection\": collection, \"query\": query, \"k\": pre_k})\n",
    "        rows = cur.fetchall()\n",
    "\n",
    "    if not rows:\n",
    "        # fallback: condense a long query to improve recall\n",
    "        q2 = \"digital transformation\" if \"digital transformation\" in (query.lower()) else \"informa strategy\"\n",
    "        with _pg_prod_conn() as conn, conn.cursor() as cur:\n",
    "            cur.execute(_INFORMA_PREFILTER_SQL, {\"collection\": collection, \"query\": q2, \"k\": pre_k})\n",
    "            rows = cur.fetchall()\n",
    "        if not rows:\n",
    "            return []\n",
    "\n",
    "    embs, items = [], []\n",
    "    for r in rows:\n",
    "        emb = np.array(r.get(\"embedding\") or [], dtype=np.float32)\n",
    "        embs.append(emb)\n",
    "        items.append({\"document\": r.get(\"document\") or \"\", \"meta\": r.get(\"cmetadata\") or {}})\n",
    "    centroid = np.mean(np.stack(embs), axis=0) if embs else None\n",
    "    if centroid is not None:\n",
    "        scored = []\n",
    "        for it, emb in zip(items, embs):\n",
    "            it_score = _cosine(centroid, emb)\n",
    "            scored.append((it_score, it))\n",
    "        scored.sort(key=lambda x: x[0], reverse=True)\n",
    "        items = [it for _, it in scored]\n",
    "\n",
    "    out = []\n",
    "    for i, it in enumerate(items[:k]):\n",
    "        doc = it[\"document\"]\n",
    "        snippet = doc if len(doc) <= max_chars else (doc[:max_chars] + \"…\")\n",
    "        out.append({\"title\": f\"informa_ctx#{i+1}\", \"snippet\": snippet})\n",
    "    return out\n",
    "\n",
    "# ---------- Dev employee profiles (via PG_DSN) ----------\n",
    "PG_DSN = os.getenv(\"PG_DSN\", \"\")  # e.g., postgresql://.../aidb?sslmode=require\n",
    "_EMP_PREFILTER_SQL = '''\n",
    "SELECT e.uuid AS id, e.embedding, e.document, e.cmetadata\n",
    "FROM ai.langchain_pg_embedding e\n",
    "JOIN ai.langchain_pg_collection c ON c.uuid = e.collection_id\n",
    "WHERE c.name = %(collection)s\n",
    "  AND (\n",
    "    e.document ILIKE '%%' || %(query)s || '%%'\n",
    "    OR CAST(e.cmetadata AS TEXT) ILIKE '%%' || %(query)s || '%%'\n",
    "  )\n",
    "LIMIT %(k)s;\n",
    "'''\n",
    "\n",
    "def _pg_dev_conn():\n",
    "    if not PG_DSN:\n",
    "        raise RuntimeError(\"PG_DSN not set for Dev employee profile DB\")\n",
    "    return psycopg.connect(PG_DSN, row_factory=dict_row)\n",
    "\n",
    "def retrieve_profile_doc(email: Optional[str] = None, name: Optional[str] = None,\n",
    "                         collection: str = \"internal_private_employee_profiles_vectorstore\",\n",
    "                         k: int = 8, pre_k: int = 48) -> Optional[Dict[str, Any]]:\n",
    "    q = (email or name or \"\").strip()\n",
    "    if not q:\n",
    "        return None\n",
    "    with _pg_dev_conn() as conn, conn.cursor() as cur:\n",
    "        cur.execute(_EMP_PREFILTER_SQL, {\"collection\": collection, \"query\": q, \"k\": pre_k})\n",
    "        rows = cur.fetchall()\n",
    "    if not rows:\n",
    "        return None\n",
    "    # Score by centroid for a tiny bit of quality\n",
    "    embs, items = [], []\n",
    "    for r in rows:\n",
    "        emb = np.array(r.get(\"embedding\") or [], dtype=np.float32)\n",
    "        embs.append(emb)\n",
    "        items.append({\"document\": r.get(\"document\") or \"\", \"meta\": r.get(\"cmetadata\") or {}})\n",
    "    centroid = np.mean(np.stack(embs), axis=0) if embs else None\n",
    "    if centroid is not None:\n",
    "        scored = []\n",
    "        for it, emb in zip(items, embs):\n",
    "            scored.append((_cosine(centroid, emb), it))\n",
    "        scored.sort(key=lambda x: x[0], reverse=True)\n",
    "        return scored[0][1]\n",
    "    return items[0] if items else None\n",
    "\n",
    "def _safe_json_load(txt: str) -> Any:\n",
    "    try:\n",
    "        return json.loads(txt)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def extract_profile_fields_from_doc(doc: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    # Try cmetadata first\n",
    "    meta = doc.get(\"meta\") or {}\n",
    "    name = meta.get(\"name\") or meta.get(\"full_name\") or meta.get(\"employee_name\")\n",
    "    title = meta.get(\"title\") or meta.get(\"job_title\")\n",
    "    skills = meta.get(\"skills\") or []\n",
    "    topics = meta.get(\"topics\") or []\n",
    "    is_manager = bool(meta.get(\"is_manager\", False))\n",
    "\n",
    "    if not (name and title and skills):\n",
    "        # Try parsing document JSON (if it's JSON)\n",
    "        j = _safe_json_load(doc.get(\"document\", \"\"))\n",
    "        if isinstance(j, dict):\n",
    "            name = name or j.get(\"name\")\n",
    "            title = title or j.get(\"title\")\n",
    "            skills = skills or j.get(\"skills\") or []\n",
    "            topics = topics or j.get(\"topics\") or []\n",
    "\n",
    "    # Fallbacks\n",
    "    if isinstance(skills, str):\n",
    "        skills = [s.strip() for s in re.split(r\"[;,\\n]\", skills) if s.strip()]\n",
    "    if not isinstance(skills, list):\n",
    "        skills = []\n",
    "\n",
    "    return {\"name\": name, \"title\": title, \"skills\": skills, \"topics\": topics, \"is_manager\": is_manager}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5070eb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Unified run_workflow(): profile + existing KB/tools + Informa (Prod PG) ---\n",
    "from types import SimpleNamespace\n",
    "\n",
    "def run_workflow(\n",
    "    user_text: str,\n",
    "    email: Optional[str] = None,\n",
    "    name: Optional[str] = None,\n",
    "    division: Optional[str] = None,\n",
    "    override_is_manager: Optional[bool] = None,\n",
    "    stream: bool = False,\n",
    ") -> Dict[str, Any]:\n",
    "\n",
    "    # 1) Gate (optional user-defined)\n",
    "    try:\n",
    "        gate = prohibitor(user_text)  # your function if present\n",
    "    except NameError:\n",
    "        gate = {\"allowed\": True, \"intents\": []}\n",
    "    if not gate.get(\"allowed\", True):\n",
    "        return {\"blocked\": True, \"gate\": gate, \"answer\": \"out_of_scope\"}\n",
    "\n",
    "    # 2) Profile/state (prefer user's setup_state/get_profile_fields if present)\n",
    "    state = None\n",
    "    profile_fields: Dict[str, Any] = {}\n",
    "    try:\n",
    "        state, profile_meta = setup_state(\n",
    "            email=email, name=name, division=division,\n",
    "            override_is_manager=override_is_manager, user_text=user_text\n",
    "        )\n",
    "        try:\n",
    "            profile_fields = get_profile_fields(email=email, name=name, division=division)  # your helper if present\n",
    "        except NameError:\n",
    "            profile_fields = {}\n",
    "    except NameError:\n",
    "        # Fallback: retrieve from Dev PG vector store\n",
    "        doc = retrieve_profile_doc(email=email, name=name)\n",
    "        if doc:\n",
    "            profile_fields = extract_profile_fields_from_doc(doc)\n",
    "        state = SimpleNamespace(is_manager=bool(profile_fields.get(\"is_manager\", False)))\n",
    "\n",
    "    # Is manager override\n",
    "    if override_is_manager is not None:\n",
    "        state.is_manager = bool(override_is_manager)\n",
    "\n",
    "    profile_found = any([profile_fields.get(\"name\"), profile_fields.get(\"title\"), profile_fields.get(\"skills\")])\n",
    "\n",
    "    # 3) Intent detection\n",
    "    try:\n",
    "        intents = detect_intents(user_text)  # your function if present\n",
    "    except NameError:\n",
    "        t = (user_text or \"\").lower()\n",
    "        intents = []\n",
    "        if \"job\" in t: intents.append(\"job\")\n",
    "        if any(k in t for k in [\"course\", \"learn\", \"training\", \"upskill\"]): intents.append(\"courses\")\n",
    "        if \"manager\" in t: intents.append(\"manager_toolkit\")\n",
    "        intents = intents or [\"general\"]\n",
    "    try:\n",
    "        intents = intent_persona(gate.get(\"intents\", intents))  # optional post-processing\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    # 4) Core retrieval (use existing helpers when available)\n",
    "    sections: Dict[str, List[Dict[str, Any]]] = {\n",
    "        \"jobs\": [], \"courses\": [], \"development_plan\": [],\n",
    "        \"manager_toolkit\": [], \"leadership_strategy\": [], \"informa_strategy\": []\n",
    "    }\n",
    "\n",
    "    # If user has a single helper to fill sections, use it\n",
    "    try:\n",
    "        sections.update(retrieve_sections(intents=intents, profile_fields=profile_fields))\n",
    "    except NameError:\n",
    "        # Otherwise call tools individually if they exist\n",
    "        try:\n",
    "            if \"job\" in intents:\n",
    "                sections[\"jobs\"] = job_reflexion(\n",
    "                    job_tool(user_text, profile_q=[], profile_fields=profile_fields)\n",
    "                )\n",
    "        except NameError:\n",
    "            pass\n",
    "        try:\n",
    "            if \"courses\" in intents:\n",
    "                sections[\"courses\"] = courses_reflexion(\n",
    "                    courses_tool(user_text, state, profile_q=[], profile_fields=profile_fields),\n",
    "                    state.is_manager\n",
    "                )\n",
    "        except NameError:\n",
    "            pass\n",
    "\n",
    "        # Optional candidate selection\n",
    "        try:\n",
    "            if \"job\" in intents:\n",
    "                sections[\"jobs\"] = choose_candidates(\n",
    "                    user_text, sections.get(\"jobs\"), profile_fields, target=\"jobs\", top_n=6\n",
    "                )\n",
    "        except NameError:\n",
    "            pass\n",
    "        try:\n",
    "            if \"courses\" in intents:\n",
    "                sections[\"courses\"] = choose_candidates(\n",
    "                    user_text, sections.get(\"courses\"), profile_fields, target=\"courses\", top_n=6\n",
    "                )\n",
    "        except NameError:\n",
    "            pass\n",
    "\n",
    "    # 5) Informa strategy snippets from Prod\n",
    "    try:\n",
    "        info_snips = retrieve_informa_snippets(user_text, \"internal_curated_informa_vectorstore\", k=8)\n",
    "        sections[\"informa_strategy\"] = info_snips\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Informa PG retrieval failed: {e}\")\n",
    "\n",
    "    # 6) Stream vs non-stream\n",
    "    if stream:\n",
    "        stream_fn = globals().get(\"synthesize_answer_llm_stream\")\n",
    "        if not callable(stream_fn):\n",
    "            raise NameError(\"synthesize_answer_llm_stream is not defined. Run the streaming cell first.\")\n",
    "        gen = stream_fn(\n",
    "            user_text=user_text,\n",
    "            intents=intents,\n",
    "            is_manager=state.is_manager,\n",
    "            profile_fields=profile_fields or {},\n",
    "            sections=sections,\n",
    "            model_id=os.getenv(\"PRIMARY_LLM_MODEL_NAME\", \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\"),\n",
    "        )\n",
    "        return {\n",
    "            \"stream\": gen,\n",
    "            \"blocked\": False,\n",
    "            \"gate\": {\"intents\": intents},\n",
    "            \"state\": state,\n",
    "            \"profile_found\": profile_found,\n",
    "            \"profile_fields\": profile_fields,\n",
    "            \"sections\": sections,\n",
    "        }\n",
    "\n",
    "    # Non-stream fallback\n",
    "    try:\n",
    "        text_answer = synthesize_answer_llm(  # user's non-stream function if present\n",
    "            user_text=user_text,\n",
    "            intents=intents,\n",
    "            is_manager=state.is_manager,\n",
    "            profile_fields=profile_fields,\n",
    "            sections=sections,\n",
    "        )\n",
    "    except NameError:\n",
    "        gen = synthesize_answer_llm_stream(\n",
    "            user_text=user_text,\n",
    "            intents=intents,\n",
    "            is_manager=state.is_manager,\n",
    "            profile_fields=profile_fields,\n",
    "            sections=sections,\n",
    "            model_id=os.getenv(\"PRIMARY_LLM_MODEL_NAME\", \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\"),\n",
    "        )\n",
    "        text_answer = \"\".join(list(gen))\n",
    "\n",
    "    return {\n",
    "        \"answer\": text_answer,\n",
    "        \"blocked\": False,\n",
    "        \"gate\": {\"intents\": intents},\n",
    "        \"state\": state,\n",
    "        \"profile_found\": profile_found,\n",
    "        \"profile_fields\": profile_fields,\n",
    "        \"sections\": sections,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be577734",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Example usage (run after setting AWS creds and PG_DSN in your env) ---\n",
    "# Sanity check: fetch a couple of Informa snippets from Prod\n",
    "try:\n",
    "    print(\"Sample Informa snippets:\", [s[\"snippet\"][:160] for s in retrieve_informa_snippets(\"digital transformation\", k=2)])\n",
    "except Exception as e:\n",
    "    print(\"Informa snippet test error:\", e)\n",
    "\n",
    "# End-to-end (streaming)\n",
    "query = \"Analyze my current skillset against Informa's digital transformation needs and recommend 5 specific learning opportunities to close these gaps.\"\n",
    "try:\n",
    "    out = run_workflow(query, email=os.getenv(\"DEFAULT_USER_EMAIL\"), stream=True)\n",
    "    rendered = render_stream(out[\"stream\"])\n",
    "except Exception as e:\n",
    "    print(\"Workflow test error:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
