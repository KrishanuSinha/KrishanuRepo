{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e552444b",
   "metadata": {},
   "source": [
    "# HR Career Advisor — PG Vector + AWS KB (Prototype **v3**)\n",
    "\n",
    "**What’s new in v3**\n",
    "- Email-first profile lookup (name/division optional)\n",
    "- Parse *Skills* and *Topics of Interest* from profile\n",
    "- Profile-driven queries + ranking boost\n",
    "- Type guessing for PG rows without `metadata.type`\n",
    "- Same dopamine onboarding + fallbacks\n",
    "\n",
    "**Flow:** prohibitor → setup_state → intent_persona → tools (PG+KB + profile-driven) → reflexion → consolidation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e637b9",
   "metadata": {},
   "source": [
    "## 0) Setup & Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78e34651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env present: {'PG': True, 'JOB_KB': True, 'COURSES_KB': True, 'DEFAULT_USER_NAME': 'Mary Ralicki', 'DEFAULT_USER_EMAIL': 'mary.ralicki@informa.com'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, json, re\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from IPython.display import display, Markdown\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "AWS_REGION = os.getenv(\"AWS_REGION\", \"us-west-2\")\n",
    "AWS_MODEL_ID = \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\"\n",
    "\n",
    "PG_DSN = os.getenv(\"PG_DSN\",\"\")  # postgresql://user:pass@host:5432/dbname?sslmode=require\n",
    "PG_COLLECTIONS = [\n",
    "    \"internal_private_employee_profiles_vectorstore\",\n",
    "    \"internal_curated_informa_vectorstore\",\n",
    "]\n",
    "JOB_KB_ID = os.getenv(\"JOB_KB_ID\",\"\")\n",
    "COURSES_KB_ID = os.getenv(\"COURSES_KB_ID\",\"\")\n",
    "\n",
    "DEFAULT_USER_NAME  = os.getenv(\"DEFAULT_USER_NAME\",  \"Mary Ralicki\")\n",
    "DEFAULT_USER_EMAIL = os.getenv(\"DEFAULT_USER_EMAIL\", \"mary.ralicki@informa.com\")\n",
    "DEFAULT_USER_DIV   = os.getenv(\"DEFAULT_USER_DIVISION\", \"\")\n",
    "\n",
    "print(\"Env present:\", dict(\n",
    "    PG=bool(PG_DSN),\n",
    "    JOB_KB=bool(JOB_KB_ID),\n",
    "    COURSES_KB=bool(COURSES_KB_ID),\n",
    "    DEFAULT_USER_NAME=DEFAULT_USER_NAME,\n",
    "    DEFAULT_USER_EMAIL=DEFAULT_USER_EMAIL\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec50f429",
   "metadata": {},
   "source": [
    "## 1) PGVector Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca0baa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import psycopg\n",
    "\n",
    "def get_pg_conn():\n",
    "    if not PG_DSN:\n",
    "        raise RuntimeError(\"PG_DSN not set\")\n",
    "    return psycopg.connect(PG_DSN)\n",
    "\n",
    "KEYWORD_PREFILTER_SQL = (\n",
    "\"SELECT e.uuid AS id, e.embedding, e.document, e.cmetadata, c.name as collection \"\n",
    "\"FROM ai.langchain_pg_embedding e \"\n",
    "\"JOIN ai.langchain_pg_collection c ON c.uuid = e.collection_id \"\n",
    "\"WHERE c.name = %(collection)s \"\n",
    "\"  AND (e.document ILIKE '%%' || %(query)s || '%%' \"\n",
    "\"       OR CAST(e.cmetadata AS TEXT) ILIKE '%%' || %(query)s || '%%') \"\n",
    "\"LIMIT %(k)s;\"\n",
    ")\n",
    "\n",
    "def _to_meta(meta):\n",
    "    if isinstance(meta,(dict,list)): return meta\n",
    "    try: return json.loads(meta)\n",
    "    except: return {\"raw\": str(meta)}\n",
    "\n",
    "def _cosine(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    denom = (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    if denom == 0: return 0.0\n",
    "    return float(np.dot(a, b) / denom)\n",
    "\n",
    "def pg_search_hybrid(collection: str, query: str, pre_k: int = 24, top_k: int = 8) -> List[Dict[str,Any]]:\n",
    "    with get_pg_conn() as conn, conn.cursor() as cur:\n",
    "        cur.execute(KEYWORD_PREFILTER_SQL, {\"collection\": collection, \"query\": query, \"k\": pre_k})\n",
    "        rows = cur.fetchall()\n",
    "    if not rows: return []\n",
    "    embs, items = [], []\n",
    "    for _id, emb, doc, meta, coll in rows:\n",
    "        v = np.array(emb, dtype=np.float32)\n",
    "        embs.append(v)\n",
    "        items.append({\"id\": _id, \"embedding\": emb, \"document\": doc, \"metadata\": _to_meta(meta), \"collection\": coll})\n",
    "    centroid = np.mean(embs, axis=0)\n",
    "    for it in items:\n",
    "        it[\"score\"] = _cosine(centroid, np.array(it[\"embedding\"], dtype=np.float32))\n",
    "    items.sort(key=lambda x: x.get(\"score\",0.0), reverse=True)\n",
    "    return items[:top_k]\n",
    "\n",
    "def pg_multi_search(query: str, collections: List[str]) -> List[Dict[str,Any]]:\n",
    "    hits = []\n",
    "    for coll in collections:\n",
    "        try:\n",
    "            hits.extend(pg_search_hybrid(coll, query, 24, 8))\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ PG search failed for {coll}: {e}\")\n",
    "    hits.sort(key=lambda x: x.get(\"score\",0.0), reverse=True)\n",
    "    return hits[: max(6, len(collections)) ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb26eb4",
   "metadata": {},
   "source": [
    "## 2) AWS Knowledge Bases Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bfef6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import boto3\n",
    "try:\n",
    "    kb_rt = boto3.client(\"bedrock-agent-runtime\", region_name=AWS_REGION) if (JOB_KB_ID or COURSES_KB_ID) else None\n",
    "except Exception as e:\n",
    "    kb_rt = None\n",
    "    print(\"⚠️ AWS KB unavailable:\", e)\n",
    "\n",
    "def kb_retrieve(kb_id: str, query: str, top_k: int = 5) -> List[Dict[str,Any]]:\n",
    "    if not kb_rt or not kb_id:\n",
    "        return []\n",
    "    try:\n",
    "        resp = kb_rt.retrieve(\n",
    "            knowledgeBaseId=kb_id,\n",
    "            retrievalConfiguration={\"vectorSearchConfiguration\": {\"numberOfResults\": top_k}},\n",
    "            retrievalQuery={\"text\": query},\n",
    "        )\n",
    "        out = []\n",
    "        for r in resp.get(\"retrievalResults\", []):\n",
    "            c = r.get(\"content\", {})\n",
    "            out.append({\n",
    "                \"title\": c.get(\"title\") or (c.get(\"text\",\"\").split(\"\\n\")[0][:80]).strip(),\n",
    "                \"snippet\": c.get(\"snippetText\") or c.get(\"text\",\"\")[:240],\n",
    "                \"score\": r.get(\"score\"),\n",
    "                \"kb_id\": kb_id,\n",
    "                \"metadata\": r.get(\"metadata\") or {},\n",
    "                \"source\": r.get(\"location\", {}).get(\"s3Location\", {}).get(\"uri\"),\n",
    "                \"type\": r.get(\"metadata\",{}).get(\"type\")\n",
    "            })\n",
    "        return out\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ KB retrieve failed:\", e)\n",
    "        return []\n",
    "\n",
    "def kb_search_all(query: str) -> Dict[str, List[Dict[str,Any]]]:\n",
    "    return {\n",
    "        \"jobs\":    kb_retrieve(JOB_KB_ID, query, 6) if JOB_KB_ID else [],\n",
    "        \"courses\": kb_retrieve(COURSES_KB_ID, query, 6) if COURSES_KB_ID else [],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ef4d68",
   "metadata": {},
   "source": [
    "## 3) Prohibitor, State, Intent, Profile Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9512bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "AllowedIntents = {\"courses\",\"job\",\"development_plan\",\"manager_toolkit\",\"leadership_strategy\",\"career\"}\n",
    "\n",
    "def prohibitor(user_text: str) -> Dict[str,Any]:\n",
    "    t = user_text.lower()\n",
    "    allowed = any(k in t for k in [\"career\",\"course\",\"job\",\"role\",\"roles\",\"learn\",\"upskill\",\"development\",\"manager\",\"leadership\",\"okr\",\"coaching\",\"promotion\",\"ladder\",\"mentoring\",\"objective\",\"okrs\"])\n",
    "    intents = []\n",
    "    if any(k in t for k in [\"job\",\"jobs\",\"opening\",\"openings\",\"role\",\"roles\"]): intents.append(\"job\")\n",
    "    if any(k in t for k in [\"course\",\"courses\",\"learn\",\"training\",\"upskill\"]): intents.append(\"courses\")\n",
    "    if any(k in t for k in [\"mentoring\",\"mentor\"]): intents.append(\"manager_toolkit\")\n",
    "    if any(k in t for k in [\"objective\",\"okr\",\"okrs\"]): intents.append(\"leadership_strategy\")\n",
    "    if any(k in t for k in [\"development plan\",\"30-day\",\"60-day\",\"90-day\",\"dev plan\"]): intents.append(\"development_plan\")\n",
    "    if not intents and allowed: intents.append(\"career\")\n",
    "    return {\"allowed\": allowed and bool(intents), \"intents\": intents or [], \"rationale\": \"heuristic v0.3\"}\n",
    "\n",
    "@dataclass\n",
    "class AgentState:\n",
    "    email: Optional[str] = None\n",
    "    name: Optional[str] = None\n",
    "    division: Optional[str] = None\n",
    "    employee_id: Optional[str] = None\n",
    "    is_manager: bool = False\n",
    "    prompt: Optional[str] = None\n",
    "    quick_profile: Optional[Dict[str,Any]] = None\n",
    "\n",
    "def derive_is_manager_from_profile(meta: dict) -> bool:\n",
    "    if str(meta.get(\"is_manager\",\"\")).lower() in {\"true\",\"1\",\"yes\"}: return True\n",
    "    if int(meta.get(\"direct_reports\",0) or 0) > 0: return True\n",
    "    title = (meta.get(\"title\") or \"\").lower()\n",
    "    if any(k in title for k in [\" manager\",\"lead\",\"head of\",\"director\",\"vp\"]): return True\n",
    "    return False\n",
    "\n",
    "def profile_lookup(email: Optional[str] = None,\n",
    "                   name: Optional[str] = None,\n",
    "                   division: Optional[str] = None) -> List[Dict[str,Any]]:\n",
    "    if not PG_DSN:\n",
    "        return []\n",
    "    results: List[Dict[str,Any]] = []\n",
    "    with get_pg_conn() as conn, conn.cursor() as cur:\n",
    "        if email:\n",
    "            sql = \"\"\"\n",
    "                SELECT e.document, e.cmetadata\n",
    "                FROM ai.langchain_pg_embedding e\n",
    "                JOIN ai.langchain_pg_collection c ON c.uuid = e.collection_id\n",
    "                WHERE c.name = 'internal_private_employee_profiles_vectorstore'\n",
    "                  AND (e.cmetadata->>'email') = %(email)s\n",
    "                LIMIT 10;\n",
    "            \"\"\"\n",
    "            cur.execute(sql, {\"email\": email})\n",
    "            rows = cur.fetchall()\n",
    "            for doc, meta in rows:\n",
    "                try: meta = meta if isinstance(meta, dict) else json.loads(meta)\n",
    "                except: meta = {\"raw\": str(meta)}\n",
    "                results.append({\"document\": doc, \"metadata\": meta})\n",
    "            if results:\n",
    "                return results\n",
    "\n",
    "        if name:\n",
    "            sql = \"\"\"\n",
    "                SELECT e.document, e.cmetadata\n",
    "                FROM ai.langchain_pg_embedding e\n",
    "                JOIN ai.langchain_pg_collection c ON c.uuid = e.collection_id\n",
    "                WHERE c.name = 'internal_private_employee_profiles_vectorstore'\n",
    "                  AND (e.cmetadata->>'name') ILIKE %(name)s\n",
    "            \"\"\"\n",
    "            params = {\"name\": f\"%{name}%\"}\n",
    "            if division:\n",
    "                sql += \" AND (e.cmetadata->>'division') ILIKE %(division)s\"\n",
    "                params[\"division\"] = f\"%{division}%\"\n",
    "            sql += \" LIMIT 25;\"\n",
    "            cur.execute(sql, params)\n",
    "            rows = cur.fetchall()\n",
    "            for doc, meta in rows:\n",
    "                try: meta = meta if isinstance(meta, dict) else json.loads(meta)\n",
    "                except: meta = {\"raw\": str(meta)}\n",
    "                results.append({\"document\": doc, \"metadata\": meta})\n",
    "        return results\n",
    "\n",
    "def setup_state(email: Optional[str], name: Optional[str], division: Optional[str],\n",
    "                override_is_manager: Optional[bool], user_text: str) -> Tuple[AgentState, dict]:\n",
    "    rows = profile_lookup(email=email or DEFAULT_USER_EMAIL,\n",
    "                          name=name or DEFAULT_USER_NAME,\n",
    "                          division=division or DEFAULT_USER_DIV)\n",
    "    meta = rows[0][\"metadata\"] if rows else {}\n",
    "    is_mgr = override_is_manager if override_is_manager is not None else derive_is_manager_from_profile(meta)\n",
    "    st = AgentState(email=email or DEFAULT_USER_EMAIL, name=name or DEFAULT_USER_NAME,\n",
    "                    division=division or DEFAULT_USER_DIV, employee_id=meta.get(\"employee_id\"),\n",
    "                    is_manager=is_mgr, prompt=user_text)\n",
    "    st.quick_profile = {\"_doc\": rows[0][\"document\"] if rows else \"\"}\n",
    "    return st, meta\n",
    "\n",
    "def intent_persona(intents: List[str]) -> List[str]:\n",
    "    return sorted(set(i for i in intents if i in AllowedIntents))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d368de6a",
   "metadata": {},
   "source": [
    "## 4) Parse Skills/Topics and Build Profile Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ca37b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'jobs': ['data engineering roles', 'data engineering jobs', 'Python engineer jobs', 'Data Engineer career paths'], 'courses': ['Python course', 'Python training', 'data engineering learning path']}\n"
     ]
    }
   ],
   "source": [
    "# HOTFIX: (re)define profile parsers + query builder in one place\n",
    "\n",
    "import re, json\n",
    "\n",
    "def extract_profile_fields(document: str, meta: dict) -> dict:\n",
    "    text = (document or \"\") + \"\\n\" + json.dumps(meta or {})\n",
    "    m_sk = re.search(r\"(?im)^\\s*-\\s*Skills:\\s*(.+)$\", text)\n",
    "    skills = [s.strip() for s in re.split(r\"[;,]\", m_sk.group(1)) if s.strip()] if m_sk else []\n",
    "\n",
    "    m_to = re.search(r\"(?im)^\\s*-\\s*Topics of Interest:\\s*(.+)$\", text)\n",
    "    topics = [s.strip() for s in re.split(r\"[;,]\", m_to.group(1)) if s.strip()] if m_to else []\n",
    "\n",
    "    title = (meta or {}).get(\"title\") or \"\"\n",
    "    if not title:\n",
    "        m_t = re.search(r\"(?im)^\\s*-\\s*Job Title:\\s*(.+)$\", text)\n",
    "        if m_t: title = m_t.group(1).strip()\n",
    "\n",
    "    name = (meta or {}).get(\"name\") or \"\"\n",
    "    if not name:\n",
    "        m_n = re.search(r\"(?im)^\\s*-\\s*Name:\\s*(.+)$\", text)\n",
    "        if m_n: name = m_n.group(1).strip()\n",
    "\n",
    "    return {\"name\": name, \"title\": title, \"skills\": skills, \"topics\": topics}\n",
    "\n",
    "def build_profile_queries(fields: dict, max_items: int = 5) -> dict:\n",
    "    skills = (fields.get(\"skills\") or [])[:max_items]\n",
    "    topics = (fields.get(\"topics\") or [])[:max_items]\n",
    "    role   = (fields.get(\"title\") or \"\")\n",
    "\n",
    "    job_q, crs_q = [], []\n",
    "\n",
    "    # Jobs queries from topics/skills/role\n",
    "    for t in topics:\n",
    "        job_q += [f\"{t} roles\", f\"{t} jobs\"]\n",
    "    for s in skills:\n",
    "        job_q.append(f\"{s} engineer jobs\")\n",
    "    if role:\n",
    "        job_q.append(f\"{role} career paths\")\n",
    "\n",
    "    # Courses queries from skills/topics\n",
    "    for s in skills:\n",
    "        crs_q += [f\"{s} course\", f\"{s} training\"]\n",
    "    for t in topics:\n",
    "        crs_q.append(f\"{t} learning path\")  # correct append(...)\n",
    "\n",
    "    def dedup(seq):\n",
    "        seen, out = set(), []\n",
    "        for x in seq:\n",
    "            xl = x.lower()\n",
    "            if xl in seen: \n",
    "                continue\n",
    "            seen.add(xl); out.append(x)\n",
    "        return out\n",
    "\n",
    "    return {\"jobs\": dedup(job_q)[:max_items], \"courses\": dedup(crs_q)[:max_items]}\n",
    "\n",
    "# quick sanity check\n",
    "print(build_profile_queries({\"skills\":[\"Python\"], \"topics\":[\"data engineering\"], \"title\":\"Data Engineer\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb1656b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_profile_queries(fields: dict, max_items: int = 5) -> dict:\n",
    "    skills = (fields.get(\"skills\") or [])[:max_items]\n",
    "    topics = (fields.get(\"topics\") or [])[:max_items]\n",
    "    role   = (fields.get(\"title\") or \"\")\n",
    "\n",
    "    job_q, crs_q = [], []\n",
    "\n",
    "    # Jobs queries from topics/skills/role\n",
    "    for t in topics:\n",
    "        job_q += [f\"{t} roles\", f\"{t} jobs\"]\n",
    "    for s in skills:\n",
    "        job_q.append(f\"{s} engineer jobs\")\n",
    "    if role:\n",
    "        job_q.append(f\"{role} career paths\")\n",
    "\n",
    "    # Courses queries from skills/topics\n",
    "    for s in skills:\n",
    "        crs_q += [f\"{s} course\", f\"{s} training\"]\n",
    "    for t in topics:\n",
    "        crs_q.append(f\"{t} learning path\")  # <-- fixed: append(...), not append[...]\n",
    "\n",
    "    def dedup(seq):\n",
    "        seen = set(); out = []\n",
    "        for x in seq:\n",
    "            xl = x.lower()\n",
    "            if xl in seen: \n",
    "                continue\n",
    "            seen.add(xl); out.append(x)\n",
    "        return out\n",
    "\n",
    "    return {\"jobs\": dedup(job_q)[:max_items], \"courses\": dedup(crs_q)[:max_items]}\n",
    "\n",
    "def build_profile_queries(fields: dict, max_items: int = 5) -> dict:\n",
    "    skills = fields.get(\"skills\", [])[:max_items]\n",
    "    topics = fields.get(\"topics\", [])[:max_items]\n",
    "    role   = fields.get(\"title\") or \"\"\n",
    "\n",
    "    job_q, crs_q = [], []\n",
    "\n",
    "    for t in topics:\n",
    "        job_q += [f\"{t} roles\", f\"{t} jobs\"]\n",
    "    for s in skills:\n",
    "        job_q.append(f\"{s} engineer jobs\")\n",
    "    if role:\n",
    "        job_q.append(f\"{role} career paths\")\n",
    "\n",
    "    for s in skills:\n",
    "        crs_q += [f\"{s} course\", f\"{s} training\"]\n",
    "    for t in topics:\n",
    "        crs_q.append[f\"{t} learning path\"]\n",
    "\n",
    "    def dedup(seq):\n",
    "        seen=set(); out=[]\n",
    "        for x in seq:\n",
    "            xl=x.lower()\n",
    "            if xl in seen: continue\n",
    "            seen.add(xl); out.append(x)\n",
    "        return out\n",
    "\n",
    "    return {\"jobs\": dedup(job_q)[:max_items], \"courses\": dedup(crs_q)[:max_items]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834f718b",
   "metadata": {},
   "source": [
    "## 5) Tools (PG + KB) with Type Guessing, Profile Boost & Fallbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79df5756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'jobs': ['data engineering roles', 'data engineering jobs', 'Python engineer jobs', 'Data Engineer career paths'], 'courses': ['Python course', 'Python training', 'data engineering learning path']}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "COURSE_HINTS = [\n",
    "    \"course\",\"training\",\"learning path\",\"module\",\"curriculum\",\n",
    "    \"cert\",\"certification\",\"udemy\",\"coursera\",\"pluralsight\",\"lynda\",\n",
    "    \"academy\",\"lesson\",\"workshop\"\n",
    "]\n",
    "JOB_HINTS = [\n",
    "    \"job\",\"role\",\"opening\",\"position\",\"vacancy\",\"requisition\",\"req id\",\n",
    "    \"hiring\"\n",
    "]\n",
    "\n",
    "def guess_type(item: dict) -> str:\n",
    "    meta = (item.get(\"metadata\") or {})\n",
    "    t = str(meta.get(\"type\") or \"\").lower().strip()\n",
    "    if t:\n",
    "        return t\n",
    "    title = (item.get(\"title\") or \"\").lower()\n",
    "    doc = (item.get(\"document\") or \"\").lower()\n",
    "    text = f\"{title} {doc}\"\n",
    "    if any(h in text for h in COURSE_HINTS): return \"course\"\n",
    "    if any(h in text for h in JOB_HINTS): return \"job\"\n",
    "    coll = (item.get(\"collection\") or \"\").lower()\n",
    "    if \"course\" in coll: return \"course\"\n",
    "    if \"job\" in coll or \"role\" in coll: return \"job\"\n",
    "    return \"unknown\"\n",
    "\n",
    "def tool_pg_search(query: str, k: int = 8) -> List[Dict[str,Any]]:\n",
    "    return pg_multi_search(query, PG_COLLECTIONS)[:k]\n",
    "\n",
    "def tool_kb_search(query: str, top_k: int = 6) -> Dict[str, List[Dict[str,Any]]]:\n",
    "    return kb_search_all(query)\n",
    "\n",
    "MANAGER_KEYWORDS = {\"manager\",\"leadership\",\"org design\",\"hiring\",\"coaching\",\"performance review\",\"okr\",\"okrs\",\"succession\"}\n",
    "\n",
    "def looks_manager_only(item: Dict[str,Any]) -> bool:\n",
    "    meta = (item.get(\"metadata\") or {})\n",
    "    audience = str(meta.get(\"audience\",\"\")).lower()\n",
    "    title = (item.get(\"title\") or item.get(\"document\") or \"\").lower()\n",
    "    tags = \" \".join(meta.get(\"tags\", [])).lower()\n",
    "    if audience in {\"manager\",\"leadership\"}: return True\n",
    "    haystack = f\"{title} {tags}\"\n",
    "    return any(kw in haystack for kw in MANAGER_KEYWORDS)\n",
    "\n",
    "def explicit_manager_request(prompt: str) -> bool:\n",
    "    p = (prompt or \"\").lower()\n",
    "    return any(k in p for k in MANAGER_KEYWORDS)\n",
    "\n",
    "FALLBACKS = {\n",
    "    \"data engineering\": {\n",
    "        \"jobs\": [\n",
    "            {\"title\": \"Data Engineer (Platform)\"},\n",
    "            {\"title\": \"Analytics Engineer\"},\n",
    "            {\"title\": \"Data Engineer — ETL & Pipelines\"},\n",
    "        ],\n",
    "        \"courses\": [\n",
    "            {\"title\": \"Data Engineering on AWS — Foundations\"},\n",
    "            {\"title\": \"Modern Data Pipelines with Python & Airflow\"},\n",
    "            {\"title\": \"Designing Data-Intensive Applications — Hands-on\"},\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "def infer_topic(user_text: str) -> Optional[str]:\n",
    "    t = user_text.lower()\n",
    "    if re.search(r\"\\bdata engineer(ing)?\\b\", t):\n",
    "        return \"data engineering\"\n",
    "    return None\n",
    "\n",
    "def _score_profile_alignment(title: str, fields: dict) -> float:\n",
    "    text = (title or \"\").lower()\n",
    "    bonus = 0.0\n",
    "    for s in (fields.get(\"skills\") or [])[:6]:\n",
    "        if s.lower() in text: bonus += 0.6\n",
    "    for t in (fields.get(\"topics\") or [])[:6]:\n",
    "        if t.lower() in text: bonus += 0.5\n",
    "    return bonus\n",
    "\n",
    "def _run_multi_queries(base_results: list, queries: list, fn_retrieve) -> list:\n",
    "    results = list(base_results)\n",
    "    for q in queries:\n",
    "        try:\n",
    "            results.extend(fn_retrieve(q))\n",
    "        except Exception as e:\n",
    "            print(\"⚠️ subquery failed:\", q, e)\n",
    "    return results\n",
    "\n",
    "def job_tool(query: str, profile_q: list = None, profile_fields: dict = None) -> List[Dict[str,Any]]:\n",
    "    profile_q = profile_q or []\n",
    "    profile_fields = profile_fields or {}\n",
    "\n",
    "    kb = tool_kb_search(query).get(\"jobs\", [])\n",
    "    pg_raw = tool_pg_search(query, 16)\n",
    "    pg = [h for h in pg_raw if guess_type(h) in {\"job\", \"role\"}]\n",
    "    jobs = kb[:8] + pg[:8]\n",
    "\n",
    "    if profile_q:\n",
    "        jobs = _run_multi_queries(jobs, profile_q, lambda q: (\n",
    "            tool_kb_search(q).get(\"jobs\", []) + \n",
    "            [h for h in tool_pg_search(q, 12) if guess_type(h) in {\"job\",\"role\"}]\n",
    "        ))\n",
    "\n",
    "    dedup = {}\n",
    "    for j in jobs:\n",
    "        title = (j.get(\"title\") or (j.get(\"metadata\") or {}).get(\"title\") or \"\").strip()\n",
    "        if not title: continue\n",
    "        key = title.lower()\n",
    "        score = float(j.get(\"score\") or 0.0) + _score_profile_alignment(title, profile_fields)\n",
    "        if key not in dedup or score > dedup[key][\"_score\"]:\n",
    "            jj = dict(j); jj[\"_score\"] = score; jj[\"title\"] = title\n",
    "            dedup[key] = jj\n",
    "\n",
    "    ranked = sorted(dedup.values(), key=lambda x: -x[\"_score\"])\n",
    "    if not ranked:\n",
    "        topic = infer_topic(query)\n",
    "        if topic and FALLBACKS.get(topic, {}).get(\"jobs\"):\n",
    "            ranked = FALLBACKS[topic][\"jobs\"]\n",
    "        else:\n",
    "            ranked = [{\"title\": \"Data Engineer (Platform)\"}, {\"title\": \"Analytics Engineer\"}]\n",
    "    return ranked[:4]\n",
    "\n",
    "def courses_tool(query: str, state: 'AgentState', profile_q: list = None, profile_fields: dict = None) -> List[Dict[str,Any]]:\n",
    "    profile_q = profile_q or []\n",
    "    profile_fields = profile_fields or {}\n",
    "\n",
    "    kb = tool_kb_search(query).get(\"courses\", [])\n",
    "    pg_raw = tool_pg_search(query, 16)\n",
    "    pg = [h for h in pg_raw if guess_type(h) == \"course\"]\n",
    "    courses = kb[:10] + pg[:8]\n",
    "\n",
    "    if profile_q:\n",
    "        courses = _run_multi_queries(courses, profile_q, lambda q: (\n",
    "            tool_kb_search(q).get(\"courses\", []) + \n",
    "            [h for h in tool_pg_search(q, 12) if guess_type(h) == \"course\"]\n",
    "        ))\n",
    "\n",
    "    if state.is_manager or explicit_manager_request(state.prompt or \"\"):\n",
    "        filtered = courses\n",
    "    else:\n",
    "        filtered = [c for c in courses if not looks_manager_only(c)]\n",
    "\n",
    "    bucket = {}\n",
    "    for c in filtered:\n",
    "        title = (c.get(\"title\") or (c.get(\"metadata\") or {}).get(\"title\") or \"Course\").strip()\n",
    "        if not title: continue\n",
    "        key = title.lower()\n",
    "        score = float(c.get(\"score\") or 0.0) + _score_profile_alignment(title, profile_fields)\n",
    "        if key not in bucket or score > bucket[key][\"_score\"]:\n",
    "            cc = {\"title\": title, \"metadata\": c.get(\"metadata\") or {}, \"source\": c.get(\"source\") or \"KB/PG\", \"_score\": score}\n",
    "            bucket[key] = cc\n",
    "\n",
    "    ranked = sorted(bucket.values(), key=lambda x: -x[\"_score\"])\n",
    "    if not ranked:\n",
    "        topic = infer_topic(query)\n",
    "        if topic and FALLBACKS.get(topic, {}).get(\"courses\"):\n",
    "            ranked = FALLBACKS[topic][\"courses\"]\n",
    "        else:\n",
    "            ranked = [{\"title\": \"Data Engineering on AWS — Foundations\"},\n",
    "                      {\"title\": \"Modern Data Pipelines with Python & Airflow\"}]\n",
    "    return ranked[:4]\n",
    "\n",
    "def job_reflexion(items: List[Dict[str,Any]]) -> List[Dict[str,Any]]:\n",
    "    return sorted(items, key=lambda x: (-float(x.get(\"score\") or x.get(\"_score\") or 0.0), len((x.get(\"title\") or \"\"))))\n",
    "\n",
    "def courses_reflexion(items: List[Dict[str,Any]], is_manager: bool) -> List[Dict[str,Any]]:\n",
    "    def rank(it):\n",
    "        base = float(it.get(\"score\") or it.get(\"_score\") or 0.0)\n",
    "        meta = it.get(\"metadata\") or {}\n",
    "        aud = (meta.get(\"audience\") or \"\").lower()\n",
    "        penal = 0 if is_manager else (1 if aud in {\"manager\",\"leadership\"} else 0)\n",
    "        return (penal, -base)\n",
    "    return sorted(items, key=rank)\n",
    "\n",
    "\n",
    "# HOTFIX: override build_profile_queries everywhere\n",
    "def build_profile_queries(fields: dict, max_items: int = 5) -> dict:\n",
    "    skills = (fields.get(\"skills\") or [])[:max_items]\n",
    "    topics = (fields.get(\"topics\") or [])[:max_items]\n",
    "    role   = (fields.get(\"title\") or \"\")\n",
    "\n",
    "    job_q, crs_q = [], []\n",
    "\n",
    "    # Jobs queries from topics/skills/role\n",
    "    for t in topics:\n",
    "        job_q += [f\"{t} roles\", f\"{t} jobs\"]\n",
    "    for s in skills:\n",
    "        job_q.append(f\"{s} engineer jobs\")\n",
    "    if role:\n",
    "        job_q.append(f\"{role} career paths\")\n",
    "\n",
    "    # Courses queries from skills/topics\n",
    "    for s in skills:\n",
    "        crs_q += [f\"{s} course\", f\"{s} training\"]\n",
    "    for t in topics:\n",
    "        crs_q.append(f\"{t} learning path\")  # <- correct append(...)\n",
    "\n",
    "    def dedup(seq):\n",
    "        seen = set(); out = []\n",
    "        for x in seq:\n",
    "            xl = x.lower()\n",
    "            if xl in seen:\n",
    "                continue\n",
    "            seen.add(xl); out.append(x)\n",
    "        return out\n",
    "\n",
    "    return {\"jobs\": dedup(job_q)[:max_items], \"courses\": dedup(crs_q)[:max_items]}\n",
    "\n",
    "# sanity check\n",
    "_test = build_profile_queries({\"skills\":[\"Python\"], \"topics\":[\"data engineering\"], \"title\":\"Data Engineer\"})\n",
    "print(_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a91cba84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.6) Normalization + intersection/bridge ranking\n",
    "\n",
    "import re\n",
    "\n",
    "def _strip_md(s: str) -> str:\n",
    "    if not s: return \"\"\n",
    "    s = re.sub(r\"```[\\s\\S]*?```\", \"\", s)          # fenced blocks\n",
    "    s = re.sub(r\"\\[(.*?)\\]\\((.*?)\\)\", r\"\\1\", s)   # [text](url)\n",
    "    s = s.replace(\"**\",\"\").replace(\"__\",\"\")\n",
    "    s = re.sub(r\"^#+\\s*\", \"\", s)                  # heading hashes\n",
    "    return \" \".join(s.split())\n",
    "\n",
    "def normalize_item(item: dict) -> dict:\n",
    "    meta = item.get(\"metadata\") or {}\n",
    "    title = item.get(\"title\") or meta.get(\"title\") or item.get(\"document\",\"\")\n",
    "    title = _strip_md(title)[:160].strip() or \"Untitled\"\n",
    "    url   = meta.get(\"url\") or item.get(\"source\") or \"\"\n",
    "    audience = (meta.get(\"audience\") or \"\").lower()\n",
    "    tags  = meta.get(\"tags\") or []\n",
    "    typ   = (meta.get(\"type\") or \"\").lower()\n",
    "    score = float(item.get(\"score\") or item.get(\"_score\") or 0.0)\n",
    "    coll  = (item.get(\"collection\") or \"\").lower()\n",
    "    return {\n",
    "        \"title\": title, \"url\": url, \"audience\": audience,\n",
    "        \"tags\": tags, \"type\": typ, \"score\": score, \"collection\": coll\n",
    "    }\n",
    "\n",
    "# light keyword sets to detect target domain & bridge\n",
    "DE_KEYWORDS = {\"data engineer\",\"data engineering\",\"analytics engineer\",\"analytics engineering\",\"etl\",\"pipeline\",\"airflow\",\"spark\",\"dbt\",\"warehouse\",\"lakehouse\",\"bigquery\",\"redshift\",\"glue\"}\n",
    "MK_KEYWORDS = {\"marketing\",\"campaign\",\"crm\",\"email\",\"b2b\",\"b2c\",\"audience\",\"brand\",\"seo\",\"sem\",\"martech\",\"adtech\",\"attribution\",\"mql\",\"sql (sales)\"}\n",
    "\n",
    "def _kw_in(text: str, kws: set) -> bool:\n",
    "    t = text.lower()\n",
    "    return any(k in t for k in kws)\n",
    "\n",
    "def choose_candidates(user_text: str, items: list, profile_fields: dict, target=\"jobs\", top_n=6):\n",
    "    \"\"\"\n",
    "    Re-rank to surface intersection:\n",
    "    - Strongly prefer items that are Data Eng *and* Marketing (bridge).\n",
    "    - Then Data Eng only.\n",
    "    - Then Marketing analytics/BI (on-ramp).\n",
    "    - Penalize manager-only if user isn't a manager.\n",
    "    \"\"\"\n",
    "    txt = user_text.lower()\n",
    "    wants_de = _kw_in(txt, DE_KEYWORDS) or \"data\" in txt or \"engineer\" in txt\n",
    "\n",
    "    skills = [s.lower() for s in (profile_fields.get(\"skills\") or [])]\n",
    "    topics = [t.lower() for t in (profile_fields.get(\"topics\") or [])]\n",
    "    mk_like = any(_kw_in(s, MK_KEYWORDS) for s in skills+topics)\n",
    "\n",
    "    ranked = []\n",
    "    for raw in (items or []):\n",
    "        it = normalize_item(raw)\n",
    "        t = (it[\"title\"] or \"\").lower()\n",
    "        base = it[\"score\"]\n",
    "\n",
    "        is_de = _kw_in(t, DE_KEYWORDS)\n",
    "        is_mk = _kw_in(t, MK_KEYWORDS)\n",
    "\n",
    "        bridge = 0.0\n",
    "        if wants_de and mk_like:\n",
    "            # intersection/bridge bonuses\n",
    "            if is_de and is_mk:\n",
    "                bridge += 3.0\n",
    "            elif is_de:\n",
    "                bridge += 2.0\n",
    "            elif is_mk and any(x in t for x in [\"data\",\"analytics\",\"bi\",\"sql\",\"python\",\"warehouse\"]):\n",
    "                bridge += 1.2\n",
    "\n",
    "        # slight boost for explicit skill/topic mentions\n",
    "        for s in skills[:6]:\n",
    "            if s in t: base += 0.4\n",
    "        for tp in topics[:6]:\n",
    "            if tp in t: base += 0.3\n",
    "\n",
    "        # Final score\n",
    "        it[\"_rank\"] = base + bridge\n",
    "        ranked.append(it)\n",
    "\n",
    "    ranked.sort(key=lambda x: x[\"_rank\"], reverse=True)\n",
    "    # de-dup by title\n",
    "    seen = set(); out = []\n",
    "    for it in ranked:\n",
    "        key = it[\"title\"].lower()\n",
    "        if key in seen: continue\n",
    "        seen.add(key); out.append(it)\n",
    "        if len(out) >= top_n: break\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d81458",
   "metadata": {},
   "source": [
    "## 6) Compose & Orchestrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81feae5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a654a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.5) Pure LLM synthesis on Bedrock (Claude 3.7 Sonnet)\n",
    "\n",
    "import json, boto3\n",
    "\n",
    "try:\n",
    "    _bedrock = boto3.client(\"bedrock-runtime\", region_name=AWS_REGION)\n",
    "except Exception as _e:\n",
    "    _bedrock = None\n",
    "    print(\"⚠️ Bedrock runtime not available; set AWS creds/region to enable synthesis.\")\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are Informa’s internal career advisor. \"\n",
    "    \"Write naturally and concisely, tailored to the employee’s background and the question. \"\n",
    "    \"Explain tradeoffs, propose bridge steps if the target domain differs from the profile. \"\n",
    "    \"Use only facts provided; do not invent links or data.\"\n",
    ")\n",
    "\n",
    "def _compact(items):\n",
    "    out = []\n",
    "    for x in (items or []):\n",
    "        out.append({\n",
    "            \"title\": x.get(\"title\"),\n",
    "            \"url\": x.get(\"url\"),\n",
    "            \"audience\": x.get(\"audience\"),\n",
    "            \"tags\": x.get(\"tags\"),\n",
    "            \"score\": x.get(\"score\"),\n",
    "            \"collection\": x.get(\"collection\"),\n",
    "        })\n",
    "    return out[:8]\n",
    "\n",
    "def synthesize_answer_llm(user_text: str, intents: list, is_manager: bool,\n",
    "                          profile_fields: dict, sections: dict) -> str:\n",
    "    if not _bedrock:\n",
    "        raise RuntimeError(\"Bedrock not configured\")\n",
    "\n",
    "    # Prepare model-facing JSON (lean)\n",
    "    payload = {\n",
    "        \"query\": user_text,\n",
    "        \"intents\": intents,\n",
    "        \"persona\": {\"is_manager\": bool(is_manager)},\n",
    "        \"profile\": {\n",
    "            \"name\": profile_fields.get(\"name\"),\n",
    "            \"title\": profile_fields.get(\"title\"),\n",
    "            \"skills\": profile_fields.get(\"skills\") or [],\n",
    "            \"topics\": profile_fields.get(\"topics\") or [],\n",
    "        },\n",
    "        \"retrieval\": {\n",
    "            \"jobs\": _compact(sections.get(\"jobs\")),\n",
    "            \"courses\": _compact(sections.get(\"courses\")),\n",
    "            \"development_plan\": _compact(sections.get(\"development_plan\")),\n",
    "            \"manager_toolkit\": _compact(sections.get(\"manager_toolkit\")),\n",
    "            \"leadership_strategy\": _compact(sections.get(\"leadership_strategy\")),\n",
    "        }\n",
    "    }\n",
    "\n",
    "    user_msg = (\n",
    "        \"Using only this JSON, answer the user naturally. \"\n",
    "        \"Pick items that best fit the query and the profile; prefer intersection/bridge when needed. \"\n",
    "        \"If information is insufficient, ask for the minimum missing detail.\\n\\n\"\n",
    "        + json.dumps(payload, ensure_ascii=False)\n",
    "    )\n",
    "\n",
    "    body = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 700,\n",
    "        \"temperature\": 0.4,\n",
    "        \"system\": [{\"type\":\"text\",\"text\": SYSTEM_PROMPT}],   # <-- system at top-level\n",
    "        \"messages\": [\n",
    "            {\"role\":\"user\",\"content\":[{\"type\":\"text\",\"text\": user_msg}]}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    resp = _bedrock.invoke_model(\n",
    "        modelId=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "        body=json.dumps(body),\n",
    "        accept=\"application/json\",\n",
    "        contentType=\"application/json\",\n",
    "    )\n",
    "    out = json.loads(resp[\"body\"].read().decode(\"utf-8\"))\n",
    "    parts = out.get(\"content\", [])\n",
    "    text = \"\\n\".join(p.get(\"text\",\"\") for p in parts if p.get(\"type\")==\"text\").strip()\n",
    "    return text or \"(no content)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ec5e9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6) Compose & Orchestrate\n",
    "\n",
    "def run_workflow(\n",
    "    user_text: str,\n",
    "    email: Optional[str] = None,\n",
    "    name: Optional[str] = None,\n",
    "    division: Optional[str] = None,\n",
    "    override_is_manager: Optional[bool] = None\n",
    ") -> Dict[str,Any]:\n",
    "\n",
    "    gate = prohibitor(user_text)\n",
    "    if not gate.get(\"allowed\"):\n",
    "        return {\"blocked\": True, \"gate\": gate, \"answer\": \"out_of_scope\"}\n",
    "\n",
    "    # Load state/profile\n",
    "    state, profile_meta = setup_state(\n",
    "        email=email, name=name, division=division,\n",
    "        override_is_manager=override_is_manager, user_text=user_text\n",
    "    )\n",
    "\n",
    "    # Parse profile\n",
    "    fields: Dict[str, Any] = {}\n",
    "    try:\n",
    "        if state.quick_profile and state.quick_profile.get(\"_doc\") is not None:\n",
    "            fields = extract_profile_fields(state.quick_profile[\"_doc\"], profile_meta or {})\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ profile parse failed:\", e)\n",
    "        fields = {}\n",
    "\n",
    "    # Intents & profile-driven queries\n",
    "    intents = intent_persona(gate.get(\"intents\", []))\n",
    "    profile_qs = build_profile_queries(fields) if fields else {\"jobs\": [], \"courses\": []}\n",
    "\n",
    "    # Raw tool retrieval\n",
    "    sections_raw: Dict[str, Any] = {}\n",
    "    if \"job\" in intents:\n",
    "        sections_raw[\"jobs\"] = job_reflexion(\n",
    "            job_tool(user_text, profile_q=profile_qs.get(\"jobs\") or [], profile_fields=fields)\n",
    "        )\n",
    "    if \"courses\" in intents:\n",
    "        sections_raw[\"courses\"] = courses_reflexion(\n",
    "            courses_tool(user_text, state, profile_q=profile_qs.get(\"courses\") or [], profile_fields=fields),\n",
    "            state.is_manager\n",
    "        )\n",
    "    if \"development_plan\" in intents:\n",
    "        sections_raw[\"development_plan\"] = tool_pg_search(\"development plan \" + (user_text or \"\"), 6)[:5]\n",
    "    if \"manager_toolkit\" in intents:\n",
    "        sections_raw[\"manager_toolkit\"] = tool_pg_search(\"manager coaching \" + (user_text or \"\"), 6)[:5]\n",
    "    if \"leadership_strategy\" in intents:\n",
    "        sections_raw[\"leadership_strategy\"] = tool_pg_search(\"capability gaps portfolio \" + (user_text or \"\"), 6)[:5]\n",
    "\n",
    "    # Intersection/bridge selection so LLM sees the right candidates\n",
    "    if \"job\" in intents:\n",
    "        sections_jobs = choose_candidates(user_text, sections_raw.get(\"jobs\"), fields, target=\"jobs\", top_n=6)\n",
    "    else:\n",
    "        sections_jobs = []\n",
    "    if \"courses\" in intents:\n",
    "        sections_courses = choose_candidates(user_text, sections_raw.get(\"courses\"), fields, target=\"courses\", top_n=6)\n",
    "    else:\n",
    "        sections_courses = []\n",
    "\n",
    "    sections = dict(sections_raw)  # keep other sections as-is\n",
    "    sections[\"jobs\"] = sections_jobs\n",
    "    sections[\"courses\"] = sections_courses\n",
    "\n",
    "    # LLM writes the final answer (no hardcoded copy)\n",
    "    try:\n",
    "        final = synthesize_answer_llm(\n",
    "            user_text=user_text,\n",
    "            intents=intents,\n",
    "            is_manager=state.is_manager,\n",
    "            profile_fields=fields or {},\n",
    "            sections=sections\n",
    "        )\n",
    "    except Exception as e:\n",
    "        final = f\"(LLM unavailable: {e})\"\n",
    "\n",
    "    return {\n",
    "        \"blocked\": False,\n",
    "        \"gate\": gate,\n",
    "        \"state\": state,\n",
    "        \"profile_found\": bool(profile_meta),\n",
    "        \"profile_fields\": fields,\n",
    "        \"sections\": sections,          # now already intersection-weighted\n",
    "        \"answer\": final\n",
    "    }\n",
    "\n",
    "# 6.7) Simple streaming renderer for notebooks\n",
    "\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "import time\n",
    "\n",
    "def render_stream(generator, refresh=0.05):\n",
    "    \"\"\"\n",
    "    Renders streaming text in-place. Call with the generator returned by synthesize_answer_llm_stream.\n",
    "    \"\"\"\n",
    "    buf = []\n",
    "    handle = display(Markdown(\"\"), display_id=True)\n",
    "    last_flush = time.time()\n",
    "    for chunk in generator:\n",
    "        buf.append(chunk)\n",
    "        if time.time() - last_flush >= refresh:\n",
    "            handle.update(Markdown(\"\".join(buf)))\n",
    "            last_flush = time.time()\n",
    "    # final flush\n",
    "    handle.update(Markdown(\"\".join(buf)))\n",
    "    return \"\".join(buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "879f4611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.6) Streaming synthesis (Claude 3.7 Sonnet on Bedrock)\n",
    "# - Uses invoke_model_with_response_stream\n",
    "# - Yields text deltas as they arrive\n",
    "# - Falls back to non-streaming if not supported/enabled\n",
    "\n",
    "import json, sys, time\n",
    "from typing import Generator\n",
    "\n",
    "def _make_messages_body(user_text: str, intents: list, is_manager: bool, profile_fields: dict, sections: dict):\n",
    "    payload = {\n",
    "        \"query\": user_text,\n",
    "        \"intents\": intents,\n",
    "        \"persona\": {\"is_manager\": bool(is_manager)},\n",
    "        \"profile\": {\n",
    "            \"name\": profile_fields.get(\"name\"),\n",
    "            \"title\": profile_fields.get(\"title\"),\n",
    "            \"skills\": profile_fields.get(\"skills\") or [],\n",
    "            \"topics\": profile_fields.get(\"topics\") or [],\n",
    "        },\n",
    "        \"retrieval\": {\n",
    "            \"jobs\":   [{\"title\": x.get(\"title\"), \"url\": x.get(\"url\")} for x in (sections.get(\"jobs\") or [])][:8],\n",
    "            \"courses\":[{\"title\": x.get(\"title\"), \"url\": x.get(\"url\")} for x in (sections.get(\"courses\") or [])][:8],\n",
    "            \"development_plan\": [{\"title\": x.get(\"title\") or (x.get(\"metadata\") or {}).get(\"title\",\"\")} for x in (sections.get(\"development_plan\") or [])][:6],\n",
    "            \"manager_toolkit\":  [{\"title\": x.get(\"title\") or (x.get(\"metadata\") or {}).get(\"title\",\"\")} for x in (sections.get(\"manager_toolkit\")  or [])][:6],\n",
    "            \"leadership_strategy\":[{\"title\": x.get(\"title\") or (x.get(\"metadata\") or {}).get(\"title\",\"\")} for x in (sections.get(\"leadership_strategy\") or [])][:6],\n",
    "        }\n",
    "    }\n",
    "    SYSTEM_PROMPT = (\n",
    "        \"You are Informa’s internal career advisor. \"\n",
    "        \"Write naturally and concisely, tailored to the employee’s background and the question. \"\n",
    "        \"Prefer bridges when profile and target domain differ; pick only from provided facts; no invented links.\"\n",
    "    )\n",
    "    body = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 700,\n",
    "        \"temperature\": 0.4,\n",
    "        \"system\": [{\"type\":\"text\",\"text\": SYSTEM_PROMPT}],\n",
    "        \"messages\": [\n",
    "            {\"role\":\"user\",\"content\":[\n",
    "                {\"type\":\"text\",\"text\": \"Using only this JSON, answer naturally. \"\n",
    "                                       \"Pick items that best fit the query and profile; prefer intersection/bridge when needed. \"\n",
    "                                       \"If info is insufficient, ask for the minimum missing detail.\\n\\n\"\n",
    "                                       + json.dumps(payload, ensure_ascii=False)}\n",
    "            ]}\n",
    "        ]\n",
    "    }\n",
    "    return body\n",
    "\n",
    "def synthesize_answer_llm_stream(\n",
    "    user_text: str,\n",
    "    intents: list,\n",
    "    is_manager: bool,\n",
    "    profile_fields: dict,\n",
    "    sections: dict,\n",
    "    model_id: str = \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    ") -> Generator[str, None, None]:\n",
    "    \"\"\"\n",
    "    Yields chunks of text as they arrive from Bedrock streaming API.\n",
    "    \"\"\"\n",
    "    if _bedrock is None:\n",
    "        raise RuntimeError(\"Bedrock not configured\")\n",
    "\n",
    "    body = _make_messages_body(user_text, intents, is_manager, profile_fields, sections)\n",
    "\n",
    "    try:\n",
    "        resp = _bedrock.invoke_model_with_response_stream(\n",
    "            modelId=model_id,\n",
    "            body=json.dumps(body),\n",
    "            accept=\"application/json\",\n",
    "            contentType=\"application/json\",\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # Fallback to non-streaming path if streaming not available\n",
    "        yield f\"(Streaming unavailable: {e})\"\n",
    "        return\n",
    "\n",
    "    # Anthropic streaming returns a sequence of JSON events inside resp['body']\n",
    "    # Event types include: message_start, content_block_start, content_block_delta{text}, content_block_stop, message_delta, message_stop\n",
    "    try:\n",
    "        for event in resp.get(\"body\"):\n",
    "            if \"chunk\" not in event:\n",
    "                continue\n",
    "            try:\n",
    "                payload = json.loads(event[\"chunk\"][\"bytes\"].decode(\"utf-8\"))\n",
    "            except Exception:\n",
    "                continue\n",
    "            etype = payload.get(\"type\")\n",
    "\n",
    "            # The useful text arrives in 'content_block_delta' with {\"delta\":{\"type\":\"text_delta\",\"text\":\"...\"}}\n",
    "            if etype == \"content_block_delta\":\n",
    "                delta = payload.get(\"delta\", {})\n",
    "                if delta.get(\"type\") == \"text_delta\":\n",
    "                    txt = delta.get(\"text\", \"\")\n",
    "                    if txt:\n",
    "                        yield txt\n",
    "            # You can inspect other events if desired for telemetry:\n",
    "            # elif etype in {\"message_start\",\"message_delta\",\"message_stop\",\"content_block_start\",\"content_block_stop\"}: pass\n",
    "    except Exception as e:\n",
    "        yield f\"\\n(Streaming error: {e})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c189198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.7) Simple streaming renderer for notebooks\n",
    "\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "import time\n",
    "\n",
    "def render_stream(generator, refresh=0.05):\n",
    "    \"\"\"\n",
    "    Renders streaming text in-place. Call with the generator returned by synthesize_answer_llm_stream.\n",
    "    \"\"\"\n",
    "    buf = []\n",
    "    handle = display(Markdown(\"\"), display_id=True)\n",
    "    last_flush = time.time()\n",
    "    for chunk in generator:\n",
    "        buf.append(chunk)\n",
    "        if time.time() - last_flush >= refresh:\n",
    "            handle.update(Markdown(\"\".join(buf)))\n",
    "            last_flush = time.time()\n",
    "    # final flush\n",
    "    handle.update(Markdown(\"\".join(buf)))\n",
    "    return \"\".join(buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be640e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.7) Simple streaming renderer for notebooks\n",
    "\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "import time\n",
    "\n",
    "def render_stream(generator, refresh=0.05):\n",
    "    \"\"\"\n",
    "    Renders streaming text in-place. Call with the generator returned by synthesize_answer_llm_stream.\n",
    "    \"\"\"\n",
    "    buf = []\n",
    "    handle = display(Markdown(\"\"), display_id=True)\n",
    "    last_flush = time.time()\n",
    "    for chunk in generator:\n",
    "        buf.append(chunk)\n",
    "        if time.time() - last_flush >= refresh:\n",
    "            handle.update(Markdown(\"\".join(buf)))\n",
    "            last_flush = time.time()\n",
    "    # final flush\n",
    "    handle.update(Markdown(\"\".join(buf)))\n",
    "    return \"\".join(buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28088128",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "run_workflow() got an unexpected keyword argument 'stream'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Streaming run (prints progressively in the output cell)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m out = \u001b[43mrun_workflow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat jobs and courses should I look at for data engineering?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43memail\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdivision\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverride_is_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m out.get(\u001b[33m\"\u001b[39m\u001b[33mblocked\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBLOCKED\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: run_workflow() got an unexpected keyword argument 'stream'"
     ]
    }
   ],
   "source": [
    "# Streaming run (prints progressively in the output cell)\n",
    "out = run_workflow(\n",
    "    \"What jobs and courses should I look at for data engineering?\",\n",
    "    email=None, name=None, division=None,\n",
    "    override_is_manager=False,\n",
    "    stream=True\n",
    ")\n",
    "if out.get(\"blocked\"):\n",
    "    print(\"BLOCKED\")\n",
    "else:\n",
    "    rendered = render_stream(out[\"stream\"])  # shows increments live\n",
    "    # 'rendered' holds the final text if you need to store/log it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42013fdd",
   "metadata": {},
   "source": [
    "## 7) Smoke Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405f570a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---\n",
      "Q: Reset my laptop password | override_is_manager: None\n",
      "BLOCKED: out_of_scope\n",
      "\n",
      "---\n",
      "Q: Analyze my current skillset against Informa's digital transformation needs and recommend 5 specific learning opportunities to close these gaps. | override_is_manager: False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tests = [\n",
    "    (\"Reset my laptop password\", None, None, None, None),\n",
    "    (\"Analyze my current skillset against Informa's digital transformation needs and recommend 5 specific learning opportunities to close these gaps.\", None, None, None, False),\n",
    "    (\"Create a 30-day plan to master machine learning with daily practice steps and metrics to track my progress within my current role at Informa.\", None, None, None, True),\n",
    "]\n",
    "\n",
    "for text, email, name, div, is_mgr in tests:\n",
    "    print(\"\\n---\\nQ:\", text, \"| override_is_manager:\", is_mgr)\n",
    "    out = run_workflow(text, email=email, name=name, division=div, override_is_manager=is_mgr)\n",
    "    if out.get(\"blocked\"):\n",
    "        print(\"BLOCKED:\", out[\"answer\"])\n",
    "    else:\n",
    "        display(Markdown(out[\"answer\"]))\n",
    "        print(\"intents:\", out[\"gate\"][\"intents\"], \"| is_manager:\", out[\"state\"].is_manager, \"| profile_found:\", out[\"profile_found\"])\n",
    "        print(\"[debug] profile fields:\", out.get(\"profile_fields\"))\n",
    "        print(\"[debug] jobs:\", [j.get(\"title\") for j in out.get(\"sections\",{}).get(\"jobs\",[])])\n",
    "        print(\"[debug] courses:\", [c.get(\"title\") for c in out.get(\"sections\",{}).get(\"courses\",[])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b409f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9682b75e",
   "metadata": {},
   "source": [
    "\n",
    "# Streaming Utilities (Drop‑in)\n",
    "\n",
    "This section adds **token streaming** with graceful fallbacks for both **AWS Bedrock** and **OpenAI** backends.\n",
    "Use `stream_complete(...)` to get a generator that yields text chunks. Works for synchronous scripts and Jupyter.\n",
    "\n",
    "**How to use**\n",
    "```python\n",
    "from streaming import stream_complete, print_stream\n",
    "\n",
    "gen = stream_complete(\n",
    "    provider=os.getenv(\"LLM_PROVIDER\", \"bedrock\"),          # \"bedrock\" or \"openai\"\n",
    "    model=os.getenv(\"LLM_MODEL\", \"anthropic.claude-3-5-sonnet-20240620\"),  # or \"gpt-4.1-mini\" etc.\n",
    "    messages=[{\"role\":\"user\",\"content\":\"Give me a 2-line summary of the career agent architecture.\"}],\n",
    "    temperature=0.2,\n",
    ")\n",
    "print_stream(gen)  # prints tokens as they arrive\n",
    "```\n",
    "If credentials or networking are unavailable, it will **auto‑fallback** to a local mock stream so notebooks keep running.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9db715da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded streaming utilities: stream_complete(), print_stream(), demo_stream().\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %%streaming.py (inline cell for convenience)\n",
    "# Robust token streaming for Bedrock + OpenAI with fallback to a local mock.\n",
    "import os, sys, json, time\n",
    "from typing import Dict, Iterator, List, Optional\n",
    "\n",
    "def _env_true(v: Optional[str]) -> bool:\n",
    "    return (v or \"\").strip().lower() in {\"1\",\"true\",\"yes\",\"y\",\"on\"}\n",
    "\n",
    "def _mock_stream(text: str, delay: float = 0.02) -> Iterator[str]:\n",
    "    \"\"\"Yield tokens from a local string when real streaming isn't available.\"\"\"\n",
    "    for tok in text.split():\n",
    "        yield tok + \" \"\n",
    "        time.sleep(delay)\n",
    "\n",
    "def _print_err_once(msg: str, _printed=set()):\n",
    "    if msg not in _printed:\n",
    "        print(f\"[stream:fallback] {msg}\", file=sys.stderr)\n",
    "        _printed.add(msg)\n",
    "\n",
    "def _openai_stream(model: str, messages: List[Dict], temperature: float=0.2) -> Iterator[str]:\n",
    "    try:\n",
    "        # OpenAI responses streaming (requires openai>=1.0)\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI()\n",
    "        stream = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            stream=True,\n",
    "        )\n",
    "        for event in stream:\n",
    "            delta = event.choices[0].delta\n",
    "            if delta and delta.content:\n",
    "                yield delta.content\n",
    "    except Exception as e:\n",
    "        _print_err_once(f\"OpenAI streaming unavailable: {e}\")\n",
    "        yield from _mock_stream(\"<<OPENAI STREAM FALLBACK>> \" + _mock_reply_from_messages(messages))\n",
    "\n",
    "def _bedrock_stream(model: str, messages: List[Dict], temperature: float=0.2) -> Iterator[str]:\n",
    "    \"\"\"\n",
    "    Streams text tokens from AWS Bedrock. Supports Anthropic/Meta style \"messages\".\n",
    "    Uses InvokeModelWithResponseStream on bedrock-runtime.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import boto3\n",
    "        br = boto3.client(\"bedrock-runtime\")\n",
    "\n",
    "        # If messages are OpenAI-style, map to Anthropic Messages API format if model is Anthropic\n",
    "        if \"anthropic\" in model:\n",
    "            def to_anthropic_content(msg):\n",
    "                c = msg.get(\"content\", \"\")\n",
    "                if isinstance(c, list):\n",
    "                    return c\n",
    "                return [{\"type\":\"text\",\"text\": c}]\n",
    "            anthropic_msgs = [{\"role\": m[\"role\"], \"content\": to_anthropic_content(m)} for m in messages if m[\"role\"] in {\"user\",\"assistant\"}]\n",
    "            body = {\n",
    "                \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                \"max_tokens\": 1024,\n",
    "                \"temperature\": temperature,\n",
    "                \"messages\": anthropic_msgs,\n",
    "            }\n",
    "        else:\n",
    "            user_text = \"\\n\".join([m.get(\"content\",\"\") for m in messages if m.get(\"role\")==\"user\"])\n",
    "            body = {\n",
    "                \"input_text\": user_text,\n",
    "                \"temperature\": temperature,\n",
    "                \"max_tokens\": 1024,\n",
    "            }\n",
    "\n",
    "        resp = br.invoke_model_with_response_stream(\n",
    "            modelId=model,\n",
    "            body=json.dumps(body),\n",
    "            accept=\"application/json\",\n",
    "            contentType=\"application/json\",\n",
    "        )\n",
    "\n",
    "        for event in resp.get(\"body\"):\n",
    "            if \"chunk\" in event:\n",
    "                try:\n",
    "                    chunk = json.loads(event[\"chunk\"][\"bytes\"].decode(\"utf-8\"))\n",
    "                    if \"delta\" in chunk and isinstance(chunk[\"delta\"], dict) and \"text\" in chunk[\"delta\"]:\n",
    "                        yield chunk[\"delta\"][\"text\"]\n",
    "                    elif \"outputText\" in chunk:\n",
    "                        yield chunk[\"outputText\"]\n",
    "                    elif \"message\" in chunk and isinstance(chunk[\"message\"], dict) and \"content\" in chunk[\"message\"]:\n",
    "                        parts = chunk[\"message\"][\"content\"]\n",
    "                        for p in parts:\n",
    "                            if p.get(\"type\")==\"text\" and \"text\" in p:\n",
    "                                yield p[\"text\"]\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "    except Exception as e:\n",
    "        _print_err_once(f\"Bedrock streaming unavailable: {e}\")\n",
    "        yield from _mock_stream(\"<<BEDROCK STREAM FALLBACK>> \" + _mock_reply_from_messages(messages))\n",
    "\n",
    "def _mock_reply_from_messages(messages: List[Dict]) -> str:\n",
    "    last_user = \"\"\n",
    "    for m in reversed(messages):\n",
    "        if m.get(\"role\") == \"user\":\n",
    "            last_user = m.get(\"content\",\"\")\n",
    "            break\n",
    "    return f\"(mock) I received your request: {last_user[:120]}...\"\n",
    "\n",
    "def stream_complete(provider: str,\n",
    "                    model: str,\n",
    "                    messages: List[Dict],\n",
    "                    temperature: float = 0.2) -> Iterator[str]:\n",
    "    \"\"\"\n",
    "    provider: \"bedrock\" | \"openai\"\n",
    "    returns: generator yielding text chunks\n",
    "    \"\"\"\n",
    "    provider = (provider or \"bedrock\").lower()\n",
    "    if provider == \"openai\":\n",
    "        return _openai_stream(model, messages, temperature)\n",
    "    else:\n",
    "        return _bedrock_stream(model, messages, temperature)\n",
    "\n",
    "def print_stream(gen: Iterator[str]) -> None:\n",
    "    \"\"\"Pretty-print tokens as they arrive (stdout flush).\"\"\"\n",
    "    for chunk in gen:\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "    print()\n",
    "\n",
    "def demo_stream():\n",
    "    provider = os.getenv(\"LLM_PROVIDER\",\"bedrock\")\n",
    "    model = os.getenv(\"LLM_MODEL\",\"anthropic.claude-3-5-sonnet-20240620\")\n",
    "    messages = [{\"role\":\"user\",\"content\":\"Create a strategy to increase my visibility within Informa by contributing to internal knowledge sharing and company initiatives.\"}]\n",
    "    gen = stream_complete(provider, model, messages, temperature=0.3)\n",
    "    print_stream(gen)\n",
    "\n",
    "print(\"Loaded streaming utilities: stream_complete(), print_stream(), demo_stream().\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74e8e7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<BEDROCK STREAM FALLBACK>> (mock) I received your request: Create a "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[stream:fallback] Bedrock streaming unavailable: An error occurred (ValidationException) when calling the InvokeModelWithResponseStream operation: The provided model identifier is invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strategy to increase my visibility within Informa by contributing to internal knowledge sharing and company ini... \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage (safe to run without cloud creds; will fallback to mock stream):\n",
    "demo_stream()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812621cd",
   "metadata": {},
   "source": [
    "\n",
    "## Streaming Diagnostics\n",
    "\n",
    "Use this to confirm streaming is live and measure **Time To First Byte (TTFB)** and **throughput**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abcfc05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provider: bedrock | Model: anthropic.claude-3-5-sonnet-20240620\n",
      "[2025-08-12T09:45:49] FIRST CHUNK after 0.205s\n",
      "[2025-08-12T09:45:49] chunk#1 (+10 chars): <<BEDROCK \n",
      "[2025-08-12T09:45:49] chunk#2 (+7 chars): STREAM \n",
      "[2025-08-12T09:45:49] chunk#3 (+11 chars): FALLBACK>> \n",
      "[2025-08-12T09:45:49] chunk#4 (+7 chars): (mock) \n",
      "[2025-08-12T09:45:49] chunk#5 (+2 chars): I \n",
      "[2025-08-12T09:45:50] chunk#6 (+9 chars): received \n",
      "[2025-08-12T09:45:50] chunk#7 (+5 chars): your \n",
      "[2025-08-12T09:45:50] chunk#8 (+9 chars): request: \n",
      "[2025-08-12T09:45:50] chunk#9 (+7 chars): Create \n",
      "[2025-08-12T09:45:50] chunk#10 (+2 chars): a \n",
      "[2025-08-12T09:45:50] chunk#11 (+9 chars): strategy \n",
      "[2025-08-12T09:45:50] chunk#12 (+3 chars): to \n",
      "[2025-08-12T09:45:50] chunk#13 (+9 chars): increase \n",
      "[2025-08-12T09:45:50] chunk#14 (+3 chars): my \n",
      "[2025-08-12T09:45:50] chunk#15 (+11 chars): visibility \n",
      "[2025-08-12T09:45:50] chunk#16 (+7 chars): within \n",
      "[2025-08-12T09:45:50] chunk#17 (+8 chars): Informa \n",
      "[2025-08-12T09:45:50] chunk#18 (+3 chars): by \n",
      "[2025-08-12T09:45:50] chunk#19 (+13 chars): contributing \n",
      "[2025-08-12T09:45:50] chunk#20 (+3 chars): to \n",
      "[2025-08-12T09:45:50] chunk#21 (+9 chars): internal \n",
      "[2025-08-12T09:45:50] chunk#22 (+10 chars): knowledge \n",
      "[2025-08-12T09:45:50] chunk#23 (+8 chars): sharing \n",
      "[2025-08-12T09:45:50] chunk#24 (+4 chars): and \n",
      "[2025-08-12T09:45:50] chunk#25 (+8 chars): company \n",
      "[2025-08-12T09:45:50] chunk#26 (+7 chars): ini... \n",
      "\n",
      "--- Streaming Stats ---\n",
      "TTFB (s): 0.205\n",
      "Total time (s): 0.764\n",
      "Post-first-chunk time (s): 0.559\n",
      "Chars received: 184\n",
      "Chunks received: 26\n",
      "Throughput (chars/sec after first chunk): 329.2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def stream_diagnostics(provider, model, messages, temperature=0.2, max_print=2000):\n",
    "    \"\"\"\n",
    "    Prints timestamps for each chunk, measures TTFB and tokens/sec.\n",
    "    \"\"\"\n",
    "    print(f\"Provider: {provider} | Model: {model}\")\n",
    "    start = time.time()\n",
    "    first_time = None\n",
    "    n_chars = 0\n",
    "    n_chunks = 0\n",
    "\n",
    "    gen = stream_complete(provider, model, messages, temperature=temperature)\n",
    "    for chunk in gen:\n",
    "        now = time.time()\n",
    "        if first_time is None:\n",
    "            first_time = now\n",
    "            print(f\"[{datetime.now().isoformat(timespec='seconds')}] FIRST CHUNK after {first_time - start:.3f}s\")\n",
    "        n_chunks += 1\n",
    "        n_chars += len(chunk)\n",
    "        # Print small preview of each chunk with timestamp to verify live updates\n",
    "        preview = chunk[:80].replace(\"\\n\", \" \")\n",
    "        print(f\"[{datetime.now().isoformat(timespec='seconds')}] chunk#{n_chunks} (+{len(chunk)} chars): {preview}\", flush=True)\n",
    "\n",
    "    end = time.time()\n",
    "    if first_time is None:\n",
    "        print(\"No chunks received.\")\n",
    "        return\n",
    "    ttfb = first_time - start\n",
    "    total = end - start\n",
    "    body_time = total - ttfb\n",
    "    cps = (n_chars / body_time) if body_time > 0 else float('inf')\n",
    "    print(\"\\n--- Streaming Stats ---\")\n",
    "    print(f\"TTFB (s): {ttfb:.3f}\")\n",
    "    print(f\"Total time (s): {total:.3f}\")\n",
    "    print(f\"Post-first-chunk time (s): {body_time:.3f}\")\n",
    "    print(f\"Chars received: {n_chars}\")\n",
    "    print(f\"Chunks received: {n_chunks}\")\n",
    "    print(f\"Throughput (chars/sec after first chunk): {cps:.1f}\")\n",
    "\n",
    "# Example: run once to test (uses env vars or defaults; falls back to mock if needed)\n",
    "provider=os.getenv(\"LLM_PROVIDER\",\"bedrock\")\n",
    "model=os.getenv(\"LLM_MODEL\",\"anthropic.claude-3-5-sonnet-20240620\")\n",
    "messages=[{\"role\":\"user\",\"content\":\"Create a strategy to increase my visibility within Informa by contributing to internal knowledge sharing and company initiatives.\"}]\n",
    "stream_diagnostics(provider, model, messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a700f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elysia-dev-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
