{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e552444b",
   "metadata": {},
   "source": [
    "# HR Career Advisor — PG Vector + AWS KB (Prototype **v3**)\n",
    "\n",
    "**What’s new in v3**\n",
    "- Email-first profile lookup (name/division optional)\n",
    "- Parse *Skills* and *Topics of Interest* from profile\n",
    "- Profile-driven queries + ranking boost\n",
    "- Type guessing for PG rows without `metadata.type`\n",
    "- Same dopamine onboarding + fallbacks\n",
    "\n",
    "**Flow:** prohibitor → setup_state → intent_persona → tools (PG+KB + profile-driven) → reflexion → consolidation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e637b9",
   "metadata": {},
   "source": [
    "## 0) Setup & Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78e34651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env present: {'PG': True, 'JOB_KB': True, 'COURSES_KB': True, 'DEFAULT_USER_NAME': 'Mary Ralicki', 'DEFAULT_USER_EMAIL': 'mary.ralicki@informa.com'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, json, re\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from IPython.display import display, Markdown\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "AWS_REGION = os.getenv(\"AWS_REGION\", \"us-west-2\")\n",
    "AWS_MODEL_ID = \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\"\n",
    "\n",
    "PG_DSN = os.getenv(\"PG_DSN\",\"\")  # postgresql://user:pass@host:5432/dbname?sslmode=require\n",
    "PG_COLLECTIONS = [\n",
    "    \"internal_private_employee_profiles_vectorstore\",\n",
    "    \"internal_curated_informa_vectorstore\",\n",
    "]\n",
    "JOB_KB_ID = os.getenv(\"JOB_KB_ID\",\"\")\n",
    "COURSES_KB_ID = os.getenv(\"COURSES_KB_ID\",\"\")\n",
    "\n",
    "DEFAULT_USER_NAME  = os.getenv(\"DEFAULT_USER_NAME\",  \"Mary Ralicki\")\n",
    "DEFAULT_USER_EMAIL = os.getenv(\"DEFAULT_USER_EMAIL\", \"mary.ralicki@informa.com\")\n",
    "DEFAULT_USER_DIV   = os.getenv(\"DEFAULT_USER_DIVISION\", \"\")\n",
    "\n",
    "print(\"Env present:\", dict(\n",
    "    PG=bool(PG_DSN),\n",
    "    JOB_KB=bool(JOB_KB_ID),\n",
    "    COURSES_KB=bool(COURSES_KB_ID),\n",
    "    DEFAULT_USER_NAME=DEFAULT_USER_NAME,\n",
    "    DEFAULT_USER_EMAIL=DEFAULT_USER_EMAIL\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec50f429",
   "metadata": {},
   "source": [
    "## 1) PGVector Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca0baa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import psycopg\n",
    "\n",
    "def get_pg_conn():\n",
    "    if not PG_DSN:\n",
    "        raise RuntimeError(\"PG_DSN not set\")\n",
    "    return psycopg.connect(PG_DSN)\n",
    "\n",
    "KEYWORD_PREFILTER_SQL = (\n",
    "\"SELECT e.uuid AS id, e.embedding, e.document, e.cmetadata, c.name as collection \"\n",
    "\"FROM ai.langchain_pg_embedding e \"\n",
    "\"JOIN ai.langchain_pg_collection c ON c.uuid = e.collection_id \"\n",
    "\"WHERE c.name = %(collection)s \"\n",
    "\"  AND (e.document ILIKE '%%' || %(query)s || '%%' \"\n",
    "\"       OR CAST(e.cmetadata AS TEXT) ILIKE '%%' || %(query)s || '%%') \"\n",
    "\"LIMIT %(k)s;\"\n",
    ")\n",
    "\n",
    "def _to_meta(meta):\n",
    "    if isinstance(meta,(dict,list)): return meta\n",
    "    try: return json.loads(meta)\n",
    "    except: return {\"raw\": str(meta)}\n",
    "\n",
    "def _cosine(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    denom = (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    if denom == 0: return 0.0\n",
    "    return float(np.dot(a, b) / denom)\n",
    "\n",
    "def pg_search_hybrid(collection: str, query: str, pre_k: int = 24, top_k: int = 8) -> List[Dict[str,Any]]:\n",
    "    with get_pg_conn() as conn, conn.cursor() as cur:\n",
    "        cur.execute(KEYWORD_PREFILTER_SQL, {\"collection\": collection, \"query\": query, \"k\": pre_k})\n",
    "        rows = cur.fetchall()\n",
    "    if not rows: return []\n",
    "    embs, items = [], []\n",
    "    for _id, emb, doc, meta, coll in rows:\n",
    "        v = np.array(emb, dtype=np.float32)\n",
    "        embs.append(v)\n",
    "        items.append({\"id\": _id, \"embedding\": emb, \"document\": doc, \"metadata\": _to_meta(meta), \"collection\": coll})\n",
    "    centroid = np.mean(embs, axis=0)\n",
    "    for it in items:\n",
    "        it[\"score\"] = _cosine(centroid, np.array(it[\"embedding\"], dtype=np.float32))\n",
    "    items.sort(key=lambda x: x.get(\"score\",0.0), reverse=True)\n",
    "    return items[:top_k]\n",
    "\n",
    "def pg_multi_search(query: str, collections: List[str]) -> List[Dict[str,Any]]:\n",
    "    hits = []\n",
    "    for coll in collections:\n",
    "        try:\n",
    "            hits.extend(pg_search_hybrid(coll, query, 24, 8))\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ PG search failed for {coll}: {e}\")\n",
    "    hits.sort(key=lambda x: x.get(\"score\",0.0), reverse=True)\n",
    "    return hits[: max(6, len(collections)) ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bd29ed",
   "metadata": {},
   "source": [
    "### PROD Postgres setup + query + retriever pool (drop-in cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6dd50ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Simple PROD retriever (Bedrock Titan embeddings + Postgres, no gemini) ---\n",
    "import os, json, urllib.parse, re, time\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import numpy as np\n",
    "import psycopg\n",
    "from psycopg.rows import dict_row\n",
    "import boto3\n",
    "\n",
    "# ------------ CONFIG ------------\n",
    "# PROD DB creds (URL-encode password)\n",
    "PG_PROD_USER = \"v_svc_usr_aidb\"\n",
    "PG_PROD_PASSWORD_RAW = \"j<pW@qNsFIc!(OR\"\n",
    "PG_PROD_PASSWORD = urllib.parse.quote(PG_PROD_PASSWORD_RAW, safe=\"\")\n",
    "PG_PROD_HOST = \"elysiadb.iris.informa.com\"\n",
    "PG_PROD_PORT = 5432\n",
    "PG_PROD_DB   = \"aidb\"\n",
    "PG_PROD_DSN  = f\"postgresql://{PG_PROD_USER}:{PG_PROD_PASSWORD}@{PG_PROD_HOST}:{PG_PROD_PORT}/{PG_PROD_DB}?sslmode=require\"\n",
    "\n",
    "# Bedrock embeddings model + region\n",
    "BEDROCK_REGION = os.getenv(\"AWS_EMBEDDING_BEDROCK_REGION\") or os.getenv(\"AWS_BEDROCK_REGION\") or os.getenv(\"AWS_REGION\") or \"us-east-1\"\n",
    "EMBED_MODEL_ID = os.getenv(\"BEDROCK_EMBEDDING_MODEL\", \"amazon.titan-embed-text-v2:0\")\n",
    "if EMBED_MODEL_ID.startswith((\"us.\", \"eu.\")):  # normalize accidental region prefix\n",
    "    EMBED_MODEL_ID = EMBED_MODEL_ID.split(\".\", 1)[1]\n",
    "\n",
    "# Default collection to search (tune as needed)\n",
    "DEFAULT_COLLECTION = \"internal_curated_informa_vectorstore\"\n",
    "\n",
    "# ------------ DB ------------\n",
    "def get_pg_conn():\n",
    "    return psycopg.connect(PG_PROD_DSN, row_factory=dict_row)\n",
    "\n",
    "# Keyword prefilter to keep latency low; then we re-rank by cosine\n",
    "KEYWORD_PREFILTER_SQL = \"\"\"\n",
    "SELECT e.uuid AS id,\n",
    "       e.embedding,\n",
    "       e.document,\n",
    "       e.cmetadata,\n",
    "       c.name as collection\n",
    "FROM ai.langchain_pg_embedding e\n",
    "JOIN ai.langchain_pg_collection c ON c.uuid = e.collection_id\n",
    "WHERE c.name = %(collection)s\n",
    "  AND (\n",
    "        e.document ILIKE '%%' || %(q)s || '%%'\n",
    "        OR CAST(e.cmetadata AS TEXT) ILIKE '%%' || %(q)s || '%%'\n",
    "      )\n",
    "LIMIT %(k)s;\n",
    "\"\"\"\n",
    "\n",
    "# ------------ Embeddings ------------\n",
    "_bedrock_rt = boto3.client(\"bedrock-runtime\", region_name=BEDROCK_REGION)\n",
    "\n",
    "def embed_query(text: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns a float32 numpy vector for the query using Titan Embeddings v2.\n",
    "    \"\"\"\n",
    "    # Titan v2 expects {\"inputText\": \"...\"} or {\"texts\":[...]} depending on version; v2:0 supports \"inputText\"\n",
    "    body = {\"inputText\": text}\n",
    "    resp = _bedrock_rt.invoke_model(\n",
    "        modelId=EMBED_MODEL_ID,\n",
    "        body=json.dumps(body),\n",
    "        accept=\"application/json\",\n",
    "        contentType=\"application/json\",\n",
    "    )\n",
    "    payload = json.loads(resp[\"body\"].read().decode(\"utf-8\"))\n",
    "    # Titan returns {\"embedding\": [...]} or {\"embeddings\":[...]} depending on variant; handle both\n",
    "    vec = payload.get(\"embedding\") or (payload.get(\"embeddings\")[0] if payload.get(\"embeddings\") else None)\n",
    "    if not vec:\n",
    "        raise RuntimeError(f\"Unexpected Titan embedding payload: {payload.keys()}\")\n",
    "    return np.asarray(vec, dtype=np.float32)\n",
    "\n",
    "def _cosine(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    denom = (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    if denom == 0: return 0.0\n",
    "    return float(np.dot(a, b) / denom)\n",
    "\n",
    "# ------------ Retriever ------------\n",
    "def retrieve_informa_context(\n",
    "    query: str,\n",
    "    collection: str = DEFAULT_COLLECTION,\n",
    "    pre_k: int = 48,\n",
    "    top_k: int = 8,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    1) Keyword prefilter in Postgres (fast)\n",
    "    2) Rank by cosine similarity to Titan embedding of query (precise)\n",
    "    Returns top_k items: [{id, score, document, metadata, collection}, ...]\n",
    "    \"\"\"\n",
    "    # 1) prefilter\n",
    "    with get_pg_conn() as conn, conn.cursor() as cur:\n",
    "        cur.execute(KEYWORD_PREFILTER_SQL, {\"collection\": collection, \"q\": query, \"k\": pre_k})\n",
    "        rows = cur.fetchall()\n",
    "\n",
    "    if not rows:\n",
    "        return []\n",
    "\n",
    "    # 2) embed query and rank\n",
    "    qvec = embed_query(query)\n",
    "    items = []\n",
    "    for r in rows:\n",
    "        emb = np.asarray(r[\"embedding\"], dtype=np.float32)\n",
    "        score = _cosine(qvec, emb)\n",
    "        items.append({\n",
    "            \"id\": r[\"id\"],\n",
    "            \"score\": score,\n",
    "            \"document\": r[\"document\"],\n",
    "            \"metadata\": r[\"cmetadata\"],\n",
    "            \"collection\": r[\"collection\"],\n",
    "        })\n",
    "    items.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "    return items[:top_k]\n",
    "\n",
    "def retrieve_text_snippets(query: str, collection: str = DEFAULT_COLLECTION, k: int = 8, max_chars: int = 1200) -> List[str]:\n",
    "    \"\"\"\n",
    "    Convenience: returns trimmed text snippets to drop into your LLM prompt.\n",
    "    \"\"\"\n",
    "    hits = retrieve_informa_context(query, collection=collection, pre_k=max(24, k*6), top_k=k)\n",
    "    out = []\n",
    "    for h in hits:\n",
    "        doc = h[\"document\"] or \"\"\n",
    "        out.append(doc if len(doc) <= max_chars else doc[:max_chars] + \"…\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb26eb4",
   "metadata": {},
   "source": [
    "## 2) AWS Knowledge Bases Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5bfef6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import boto3\n",
    "try:\n",
    "    kb_rt = boto3.client(\"bedrock-agent-runtime\", region_name=AWS_REGION) if (JOB_KB_ID or COURSES_KB_ID) else None\n",
    "except Exception as e:\n",
    "    kb_rt = None\n",
    "    print(\"⚠️ AWS KB unavailable:\", e)\n",
    "\n",
    "def kb_retrieve(kb_id: str, query: str, top_k: int = 5) -> List[Dict[str,Any]]:\n",
    "    if not kb_rt or not kb_id:\n",
    "        return []\n",
    "    try:\n",
    "        resp = kb_rt.retrieve(\n",
    "            knowledgeBaseId=kb_id,\n",
    "            retrievalConfiguration={\"vectorSearchConfiguration\": {\"numberOfResults\": top_k}},\n",
    "            retrievalQuery={\"text\": query},\n",
    "        )\n",
    "        out = []\n",
    "        for r in resp.get(\"retrievalResults\", []):\n",
    "            c = r.get(\"content\", {})\n",
    "            out.append({\n",
    "                \"title\": c.get(\"title\") or (c.get(\"text\",\"\").split(\"\\n\")[0][:80]).strip(),\n",
    "                \"snippet\": c.get(\"snippetText\") or c.get(\"text\",\"\")[:240],\n",
    "                \"score\": r.get(\"score\"),\n",
    "                \"kb_id\": kb_id,\n",
    "                \"metadata\": r.get(\"metadata\") or {},\n",
    "                \"source\": r.get(\"location\", {}).get(\"s3Location\", {}).get(\"uri\"),\n",
    "                \"type\": r.get(\"metadata\",{}).get(\"type\")\n",
    "            })\n",
    "        return out\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ KB retrieve failed:\", e)\n",
    "        return []\n",
    "\n",
    "def kb_search_all(query: str) -> Dict[str, List[Dict[str,Any]]]:\n",
    "    return {\n",
    "        \"jobs\":    kb_retrieve(JOB_KB_ID, query, 6) if JOB_KB_ID else [],\n",
    "        \"courses\": kb_retrieve(COURSES_KB_ID, query, 6) if COURSES_KB_ID else [],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bc6410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4ef4d68",
   "metadata": {},
   "source": [
    "## 3) Prohibitor, State, Intent, Profile Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9512bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "AllowedIntents = {\"courses\",\"job\",\"development_plan\",\"manager_toolkit\",\"leadership_strategy\",\"career\"}\n",
    "\n",
    "def prohibitor(user_text: str) -> Dict[str,Any]:\n",
    "    t = user_text.lower()\n",
    "    allowed = any(k in t for k in [\"career\",\"course\",\"job\",\"role\",\"roles\",\"learn\",\"upskill\",\"development\",\"manager\",\"leadership\",\"okr\",\"coaching\",\"promotion\",\"ladder\",\"mentoring\",\"objective\",\"okrs\"])\n",
    "    intents = []\n",
    "    if any(k in t for k in [\"job\",\"jobs\",\"opening\",\"openings\",\"role\",\"roles\"]): intents.append(\"job\")\n",
    "    if any(k in t for k in [\"course\",\"courses\",\"learn\",\"training\",\"upskill\"]): intents.append(\"courses\")\n",
    "    if any(k in t for k in [\"mentoring\",\"mentor\"]): intents.append(\"manager_toolkit\")\n",
    "    if any(k in t for k in [\"objective\",\"okr\",\"okrs\"]): intents.append(\"leadership_strategy\")\n",
    "    if any(k in t for k in [\"development plan\",\"30-day\",\"60-day\",\"90-day\",\"dev plan\"]): intents.append(\"development_plan\")\n",
    "    if not intents and allowed: intents.append(\"career\")\n",
    "    return {\"allowed\": allowed and bool(intents), \"intents\": intents or [], \"rationale\": \"heuristic v0.3\"}\n",
    "\n",
    "@dataclass\n",
    "class AgentState:\n",
    "    email: Optional[str] = None\n",
    "    name: Optional[str] = None\n",
    "    division: Optional[str] = None\n",
    "    employee_id: Optional[str] = None\n",
    "    is_manager: bool = False\n",
    "    prompt: Optional[str] = None\n",
    "    quick_profile: Optional[Dict[str,Any]] = None\n",
    "\n",
    "def derive_is_manager_from_profile(meta: dict) -> bool:\n",
    "    if str(meta.get(\"is_manager\",\"\")).lower() in {\"true\",\"1\",\"yes\"}: return True\n",
    "    if int(meta.get(\"direct_reports\",0) or 0) > 0: return True\n",
    "    title = (meta.get(\"title\") or \"\").lower()\n",
    "    if any(k in title for k in [\" manager\",\"lead\",\"head of\",\"director\",\"vp\"]): return True\n",
    "    return False\n",
    "\n",
    "def profile_lookup(email: Optional[str] = None,\n",
    "                   name: Optional[str] = None,\n",
    "                   division: Optional[str] = None) -> List[Dict[str,Any]]:\n",
    "    if not PG_DSN:\n",
    "        return []\n",
    "    results: List[Dict[str,Any]] = []\n",
    "    with get_pg_conn() as conn, conn.cursor() as cur:\n",
    "        if email:\n",
    "            sql = \"\"\"\n",
    "                SELECT e.document, e.cmetadata\n",
    "                FROM ai.langchain_pg_embedding e\n",
    "                JOIN ai.langchain_pg_collection c ON c.uuid = e.collection_id\n",
    "                WHERE c.name = 'internal_private_employee_profiles_vectorstore'\n",
    "                  AND (e.cmetadata->>'email') = %(email)s\n",
    "                LIMIT 10;\n",
    "            \"\"\"\n",
    "            cur.execute(sql, {\"email\": email})\n",
    "            rows = cur.fetchall()\n",
    "            for doc, meta in rows:\n",
    "                try: meta = meta if isinstance(meta, dict) else json.loads(meta)\n",
    "                except: meta = {\"raw\": str(meta)}\n",
    "                results.append({\"document\": doc, \"metadata\": meta})\n",
    "            if results:\n",
    "                return results\n",
    "\n",
    "        if name:\n",
    "            sql = \"\"\"\n",
    "                SELECT e.document, e.cmetadata\n",
    "                FROM ai.langchain_pg_embedding e\n",
    "                JOIN ai.langchain_pg_collection c ON c.uuid = e.collection_id\n",
    "                WHERE c.name = 'internal_private_employee_profiles_vectorstore'\n",
    "                  AND (e.cmetadata->>'name') ILIKE %(name)s\n",
    "            \"\"\"\n",
    "            params = {\"name\": f\"%{name}%\"}\n",
    "            if division:\n",
    "                sql += \" AND (e.cmetadata->>'division') ILIKE %(division)s\"\n",
    "                params[\"division\"] = f\"%{division}%\"\n",
    "            sql += \" LIMIT 25;\"\n",
    "            cur.execute(sql, params)\n",
    "            rows = cur.fetchall()\n",
    "            for doc, meta in rows:\n",
    "                try: meta = meta if isinstance(meta, dict) else json.loads(meta)\n",
    "                except: meta = {\"raw\": str(meta)}\n",
    "                results.append({\"document\": doc, \"metadata\": meta})\n",
    "        return results\n",
    "\n",
    "def setup_state(email: Optional[str], name: Optional[str], division: Optional[str],\n",
    "                override_is_manager: Optional[bool], user_text: str) -> Tuple[AgentState, dict]:\n",
    "    rows = profile_lookup(email=email or DEFAULT_USER_EMAIL,\n",
    "                          name=name or DEFAULT_USER_NAME,\n",
    "                          division=division or DEFAULT_USER_DIV)\n",
    "    meta = rows[0][\"metadata\"] if rows else {}\n",
    "    is_mgr = override_is_manager if override_is_manager is not None else derive_is_manager_from_profile(meta)\n",
    "    st = AgentState(email=email or DEFAULT_USER_EMAIL, name=name or DEFAULT_USER_NAME,\n",
    "                    division=division or DEFAULT_USER_DIV, employee_id=meta.get(\"employee_id\"),\n",
    "                    is_manager=is_mgr, prompt=user_text)\n",
    "    st.quick_profile = {\"_doc\": rows[0][\"document\"] if rows else \"\"}\n",
    "    return st, meta\n",
    "\n",
    "def intent_persona(intents: List[str]) -> List[str]:\n",
    "    return sorted(set(i for i in intents if i in AllowedIntents))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d368de6a",
   "metadata": {},
   "source": [
    "## 4) Parse Skills/Topics and Build Profile Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ca37b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'jobs': ['data engineering roles', 'data engineering jobs', 'Python engineer jobs', 'Data Engineer career paths'], 'courses': ['Python course', 'Python training', 'data engineering learning path']}\n"
     ]
    }
   ],
   "source": [
    "# HOTFIX: (re)define profile parsers + query builder in one place\n",
    "\n",
    "import re, json\n",
    "\n",
    "def extract_profile_fields(document: str, meta: dict) -> dict:\n",
    "    text = (document or \"\") + \"\\n\" + json.dumps(meta or {})\n",
    "    m_sk = re.search(r\"(?im)^\\s*-\\s*Skills:\\s*(.+)$\", text)\n",
    "    skills = [s.strip() for s in re.split(r\"[;,]\", m_sk.group(1)) if s.strip()] if m_sk else []\n",
    "\n",
    "    m_to = re.search(r\"(?im)^\\s*-\\s*Topics of Interest:\\s*(.+)$\", text)\n",
    "    topics = [s.strip() for s in re.split(r\"[;,]\", m_to.group(1)) if s.strip()] if m_to else []\n",
    "\n",
    "    title = (meta or {}).get(\"title\") or \"\"\n",
    "    if not title:\n",
    "        m_t = re.search(r\"(?im)^\\s*-\\s*Job Title:\\s*(.+)$\", text)\n",
    "        if m_t: title = m_t.group(1).strip()\n",
    "\n",
    "    name = (meta or {}).get(\"name\") or \"\"\n",
    "    if not name:\n",
    "        m_n = re.search(r\"(?im)^\\s*-\\s*Name:\\s*(.+)$\", text)\n",
    "        if m_n: name = m_n.group(1).strip()\n",
    "\n",
    "    return {\"name\": name, \"title\": title, \"skills\": skills, \"topics\": topics}\n",
    "\n",
    "def build_profile_queries(fields: dict, max_items: int = 5) -> dict:\n",
    "    skills = (fields.get(\"skills\") or [])[:max_items]\n",
    "    topics = (fields.get(\"topics\") or [])[:max_items]\n",
    "    role   = (fields.get(\"title\") or \"\")\n",
    "\n",
    "    job_q, crs_q = [], []\n",
    "\n",
    "    # Jobs queries from topics/skills/role\n",
    "    for t in topics:\n",
    "        job_q += [f\"{t} roles\", f\"{t} jobs\"]\n",
    "    for s in skills:\n",
    "        job_q.append(f\"{s} engineer jobs\")\n",
    "    if role:\n",
    "        job_q.append(f\"{role} career paths\")\n",
    "\n",
    "    # Courses queries from skills/topics\n",
    "    for s in skills:\n",
    "        crs_q += [f\"{s} course\", f\"{s} training\"]\n",
    "    for t in topics:\n",
    "        crs_q.append(f\"{t} learning path\")  # correct append(...)\n",
    "\n",
    "    def dedup(seq):\n",
    "        seen, out = set(), []\n",
    "        for x in seq:\n",
    "            xl = x.lower()\n",
    "            if xl in seen: \n",
    "                continue\n",
    "            seen.add(xl); out.append(x)\n",
    "        return out\n",
    "\n",
    "    return {\"jobs\": dedup(job_q)[:max_items], \"courses\": dedup(crs_q)[:max_items]}\n",
    "\n",
    "# quick sanity check\n",
    "print(build_profile_queries({\"skills\":[\"Python\"], \"topics\":[\"data engineering\"], \"title\":\"Data Engineer\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb1656b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_profile_queries(fields: dict, max_items: int = 5) -> dict:\n",
    "    skills = (fields.get(\"skills\") or [])[:max_items]\n",
    "    topics = (fields.get(\"topics\") or [])[:max_items]\n",
    "    role   = (fields.get(\"title\") or \"\")\n",
    "\n",
    "    job_q, crs_q = [], []\n",
    "\n",
    "    # Jobs queries from topics/skills/role\n",
    "    for t in topics:\n",
    "        job_q += [f\"{t} roles\", f\"{t} jobs\"]\n",
    "    for s in skills:\n",
    "        job_q.append(f\"{s} engineer jobs\")\n",
    "    if role:\n",
    "        job_q.append(f\"{role} career paths\")\n",
    "\n",
    "    # Courses queries from skills/topics\n",
    "    for s in skills:\n",
    "        crs_q += [f\"{s} course\", f\"{s} training\"]\n",
    "    for t in topics:\n",
    "        crs_q.append(f\"{t} learning path\")  # <-- fixed: append(...), not append[...]\n",
    "\n",
    "    def dedup(seq):\n",
    "        seen = set(); out = []\n",
    "        for x in seq:\n",
    "            xl = x.lower()\n",
    "            if xl in seen: \n",
    "                continue\n",
    "            seen.add(xl); out.append(x)\n",
    "        return out\n",
    "\n",
    "    return {\"jobs\": dedup(job_q)[:max_items], \"courses\": dedup(crs_q)[:max_items]}\n",
    "\n",
    "def build_profile_queries(fields: dict, max_items: int = 5) -> dict:\n",
    "    skills = fields.get(\"skills\", [])[:max_items]\n",
    "    topics = fields.get(\"topics\", [])[:max_items]\n",
    "    role   = fields.get(\"title\") or \"\"\n",
    "\n",
    "    job_q, crs_q = [], []\n",
    "\n",
    "    for t in topics:\n",
    "        job_q += [f\"{t} roles\", f\"{t} jobs\"]\n",
    "    for s in skills:\n",
    "        job_q.append(f\"{s} engineer jobs\")\n",
    "    if role:\n",
    "        job_q.append(f\"{role} career paths\")\n",
    "\n",
    "    for s in skills:\n",
    "        crs_q += [f\"{s} course\", f\"{s} training\"]\n",
    "    for t in topics:\n",
    "        crs_q.append[f\"{t} learning path\"]\n",
    "\n",
    "    def dedup(seq):\n",
    "        seen=set(); out=[]\n",
    "        for x in seq:\n",
    "            xl=x.lower()\n",
    "            if xl in seen: continue\n",
    "            seen.add(xl); out.append(x)\n",
    "        return out\n",
    "\n",
    "    return {\"jobs\": dedup(job_q)[:max_items], \"courses\": dedup(crs_q)[:max_items]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834f718b",
   "metadata": {},
   "source": [
    "## 5) Tools (PG + KB) with Type Guessing, Profile Boost & Fallbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "79df5756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'jobs': ['data engineering roles', 'data engineering jobs', 'Python engineer jobs', 'Data Engineer career paths'], 'courses': ['Python course', 'Python training', 'data engineering learning path']}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "COURSE_HINTS = [\n",
    "    \"course\",\"training\",\"learning path\",\"module\",\"curriculum\",\n",
    "    \"cert\",\"certification\",\"udemy\",\"coursera\",\"pluralsight\",\"lynda\",\n",
    "    \"academy\",\"lesson\",\"workshop\"\n",
    "]\n",
    "JOB_HINTS = [\n",
    "    \"job\",\"role\",\"opening\",\"position\",\"vacancy\",\"requisition\",\"req id\",\n",
    "    \"hiring\"\n",
    "]\n",
    "\n",
    "def guess_type(item: dict) -> str:\n",
    "    meta = (item.get(\"metadata\") or {})\n",
    "    t = str(meta.get(\"type\") or \"\").lower().strip()\n",
    "    if t:\n",
    "        return t\n",
    "    title = (item.get(\"title\") or \"\").lower()\n",
    "    doc = (item.get(\"document\") or \"\").lower()\n",
    "    text = f\"{title} {doc}\"\n",
    "    if any(h in text for h in COURSE_HINTS): return \"course\"\n",
    "    if any(h in text for h in JOB_HINTS): return \"job\"\n",
    "    coll = (item.get(\"collection\") or \"\").lower()\n",
    "    if \"course\" in coll: return \"course\"\n",
    "    if \"job\" in coll or \"role\" in coll: return \"job\"\n",
    "    return \"unknown\"\n",
    "\n",
    "def tool_pg_search(query: str, k: int = 8) -> List[Dict[str,Any]]:\n",
    "    return pg_multi_search(query, PG_COLLECTIONS)[:k]\n",
    "\n",
    "def tool_kb_search(query: str, top_k: int = 6) -> Dict[str, List[Dict[str,Any]]]:\n",
    "    return kb_search_all(query)\n",
    "\n",
    "MANAGER_KEYWORDS = {\"manager\",\"leadership\",\"org design\",\"hiring\",\"coaching\",\"performance review\",\"okr\",\"okrs\",\"succession\"}\n",
    "\n",
    "def looks_manager_only(item: Dict[str,Any]) -> bool:\n",
    "    meta = (item.get(\"metadata\") or {})\n",
    "    audience = str(meta.get(\"audience\",\"\")).lower()\n",
    "    title = (item.get(\"title\") or item.get(\"document\") or \"\").lower()\n",
    "    tags = \" \".join(meta.get(\"tags\", [])).lower()\n",
    "    if audience in {\"manager\",\"leadership\"}: return True\n",
    "    haystack = f\"{title} {tags}\"\n",
    "    return any(kw in haystack for kw in MANAGER_KEYWORDS)\n",
    "\n",
    "def explicit_manager_request(prompt: str) -> bool:\n",
    "    p = (prompt or \"\").lower()\n",
    "    return any(k in p for k in MANAGER_KEYWORDS)\n",
    "\n",
    "FALLBACKS = {\n",
    "    \"data engineering\": {\n",
    "        \"jobs\": [\n",
    "            {\"title\": \"Data Engineer (Platform)\"},\n",
    "            {\"title\": \"Analytics Engineer\"},\n",
    "            {\"title\": \"Data Engineer — ETL & Pipelines\"},\n",
    "        ],\n",
    "        \"courses\": [\n",
    "            {\"title\": \"Data Engineering on AWS — Foundations\"},\n",
    "            {\"title\": \"Modern Data Pipelines with Python & Airflow\"},\n",
    "            {\"title\": \"Designing Data-Intensive Applications — Hands-on\"},\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "def infer_topic(user_text: str) -> Optional[str]:\n",
    "    t = user_text.lower()\n",
    "    if re.search(r\"\\bdata engineer(ing)?\\b\", t):\n",
    "        return \"data engineering\"\n",
    "    return None\n",
    "\n",
    "def _score_profile_alignment(title: str, fields: dict) -> float:\n",
    "    text = (title or \"\").lower()\n",
    "    bonus = 0.0\n",
    "    for s in (fields.get(\"skills\") or [])[:6]:\n",
    "        if s.lower() in text: bonus += 0.6\n",
    "    for t in (fields.get(\"topics\") or [])[:6]:\n",
    "        if t.lower() in text: bonus += 0.5\n",
    "    return bonus\n",
    "\n",
    "def _run_multi_queries(base_results: list, queries: list, fn_retrieve) -> list:\n",
    "    results = list(base_results)\n",
    "    for q in queries:\n",
    "        try:\n",
    "            results.extend(fn_retrieve(q))\n",
    "        except Exception as e:\n",
    "            print(\"⚠️ subquery failed:\", q, e)\n",
    "    return results\n",
    "\n",
    "def job_tool(query: str, profile_q: list = None, profile_fields: dict = None) -> List[Dict[str,Any]]:\n",
    "    profile_q = profile_q or []\n",
    "    profile_fields = profile_fields or {}\n",
    "\n",
    "    kb = tool_kb_search(query).get(\"jobs\", [])\n",
    "    pg_raw = tool_pg_search(query, 16)\n",
    "    pg = [h for h in pg_raw if guess_type(h) in {\"job\", \"role\"}]\n",
    "    jobs = kb[:8] + pg[:8]\n",
    "\n",
    "    if profile_q:\n",
    "        jobs = _run_multi_queries(jobs, profile_q, lambda q: (\n",
    "            tool_kb_search(q).get(\"jobs\", []) + \n",
    "            [h for h in tool_pg_search(q, 12) if guess_type(h) in {\"job\",\"role\"}]\n",
    "        ))\n",
    "\n",
    "    dedup = {}\n",
    "    for j in jobs:\n",
    "        title = (j.get(\"title\") or (j.get(\"metadata\") or {}).get(\"title\") or \"\").strip()\n",
    "        if not title: continue\n",
    "        key = title.lower()\n",
    "        score = float(j.get(\"score\") or 0.0) + _score_profile_alignment(title, profile_fields)\n",
    "        if key not in dedup or score > dedup[key][\"_score\"]:\n",
    "            jj = dict(j); jj[\"_score\"] = score; jj[\"title\"] = title\n",
    "            dedup[key] = jj\n",
    "\n",
    "    ranked = sorted(dedup.values(), key=lambda x: -x[\"_score\"])\n",
    "    if not ranked:\n",
    "        topic = infer_topic(query)\n",
    "        if topic and FALLBACKS.get(topic, {}).get(\"jobs\"):\n",
    "            ranked = FALLBACKS[topic][\"jobs\"]\n",
    "        else:\n",
    "            ranked = [{\"title\": \"Data Engineer (Platform)\"}, {\"title\": \"Analytics Engineer\"}]\n",
    "    return ranked[:4]\n",
    "\n",
    "def courses_tool(query: str, state: 'AgentState', profile_q: list = None, profile_fields: dict = None) -> List[Dict[str,Any]]:\n",
    "    profile_q = profile_q or []\n",
    "    profile_fields = profile_fields or {}\n",
    "\n",
    "    kb = tool_kb_search(query).get(\"courses\", [])\n",
    "    pg_raw = tool_pg_search(query, 16)\n",
    "    pg = [h for h in pg_raw if guess_type(h) == \"course\"]\n",
    "    courses = kb[:10] + pg[:8]\n",
    "\n",
    "    if profile_q:\n",
    "        courses = _run_multi_queries(courses, profile_q, lambda q: (\n",
    "            tool_kb_search(q).get(\"courses\", []) + \n",
    "            [h for h in tool_pg_search(q, 12) if guess_type(h) == \"course\"]\n",
    "        ))\n",
    "\n",
    "    if state.is_manager or explicit_manager_request(state.prompt or \"\"):\n",
    "        filtered = courses\n",
    "    else:\n",
    "        filtered = [c for c in courses if not looks_manager_only(c)]\n",
    "\n",
    "    bucket = {}\n",
    "    for c in filtered:\n",
    "        title = (c.get(\"title\") or (c.get(\"metadata\") or {}).get(\"title\") or \"Course\").strip()\n",
    "        if not title: continue\n",
    "        key = title.lower()\n",
    "        score = float(c.get(\"score\") or 0.0) + _score_profile_alignment(title, profile_fields)\n",
    "        if key not in bucket or score > bucket[key][\"_score\"]:\n",
    "            cc = {\"title\": title, \"metadata\": c.get(\"metadata\") or {}, \"source\": c.get(\"source\") or \"KB/PG\", \"_score\": score}\n",
    "            bucket[key] = cc\n",
    "\n",
    "    ranked = sorted(bucket.values(), key=lambda x: -x[\"_score\"])\n",
    "    if not ranked:\n",
    "        topic = infer_topic(query)\n",
    "        if topic and FALLBACKS.get(topic, {}).get(\"courses\"):\n",
    "            ranked = FALLBACKS[topic][\"courses\"]\n",
    "        else:\n",
    "            ranked = [{\"title\": \"Data Engineering on AWS — Foundations\"},\n",
    "                      {\"title\": \"Modern Data Pipelines with Python & Airflow\"}]\n",
    "    return ranked[:4]\n",
    "\n",
    "def job_reflexion(items: List[Dict[str,Any]]) -> List[Dict[str,Any]]:\n",
    "    return sorted(items, key=lambda x: (-float(x.get(\"score\") or x.get(\"_score\") or 0.0), len((x.get(\"title\") or \"\"))))\n",
    "\n",
    "def courses_reflexion(items: List[Dict[str,Any]], is_manager: bool) -> List[Dict[str,Any]]:\n",
    "    def rank(it):\n",
    "        base = float(it.get(\"score\") or it.get(\"_score\") or 0.0)\n",
    "        meta = it.get(\"metadata\") or {}\n",
    "        aud = (meta.get(\"audience\") or \"\").lower()\n",
    "        penal = 0 if is_manager else (1 if aud in {\"manager\",\"leadership\"} else 0)\n",
    "        return (penal, -base)\n",
    "    return sorted(items, key=rank)\n",
    "\n",
    "\n",
    "# HOTFIX: override build_profile_queries everywhere\n",
    "def build_profile_queries(fields: dict, max_items: int = 5) -> dict:\n",
    "    skills = (fields.get(\"skills\") or [])[:max_items]\n",
    "    topics = (fields.get(\"topics\") or [])[:max_items]\n",
    "    role   = (fields.get(\"title\") or \"\")\n",
    "\n",
    "    job_q, crs_q = [], []\n",
    "\n",
    "    # Jobs queries from topics/skills/role\n",
    "    for t in topics:\n",
    "        job_q += [f\"{t} roles\", f\"{t} jobs\"]\n",
    "    for s in skills:\n",
    "        job_q.append(f\"{s} engineer jobs\")\n",
    "    if role:\n",
    "        job_q.append(f\"{role} career paths\")\n",
    "\n",
    "    # Courses queries from skills/topics\n",
    "    for s in skills:\n",
    "        crs_q += [f\"{s} course\", f\"{s} training\"]\n",
    "    for t in topics:\n",
    "        crs_q.append(f\"{t} learning path\")  # <- correct append(...)\n",
    "\n",
    "    def dedup(seq):\n",
    "        seen = set(); out = []\n",
    "        for x in seq:\n",
    "            xl = x.lower()\n",
    "            if xl in seen:\n",
    "                continue\n",
    "            seen.add(xl); out.append(x)\n",
    "        return out\n",
    "\n",
    "    return {\"jobs\": dedup(job_q)[:max_items], \"courses\": dedup(crs_q)[:max_items]}\n",
    "\n",
    "# sanity check\n",
    "_test = build_profile_queries({\"skills\":[\"Python\"], \"topics\":[\"data engineering\"], \"title\":\"Data Engineer\"})\n",
    "print(_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a91cba84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.6) Normalization + intersection/bridge ranking\n",
    "\n",
    "import re\n",
    "\n",
    "def _strip_md(s: str) -> str:\n",
    "    if not s: return \"\"\n",
    "    s = re.sub(r\"```[\\s\\S]*?```\", \"\", s)          # fenced blocks\n",
    "    s = re.sub(r\"\\[(.*?)\\]\\((.*?)\\)\", r\"\\1\", s)   # [text](url)\n",
    "    s = s.replace(\"**\",\"\").replace(\"__\",\"\")\n",
    "    s = re.sub(r\"^#+\\s*\", \"\", s)                  # heading hashes\n",
    "    return \" \".join(s.split())\n",
    "\n",
    "def normalize_item(item: dict) -> dict:\n",
    "    meta = item.get(\"metadata\") or {}\n",
    "    title = item.get(\"title\") or meta.get(\"title\") or item.get(\"document\",\"\")\n",
    "    title = _strip_md(title)[:160].strip() or \"Untitled\"\n",
    "    url   = meta.get(\"url\") or item.get(\"source\") or \"\"\n",
    "    audience = (meta.get(\"audience\") or \"\").lower()\n",
    "    tags  = meta.get(\"tags\") or []\n",
    "    typ   = (meta.get(\"type\") or \"\").lower()\n",
    "    score = float(item.get(\"score\") or item.get(\"_score\") or 0.0)\n",
    "    coll  = (item.get(\"collection\") or \"\").lower()\n",
    "    return {\n",
    "        \"title\": title, \"url\": url, \"audience\": audience,\n",
    "        \"tags\": tags, \"type\": typ, \"score\": score, \"collection\": coll\n",
    "    }\n",
    "\n",
    "# light keyword sets to detect target domain & bridge\n",
    "DE_KEYWORDS = {\"data engineer\",\"data engineering\",\"analytics engineer\",\"analytics engineering\",\"etl\",\"pipeline\",\"airflow\",\"spark\",\"dbt\",\"warehouse\",\"lakehouse\",\"bigquery\",\"redshift\",\"glue\"}\n",
    "MK_KEYWORDS = {\"marketing\",\"campaign\",\"crm\",\"email\",\"b2b\",\"b2c\",\"audience\",\"brand\",\"seo\",\"sem\",\"martech\",\"adtech\",\"attribution\",\"mql\",\"sql (sales)\"}\n",
    "\n",
    "def _kw_in(text: str, kws: set) -> bool:\n",
    "    t = text.lower()\n",
    "    return any(k in t for k in kws)\n",
    "\n",
    "def choose_candidates(user_text: str, items: list, profile_fields: dict, target=\"jobs\", top_n=6):\n",
    "    \"\"\"\n",
    "    Re-rank to surface intersection:\n",
    "    - Strongly prefer items that are Data Eng *and* Marketing (bridge).\n",
    "    - Then Data Eng only.\n",
    "    - Then Marketing analytics/BI (on-ramp).\n",
    "    - Penalize manager-only if user isn't a manager.\n",
    "    \"\"\"\n",
    "    txt = user_text.lower()\n",
    "    wants_de = _kw_in(txt, DE_KEYWORDS) or \"data\" in txt or \"engineer\" in txt\n",
    "\n",
    "    skills = [s.lower() for s in (profile_fields.get(\"skills\") or [])]\n",
    "    topics = [t.lower() for t in (profile_fields.get(\"topics\") or [])]\n",
    "    mk_like = any(_kw_in(s, MK_KEYWORDS) for s in skills+topics)\n",
    "\n",
    "    ranked = []\n",
    "    for raw in (items or []):\n",
    "        it = normalize_item(raw)\n",
    "        t = (it[\"title\"] or \"\").lower()\n",
    "        base = it[\"score\"]\n",
    "\n",
    "        is_de = _kw_in(t, DE_KEYWORDS)\n",
    "        is_mk = _kw_in(t, MK_KEYWORDS)\n",
    "\n",
    "        bridge = 0.0\n",
    "        if wants_de and mk_like:\n",
    "            # intersection/bridge bonuses\n",
    "            if is_de and is_mk:\n",
    "                bridge += 3.0\n",
    "            elif is_de:\n",
    "                bridge += 2.0\n",
    "            elif is_mk and any(x in t for x in [\"data\",\"analytics\",\"bi\",\"sql\",\"python\",\"warehouse\"]):\n",
    "                bridge += 1.2\n",
    "\n",
    "        # slight boost for explicit skill/topic mentions\n",
    "        for s in skills[:6]:\n",
    "            if s in t: base += 0.4\n",
    "        for tp in topics[:6]:\n",
    "            if tp in t: base += 0.3\n",
    "\n",
    "        # Final score\n",
    "        it[\"_rank\"] = base + bridge\n",
    "        ranked.append(it)\n",
    "\n",
    "    ranked.sort(key=lambda x: x[\"_rank\"], reverse=True)\n",
    "    # de-dup by title\n",
    "    seen = set(); out = []\n",
    "    for it in ranked:\n",
    "        key = it[\"title\"].lower()\n",
    "        if key in seen: continue\n",
    "        seen.add(key); out.append(it)\n",
    "        if len(out) >= top_n: break\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d81458",
   "metadata": {},
   "source": [
    "## 6) Compose & Orchestrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b81feae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Bedrock helpers (region + model id) ---\n",
    "import os, boto3\n",
    "\n",
    "def _normalize_bedrock_model_id(model: str) -> str:\n",
    "    # Strip accidental region prefixes like \"us.\" / \"eu.\"\n",
    "    # if model.startswith((\"us.\", \"eu.\")):\n",
    "    #     model = model.split(\".\", 1)[1]\n",
    "    alias = {\n",
    "        \"anthropic.claude-3-7-sonnet-20250219\": \"anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "        \"anthropic.claude-3-5-sonnet-20241022\": \"anthropic.claude-3-5-sonnet-20241022-v2:0\",\n",
    "        \"anthropic.claude-3-5-sonnet-20240620\": \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n",
    "    }\n",
    "    return alias.get(model, model)\n",
    "\n",
    "def _bedrock_client():\n",
    "    # FORCE region from AWS_BEDROCK_REGION (fallbacks to AWS_REGION, then us-east-1)\n",
    "    region = os.getenv(\"AWS_BEDROCK_REGION\") or os.getenv(\"AWS_REGION\") or \"us-east-1\"\n",
    "    return boto3.client(\"bedrock-runtime\", region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8a654a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.5) Pure LLM synthesis on Bedrock (Claude 3.7 Sonnet)\n",
    "\n",
    "import json, boto3\n",
    "\n",
    "try:\n",
    "    _bedrock = boto3.client(\"bedrock-runtime\", region_name=AWS_REGION)\n",
    "except Exception as _e:\n",
    "    _bedrock = None\n",
    "    print(\"⚠️ Bedrock runtime not available; set AWS creds/region to enable synthesis.\")\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are Informa’s internal career advisor. \"\n",
    "    \"Write naturally and concisely, tailored to the employee’s background and the question. \"\n",
    "    \"Explain tradeoffs, propose bridge steps if the target domain differs from the profile. \"\n",
    "    \"Use only facts provided; do not invent links or data.\"\n",
    ")\n",
    "\n",
    "def _compact(items):\n",
    "    out = []\n",
    "    for x in (items or []):\n",
    "        out.append({\n",
    "            \"title\": x.get(\"title\"),\n",
    "            \"url\": x.get(\"url\"),\n",
    "            \"audience\": x.get(\"audience\"),\n",
    "            \"tags\": x.get(\"tags\"),\n",
    "            \"score\": x.get(\"score\"),\n",
    "            \"collection\": x.get(\"collection\"),\n",
    "        })\n",
    "    return out[:8]\n",
    "\n",
    "def synthesize_answer_llm(user_text: str, intents: list, is_manager: bool,\n",
    "                          profile_fields: dict, sections: dict) -> str:\n",
    "    if not _bedrock:\n",
    "        raise RuntimeError(\"Bedrock not configured\")\n",
    "\n",
    "    # Prepare model-facing JSON (lean)\n",
    "    payload = {\n",
    "        \"query\": user_text,\n",
    "        \"intents\": intents,\n",
    "        \"persona\": {\"is_manager\": bool(is_manager)},\n",
    "        \"profile\": {\n",
    "            \"name\": profile_fields.get(\"name\"),\n",
    "            \"title\": profile_fields.get(\"title\"),\n",
    "            \"skills\": profile_fields.get(\"skills\") or [],\n",
    "            \"topics\": profile_fields.get(\"topics\") or [],\n",
    "        },\n",
    "        \"retrieval\": {\n",
    "            \"jobs\": _compact(sections.get(\"jobs\")),\n",
    "            \"courses\": _compact(sections.get(\"courses\")),\n",
    "            \"development_plan\": _compact(sections.get(\"development_plan\")),\n",
    "            \"manager_toolkit\": _compact(sections.get(\"manager_toolkit\")),\n",
    "            \"leadership_strategy\": _compact(sections.get(\"leadership_strategy\")),\n",
    "        }\n",
    "    }\n",
    "\n",
    "    user_msg = (\n",
    "        \"Using only this JSON, answer the user naturally. \"\n",
    "        \"Pick items that best fit the query and the profile; prefer intersection/bridge when needed. \"\n",
    "        \"If information is insufficient, ask for the minimum missing detail.\\n\\n\"\n",
    "        + json.dumps(payload, ensure_ascii=False)\n",
    "    )\n",
    "\n",
    "    body = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 700,\n",
    "        \"temperature\": 0.4,\n",
    "        \"system\": [{\"type\":\"text\",\"text\": SYSTEM_PROMPT}],   # <-- system at top-level\n",
    "        \"messages\": [\n",
    "            {\"role\":\"user\",\"content\":[{\"type\":\"text\",\"text\": user_msg}]}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    resp = _bedrock.invoke_model(\n",
    "        modelId=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "        body=json.dumps(body),\n",
    "        accept=\"application/json\",\n",
    "        contentType=\"application/json\",\n",
    "    )\n",
    "    out = json.loads(resp[\"body\"].read().decode(\"utf-8\"))\n",
    "    parts = out.get(\"content\", [])\n",
    "    text = \"\\n\".join(p.get(\"text\",\"\") for p in parts if p.get(\"type\")==\"text\").strip()\n",
    "    return text or \"(no content)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3ec5e9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6) Compose & Orchestrate\n",
    "\n",
    "def run_workflow(\n",
    "    user_text: str,\n",
    "    email: Optional[str] = None,\n",
    "    name: Optional[str] = None,\n",
    "    division: Optional[str] = None,\n",
    "    override_is_manager: Optional[bool] = None\n",
    ") -> Dict[str,Any]:\n",
    "\n",
    "    gate = prohibitor(user_text)\n",
    "    if not gate.get(\"allowed\"):\n",
    "        return {\"blocked\": True, \"gate\": gate, \"answer\": \"out_of_scope\"}\n",
    "\n",
    "    # Load state/profile\n",
    "    state, profile_meta = setup_state(\n",
    "        email=email, name=name, division=division,\n",
    "        override_is_manager=override_is_manager, user_text=user_text\n",
    "    )\n",
    "\n",
    "    # Parse profile\n",
    "    fields: Dict[str, Any] = {}\n",
    "    try:\n",
    "        if state.quick_profile and state.quick_profile.get(\"_doc\") is not None:\n",
    "            fields = extract_profile_fields(state.quick_profile[\"_doc\"], profile_meta or {})\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ profile parse failed:\", e)\n",
    "        fields = {}\n",
    "\n",
    "    # Intents & profile-driven queries\n",
    "    intents = intent_persona(gate.get(\"intents\", []))\n",
    "    profile_qs = build_profile_queries(fields) if fields else {\"jobs\": [], \"courses\": []}\n",
    "\n",
    "    # Raw tool retrieval\n",
    "    sections_raw: Dict[str, Any] = {}\n",
    "    if \"job\" in intents:\n",
    "        sections_raw[\"jobs\"] = job_reflexion(\n",
    "            job_tool(user_text, profile_q=profile_qs.get(\"jobs\") or [], profile_fields=fields)\n",
    "        )\n",
    "    if \"courses\" in intents:\n",
    "        sections_raw[\"courses\"] = courses_reflexion(\n",
    "            courses_tool(user_text, state, profile_q=profile_qs.get(\"courses\") or [], profile_fields=fields),\n",
    "            state.is_manager\n",
    "        )\n",
    "    if \"development_plan\" in intents:\n",
    "        sections_raw[\"development_plan\"] = tool_pg_search(\"development plan \" + (user_text or \"\"), 6)[:5]\n",
    "    if \"manager_toolkit\" in intents:\n",
    "        sections_raw[\"manager_toolkit\"] = tool_pg_search(\"manager coaching \" + (user_text or \"\"), 6)[:5]\n",
    "    if \"leadership_strategy\" in intents:\n",
    "        sections_raw[\"leadership_strategy\"] = tool_pg_search(\"capability gaps portfolio \" + (user_text or \"\"), 6)[:5]\n",
    "\n",
    "    # Intersection/bridge selection so LLM sees the right candidates\n",
    "    if \"job\" in intents:\n",
    "        sections_jobs = choose_candidates(user_text, sections_raw.get(\"jobs\"), fields, target=\"jobs\", top_n=6)\n",
    "    else:\n",
    "        sections_jobs = []\n",
    "    if \"courses\" in intents:\n",
    "        sections_courses = choose_candidates(user_text, sections_raw.get(\"courses\"), fields, target=\"courses\", top_n=6)\n",
    "    else:\n",
    "        sections_courses = []\n",
    "\n",
    "    sections = dict(sections_raw)  # keep other sections as-is\n",
    "    sections[\"jobs\"] = sections_jobs\n",
    "    sections[\"courses\"] = sections_courses\n",
    "\n",
    "    # LLM writes the final answer (no hardcoded copy)\n",
    "    try:\n",
    "        final = synthesize_answer_llm(\n",
    "            user_text=user_text,\n",
    "            intents=intents,\n",
    "            is_manager=state.is_manager,\n",
    "            profile_fields=fields or {},\n",
    "            sections=sections\n",
    "        )\n",
    "    except Exception as e:\n",
    "        final = f\"(LLM unavailable: {e})\"\n",
    "\n",
    "    return {\n",
    "        \"blocked\": False,\n",
    "        \"gate\": gate,\n",
    "        \"state\": state,\n",
    "        \"profile_found\": bool(profile_meta),\n",
    "        \"profile_fields\": fields,\n",
    "        \"sections\": sections,          # now already intersection-weighted\n",
    "        \"answer\": final\n",
    "    }\n",
    "\n",
    "# 6.7) Simple streaming renderer for notebooks\n",
    "\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "import time\n",
    "\n",
    "def render_stream(generator, refresh=0.05):\n",
    "    \"\"\"\n",
    "    Renders streaming text in-place. Call with the generator returned by synthesize_answer_llm_stream.\n",
    "    \"\"\"\n",
    "    buf = []\n",
    "    handle = display(Markdown(\"\"), display_id=True)\n",
    "    last_flush = time.time()\n",
    "    for chunk in generator:\n",
    "        buf.append(chunk)\n",
    "        if time.time() - last_flush >= refresh:\n",
    "            handle.update(Markdown(\"\".join(buf)))\n",
    "            last_flush = time.time()\n",
    "    # final flush\n",
    "    handle.update(Markdown(\"\".join(buf)))\n",
    "    return \"\".join(buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "879f4611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.6) Streaming synthesis (Claude 3.7 Sonnet on Bedrock)\n",
    "# - Uses invoke_model_with_response_stream\n",
    "# - Yields text deltas as they arrive\n",
    "# - Falls back to non-streaming if not supported/enabled\n",
    "\n",
    "import json\n",
    "\n",
    "def _make_messages_body(user_text: str, intents: list, is_manager: bool, profile_fields: dict, sections: dict):\n",
    "    payload = {\n",
    "        \"query\": user_text,\n",
    "        \"intents\": intents,\n",
    "        \"persona\": {\"is_manager\": bool(is_manager)},\n",
    "        \"profile\": {\n",
    "            \"name\":  profile_fields.get(\"name\"),\n",
    "            \"title\": profile_fields.get(\"title\"),\n",
    "            \"skills\": profile_fields.get(\"skills\") or [],\n",
    "            \"topics\": profile_fields.get(\"topics\") or [],\n",
    "        },\n",
    "        \"retrieval\": {\n",
    "            \"jobs\":   [{\"title\": x.get(\"title\"), \"url\": x.get(\"url\")} for x in (sections.get(\"jobs\") or [])][:8],\n",
    "            \"courses\":[{\"title\": x.get(\"title\"), \"url\": x.get(\"url\")} for x in (sections.get(\"courses\") or [])][:8],\n",
    "            \"development_plan\":   [{\"title\": x.get(\"title\") or (x.get(\"metadata\") or {}).get(\"title\",\"\")} for x in (sections.get(\"development_plan\") or [])][:6],\n",
    "            \"manager_toolkit\":    [{\"title\": x.get(\"title\") or (x.get(\"metadata\") or {}).get(\"title\",\"\")} for x in (sections.get(\"manager_toolkit\")  or [])][:6],\n",
    "            \"leadership_strategy\":[{\"title\": x.get(\"title\") or (x.get(\"metadata\") or {}).get(\"title\",\"\")} for x in (sections.get(\"leadership_strategy\") or [])][:6],\n",
    "        }\n",
    "    }\n",
    "    SYSTEM_PROMPT = (\n",
    "        \"You are Informa’s internal career advisor. \"\n",
    "        \"Write naturally and concisely, tailored to the employee’s background and the question. \"\n",
    "        \"Prefer bridges when profile and target domain differ; pick only from provided facts; no invented links.\"\n",
    "    )\n",
    "    return {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 700,\n",
    "        \"temperature\": 0.4,\n",
    "        \"system\": [{\"type\":\"text\",\"text\": SYSTEM_PROMPT}],\n",
    "        \"messages\": [{\n",
    "            \"role\":\"user\",\n",
    "            \"content\":[{\"type\":\"text\",\"text\":\n",
    "                \"Using only this JSON, answer naturally. \"\n",
    "                \"Pick items that best fit the query and profile; prefer intersection/bridge when needed. \"\n",
    "                \"If info is insufficient, ask for the minimum missing detail.\\n\\n\"\n",
    "                + json.dumps(payload, ensure_ascii=False)}]\n",
    "        }]\n",
    "    }\n",
    "\n",
    "def synthesize_answer_llm_stream(\n",
    "    user_text: str,\n",
    "    intents: list,\n",
    "    is_manager: bool,\n",
    "    profile_fields: dict,\n",
    "    sections: dict,\n",
    "    model_id: str = None,\n",
    ") -> Iterator[str]:\n",
    "    \"\"\"\n",
    "    Streams text deltas using Bedrock Converse streaming.\n",
    "    Returns a generator of text chunks.\n",
    "    \"\"\"\n",
    "    br = _bedrock_client()\n",
    "    model_id = model_id or os.getenv(\"PRIMARY_LLM_MODEL_NAME\", \"anthropic.claude-3-7-sonnet-20250219-v1:0\")\n",
    "    model_id = _normalize_bedrock_model_id(model_id)\n",
    "\n",
    "    body = _make_messages_body(user_text, intents, is_manager, profile_fields, sections)\n",
    "\n",
    "    # Map Anthropic \"system\"/\"messages\" => Converse messages\n",
    "    conv_msgs: List[Dict] = []\n",
    "    for s in (body.get(\"system\") or []):\n",
    "        if s.get(\"type\") == \"text\":\n",
    "            conv_msgs.append({\"role\": \"system\", \"content\": [{\"text\": s[\"text\"]}]})\n",
    "    for m in body[\"messages\"]:\n",
    "        role = m.get(\"role\",\"user\")\n",
    "        text_parts = [c.get(\"text\",\"\") for c in m.get(\"content\",[]) if c.get(\"type\")==\"text\"]\n",
    "        conv_msgs.append({\"role\": role, \"content\": [{\"text\": \"\".join(text_parts)}]})\n",
    "\n",
    "    inference = {\"temperature\": body.get(\"temperature\", 0.4), \"maxTokens\": body.get(\"max_tokens\", 700)}\n",
    "\n",
    "    # Simple retry on throttling\n",
    "    attempts, backoff = 0, 0.5\n",
    "    while True:\n",
    "        try:\n",
    "            resp = br.converse_stream(\n",
    "                modelId=model_id,\n",
    "                messages=conv_msgs if conv_msgs else [{\"role\":\"user\",\"content\":[{\"text\":\"Hello\"}]}],\n",
    "                inferenceConfig=inference,\n",
    "            )\n",
    "            break\n",
    "        except br.exceptions.ThrottlingException:\n",
    "            attempts += 1\n",
    "            if attempts > 2:\n",
    "                raise\n",
    "            time.sleep(backoff)\n",
    "            backoff *= 2\n",
    "\n",
    "    stream = resp.get(\"stream\")\n",
    "    if not stream:\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        for event in stream:\n",
    "            if \"contentBlockDelta\" in event:\n",
    "                delta = event[\"contentBlockDelta\"][\"delta\"].get(\"text\")\n",
    "                if delta:\n",
    "                    yield delta\n",
    "            elif \"messageStop\" in event:\n",
    "                break\n",
    "    except Exception as e:\n",
    "        yield f\"\\n(Streaming error: {e})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0c189198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "import time\n",
    "\n",
    "def render_stream(generator, refresh=0.05, min_early_flush_chars=60) -> str:\n",
    "    \"\"\"\n",
    "    Renders streaming text in-place with low perceived latency.\n",
    "    \"\"\"\n",
    "    buf = []\n",
    "    handle = display(Markdown(\"\"), display_id=True)\n",
    "    last_flush = 0.0\n",
    "    early_flushed = False\n",
    "\n",
    "    def flush():\n",
    "        handle.update(Markdown(\"\".join(buf)))\n",
    "\n",
    "    for chunk in generator:\n",
    "        buf.append(chunk)\n",
    "\n",
    "        # Early first flush to show something quickly\n",
    "        if not early_flushed and sum(len(x) for x in buf) >= min_early_flush_chars:\n",
    "            flush()\n",
    "            early_flushed = True\n",
    "            last_flush = time.time()\n",
    "            continue\n",
    "\n",
    "        # Throttle updates to avoid Jupyter lag\n",
    "        now = time.time()\n",
    "        if (now - last_flush) >= refresh:\n",
    "            flush()\n",
    "            last_flush = now\n",
    "\n",
    "    # final flush\n",
    "    flush()\n",
    "    return \"\".join(buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "be640e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "import time\n",
    "\n",
    "def render_stream(generator, refresh=0.05, min_early_flush_chars=60) -> str:\n",
    "    \"\"\"\n",
    "    Renders streaming text in-place with low perceived latency.\n",
    "    \"\"\"\n",
    "    buf = []\n",
    "    handle = display(Markdown(\"\"), display_id=True)\n",
    "    last_flush = 0.0\n",
    "    early_flushed = False\n",
    "\n",
    "    def flush():\n",
    "        handle.update(Markdown(\"\".join(buf)))\n",
    "\n",
    "    for chunk in generator:\n",
    "        buf.append(chunk)\n",
    "\n",
    "        # Early first flush to show something quickly\n",
    "        if not early_flushed and sum(len(x) for x in buf) >= min_early_flush_chars:\n",
    "            flush()\n",
    "            early_flushed = True\n",
    "            last_flush = time.time()\n",
    "            continue\n",
    "\n",
    "        # Throttle updates to avoid Jupyter lag\n",
    "        now = time.time()\n",
    "        if (now - last_flush) >= refresh:\n",
    "            flush()\n",
    "            last_flush = now\n",
    "\n",
    "    # final flush\n",
    "    flush()\n",
    "    return \"\".join(buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "28088128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "import os\n",
    "\n",
    "def run_workflow(\n",
    "    user_text: str,\n",
    "    email=None,\n",
    "    name=None,\n",
    "    division=None,\n",
    "    override_is_manager=None,\n",
    "    stream: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Orchestrates the request. Always computes shared state first, then branches on streaming.\n",
    "    Returns:\n",
    "      - stream=True  -> {\"stream\": <generator>, ...}\n",
    "      - stream=False -> {\"answer\": <str>, ...}\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 1) Gate & profile ---\n",
    "    # Detect intents (prefer your real analyzer if available)\n",
    "    try:\n",
    "        intents = detect_intents(user_text)  # your real function\n",
    "    except NameError:\n",
    "        # fallback: super simple heuristic\n",
    "        intents = []\n",
    "        t = user_text.lower()\n",
    "        if \"job\" in t: intents.append(\"job\")\n",
    "        if \"course\" in t: intents.append(\"courses\")\n",
    "        if not intents: intents = [\"general\"]\n",
    "\n",
    "    # Get profile fields (prefer your real fetcher)\n",
    "    try:\n",
    "        profile_fields = get_profile_fields(email=email, name=name, division=division)  # your function\n",
    "    except NameError:\n",
    "        profile_fields = {\n",
    "            \"name\": name,\n",
    "            \"title\": None,\n",
    "            \"skills\": [],\n",
    "            \"topics\": [],\n",
    "            # add anything else your prompt expects\n",
    "        }\n",
    "\n",
    "    # Resolve manager flag (override takes precedence, otherwise derive, otherwise default False)\n",
    "    if override_is_manager is not None:\n",
    "        is_manager = bool(override_is_manager)\n",
    "    else:\n",
    "        is_manager = bool(profile_fields.get(\"is_manager\", False))\n",
    "\n",
    "    # Minimal 'state' object if you use it downstream\n",
    "    state = SimpleNamespace(is_manager=is_manager)\n",
    "\n",
    "    # --- 2) Retrieval sections (jobs, courses, etc.) ---\n",
    "    try:\n",
    "        sections = retrieve_sections(intents=intents, profile_fields=profile_fields)  # your function\n",
    "    except NameError:\n",
    "        sections = {\n",
    "            \"jobs\": [],\n",
    "            \"courses\": [],\n",
    "            \"development_plan\": [],\n",
    "            \"manager_toolkit\": [],\n",
    "            \"leadership_strategy\": [],\n",
    "        }\n",
    "\n",
    "    # Whether we found a profile (toggle per your logic)\n",
    "    profile_found = bool(profile_fields.get(\"name\") or profile_fields.get(\"title\") or profile_fields.get(\"skills\"))\n",
    "\n",
    "    # --- 3) Branch: streaming vs non-streaming ---\n",
    "    if stream:\n",
    "        gen = synthesize_answer_llm_stream(\n",
    "            user_text=user_text,\n",
    "            intents=intents,\n",
    "            is_manager=is_manager,\n",
    "            profile_fields=profile_fields,\n",
    "            sections=sections,\n",
    "            model_id=os.getenv(\"PRIMARY_LLM_MODEL_NAME\", \"anthropic.claude-3-7-sonnet-20250219-v1:0\"),\n",
    "        )\n",
    "        return {\n",
    "            \"stream\": gen,\n",
    "            \"blocked\": False,\n",
    "            \"gate\": {\"intents\": intents},\n",
    "            \"state\": state,\n",
    "            \"profile_found\": profile_found,\n",
    "            \"profile_fields\": profile_fields,\n",
    "            \"sections\": sections,\n",
    "        }\n",
    "\n",
    "    # Non-stream fallback (make sure this function exists)\n",
    "    answer = synthesize_answer_llm(\n",
    "        user_text=user_text,\n",
    "        intents=intents,\n",
    "        is_manager=is_manager,\n",
    "        profile_fields=profile_fields,\n",
    "        sections=sections\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"blocked\": False,\n",
    "        \"gate\": {\"intents\": intents},\n",
    "        \"state\": state,\n",
    "        \"profile_found\": profile_found,\n",
    "        \"profile_fields\": profile_fields,\n",
    "        \"sections\": sections,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42013fdd",
   "metadata": {},
   "source": [
    "## 7) Smoke Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0d528d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Analyze my current skillset against Informa's digital transformation needs and recommend 5 specific learning opportunities to close these gaps.\"\n",
    "\n",
    "# Pull top snippets from Prod\n",
    "ctx_snippets = retrieve_text_snippets(query, collection=\"internal_curated_informa_vectorstore\", k=8)\n",
    "\n",
    "# Hand those into your LLM call (streaming or non-streaming)\n",
    "# Example: merge into your `sections` dict you've been passing to synthesize_answer_llm(_stream)\n",
    "sections = {\n",
    "    \"informa_strategy\": [{\"title\": f\"ctx#{i+1}\", \"snippet\": s} for i, s in enumerate(ctx_snippets)],\n",
    "    # ... keep your other sections too (jobs/courses/etc.) if you have them\n",
    "}\n",
    "\n",
    "# Then call your existing streaming function\n",
    "gen = synthesize_answer_llm_stream(\n",
    "    user_text=query,\n",
    "    intents=[\"informa_strategy\"],\n",
    "    is_manager=False,                    # or your detected flag\n",
    "    profile_fields={\"name\": None, \"title\": None, \"skills\": [], \"topics\": []},\n",
    "    sections=sections,\n",
    "    # model_id stays as your working Claude ID\n",
    ")\n",
    "\n",
    "rendered = render_stream(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "405f570a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---\n",
      "Q: Reset my laptop password | override_is_manager: None\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "I understand you need help with resetting your laptop password. This appears to be an IT support request rather than a career development question.\n",
       "\n",
       "As Informa's career advisor, I focus on helping with professional development, career paths, and skills growth. For technical support issues like password resets, you'll need to contact Informa's IT Help Desk directly.\n",
       "\n",
       "They should be able to assist you with resetting your laptop password through the proper authentication procedures. Would you like me to help you with any career-related questions instead?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intents: ['general'] | is_manager: False | profile_found: False\n",
      "[debug] profile fields: {'name': None, 'title': None, 'skills': [], 'topics': []}\n",
      "[debug] jobs: []\n",
      "[debug] courses: []\n",
      "\n",
      "---\n",
      "Q: Analyze my current skillset against Informa's digital transformation needs and recommend 5 specific learning opportunities to close these gaps. | override_is_manager: False\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "I'd be happy to analyze your skillset against Informa's digital transformation needs, but I notice I don't have any information about your current skills, role, or experience. \n",
       "\n",
       "To provide meaningful recommendations for learning opportunities that would help close gaps related to digital transformation, I'll need to know:\n",
       "\n",
       "1. What is your current role at Informa?\n",
       "2. What key skills do you currently possess (technical, business, leadership)?\n",
       "3. Are there specific areas of digital transformation you're interested in (data analytics, customer experience, process automation, etc.)?\n",
       "\n",
       "Once you share this information, I can provide targeted recommendations for learning opportunities that would help you develop skills aligned with Informa's digital transformation initiatives."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intents: ['general'] | is_manager: False | profile_found: False\n",
      "[debug] profile fields: {'name': None, 'title': None, 'skills': [], 'topics': []}\n",
      "[debug] jobs: []\n",
      "[debug] courses: []\n",
      "\n",
      "---\n",
      "Q: Create a 30-day plan to master machine learning with daily practice steps and metrics to track my progress within my current role at Informa. | override_is_manager: True\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# 30-Day Machine Learning Development Plan\n",
       "\n",
       "I'd be happy to create a 30-day plan to help you build machine learning skills within your current role. Since I don't have specific information about your current technical background or how ML might apply to your managerial responsibilities at Informa, I'll create a general framework that you can adjust based on your starting point.\n",
       "\n",
       "## Week 1: Foundations\n",
       "**Days 1-3: Basics & Setup**\n",
       "- Set up a Python environment with essential ML libraries\n",
       "- Complete 1-2 introductory ML tutorials daily (30-60 min)\n",
       "- Identify 2-3 potential use cases for ML in your team's work\n",
       "- **Metrics**: Environment ready, 3-5 tutorials completed, use cases documented\n",
       "\n",
       "**Days 4-7: Core Concepts**\n",
       "- Study one ML algorithm daily (linear regression, decision trees, etc.)\n",
       "- Apply each algorithm to a simple dataset\n",
       "- Connect with 1-2 data-focused colleagues at Informa\n",
       "- **Metrics**: Understanding of 4 algorithms, 3-4 practice implementations\n",
       "\n",
       "## Week 2: Application to Your Role\n",
       "**Days 8-10: Data Exploration**\n",
       "- Identify datasets relevant to your team's work\n",
       "- Practice data cleaning and preprocessing techniques\n",
       "- Create visualizations of your team's data\n",
       "- **Metrics**: 1-2 relevant datasets identified, 3+ visualizations created\n",
       "\n",
       "**Days 11-14: First Project**\n",
       "- Define a small ML project relevant to your management responsibilities\n",
       "- Build a simple predictive model\n",
       "- Document approach and results\n",
       "- **Metrics**: Project defined, initial model built with baseline accuracy\n",
       "\n",
       "## Week 3: Deepening Knowledge\n",
       "**Days 15-18: Advanced Techniques**\n",
       "- Study more complex algorithms and techniques\n",
       "- Improve your first project with new approaches\n",
       "- Read 1-2 ML case studies from your industry daily\n",
       "- **Metrics**: Project v2 with improved metrics, 5+ case studies reviewed\n",
       "\n",
       "**Days 19-21: Evaluation & Interpretation**\n",
       "- Learn model evaluation techniques\n",
       "- Practice explaining ML insights to non-technical stakeholders\n",
       "- Create a dashboard for your project results\n",
       "- **Metrics**: Evaluation framework documented, presentation draft created\n",
       "\n",
       "## Week 4: Integration & Application\n",
       "**Days 22-25: Operational Integration**\n",
       "- Explore how to integrate ML insights into decision processes\n",
       "- Document potential ML applications for your team\n",
       "- Create a simple workflow for regular model updates\n",
       "- **Metrics**: Integration plan documented, workflow tested\n",
       "\n",
       "**Days 26-30: Future Planning**\n",
       "- Identify next steps for continued learning\n",
       "- Plan a larger ML project with your team\n",
       "- Share learnings with colleagues\n",
       "- Create a 60-day follow-up plan\n",
       "- **Metrics**: Learning roadmap created, team project defined\n",
       "\n",
       "## Daily Habits Throughout\n",
       "- 30-60 minutes of hands-on coding/practice\n",
       "- 15-30 minutes of reading/theory\n",
       "- Brief reflection on how the day's learning applies to your role\n",
       "\n",
       "Would you like me to adjust this plan based on your specific technical background or management focus areas at Informa?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intents: ['general'] | is_manager: True | profile_found: False\n",
      "[debug] profile fields: {'name': None, 'title': None, 'skills': [], 'topics': []}\n",
      "[debug] jobs: []\n",
      "[debug] courses: []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tests = [\n",
    "    (\"Reset my laptop password\", None, None, None, None),\n",
    "    (\"Analyze my current skillset against Informa's digital transformation needs and recommend 5 specific learning opportunities to close these gaps.\", None, None, None, False),\n",
    "    (\"Create a 30-day plan to master machine learning with daily practice steps and metrics to track my progress within my current role at Informa.\", None, None, None, True),\n",
    "]\n",
    "\n",
    "for text, email, name, div, is_mgr in tests:\n",
    "    print(\"\\n---\\nQ:\", text, \"| override_is_manager:\", is_mgr)\n",
    "    out = run_workflow(text, email=email, name=name, division=div, override_is_manager=is_mgr)\n",
    "    if out.get(\"blocked\"):\n",
    "        print(\"BLOCKED:\", out[\"answer\"])\n",
    "    else:\n",
    "        display(Markdown(out[\"answer\"]))\n",
    "        print(\"intents:\", out[\"gate\"][\"intents\"], \"| is_manager:\", out[\"state\"].is_manager, \"| profile_found:\", out[\"profile_found\"])\n",
    "        print(\"[debug] profile fields:\", out.get(\"profile_fields\"))\n",
    "        print(\"[debug] jobs:\", [j.get(\"title\") for j in out.get(\"sections\",{}).get(\"jobs\",[])])\n",
    "        print(\"[debug] courses:\", [c.get(\"title\") for c in out.get(\"sections\",{}).get(\"courses\",[])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ee80d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e971b73c",
   "metadata": {},
   "source": [
    "## 🔗 Unified Retrieval (Prod) + Orchestration\n",
    "\n",
    "Adds unified retrieval to `run_workflow()` without changing your streaming code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e195456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Unified Retrieval + run_workflow override (keeps your streaming code intact) =====\n",
    "import os, json, time, urllib.parse\n",
    "from typing import List, Dict, Any\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import numpy as np\n",
    "import psycopg\n",
    "from psycopg.rows import dict_row\n",
    "import boto3\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "REGION = os.getenv('AWS_BEDROCK_REGION') or os.getenv('AWS_REGION') or 'us-east-1'\n",
    "\n",
    "# DB DSN: prefer PG_DSN env; else build from Prod creds you provided\n",
    "_PG_DSN = os.getenv('PG_DSN')\n",
    "if not _PG_DSN:\n",
    "    _pw_enc = urllib.parse.quote('j<pW@qNsFIc!(OR', safe='')\n",
    "    _PG_DSN = f'postgresql://v_svc_usr_aidb:{_pw_enc}@elysiadb.iris.informa.com:5432/aidb?sslmode=require'\n",
    "\n",
    "# AWS KB ids (override by env if present)\n",
    "JOB_KB_ID     = os.getenv('JOB_KB_ID', '9PFZZ5FEIF')\n",
    "COURSES_KB_ID = os.getenv('COURSES_KB_ID', 'DENPFPR7CR')\n",
    "\n",
    "# Vector collections (prod)\n",
    "COLL_INFORMA  = 'internal_curated_informa_vectorstore'\n",
    "COLL_PROFILE  = 'internal_private_employee_profiles_vectorstore'\n",
    "\n",
    "# Bedrock models\n",
    "CHAT_MODEL_ID  = os.getenv('PRIMARY_LLM_MODEL_NAME', 'anthropic.claude-3-7-sonnet-20250219-v1:0')\n",
    "EMBED_MODEL_ID = os.getenv('BEDROCK_EMBEDDING_MODEL', 'amazon.titan-embed-text-v2:0')\n",
    "if CHAT_MODEL_ID.startswith(('us.','eu.')):   CHAT_MODEL_ID  = CHAT_MODEL_ID.split('.', 1)[1]\n",
    "if EMBED_MODEL_ID.startswith(('us.','eu.')):  EMBED_MODEL_ID = EMBED_MODEL_ID.split('.', 1)[1]\n",
    "\n",
    "# ---------- AWS CLIENTS ----------\n",
    "_brt  = boto3.client('bedrock-runtime',       region_name=REGION)\n",
    "_bart = boto3.client('bedrock-agent-runtime', region_name=REGION)\n",
    "\n",
    "# ---------- DB ----------\n",
    "def _pg_conn():\n",
    "    return psycopg.connect(_PG_DSN, row_factory=dict_row)\n",
    "\n",
    "# ---------- Embeddings (Titan v2) ----------\n",
    "def _embed_text(text: str) -> np.ndarray:\n",
    "    body = {'inputText': text}\n",
    "    resp = _brt.invoke_model(\n",
    "        modelId=EMBED_MODEL_ID,\n",
    "        body=json.dumps(body),\n",
    "        accept='application/json',\n",
    "        contentType='application/json',\n",
    "    )\n",
    "    payload = json.loads(resp['body'].read().decode('utf-8'))\n",
    "    vec = payload.get('embedding') or (payload.get('embeddings')[0] if payload.get('embeddings') else None)\n",
    "    if not vec:\n",
    "        raise RuntimeError(f'Unexpected Titan embedding payload: keys={list(payload.keys())}')\n",
    "    return np.asarray(vec, dtype=np.float32)\n",
    "\n",
    "def _cosine(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    denom = (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    if denom == 0: return 0.0\n",
    "    return float(np.dot(a, b) / denom)\n",
    "\n",
    "# ---------- Vector retrieval (prefilter + rerank) ----------\n",
    "_PREFILTER_SQL = (\n",
    "    'SELECT e.uuid AS id, e.embedding, e.document, e.cmetadata, c.name as collection '\n",
    "    'FROM ai.langchain_pg_embedding e '\n",
    "    'JOIN ai.langchain_pg_collection c ON c.uuid = e.collection_id '\n",
    "    'WHERE c.name = %(collection)s '\n",
    "    '  AND ( e.document ILIKE ''%%'' || %(q)s || ''%%'' '\n",
    "    '        OR CAST(e.cmetadata AS TEXT) ILIKE ''%%'' || %(q)s || ''%%'' ) '\n",
    "    'LIMIT %(k)s;'\n",
    ")\n",
    "\n",
    "def _pg_retrieve(query: str, collection: str, pre_k: int = 48, top_k: int = 8) -> List[Dict[str, Any]]:\n",
    "    with _pg_conn() as conn, conn.cursor() as cur:\n",
    "        cur.execute(_PREFILTER_SQL, {'collection': collection, 'q': query, 'k': pre_k})\n",
    "        rows = cur.fetchall()\n",
    "    if not rows: return []\n",
    "    qvec = _embed_text(query)\n",
    "    items = []\n",
    "    for r in rows:\n",
    "        emb = np.asarray(r['embedding'], dtype=np.float32)\n",
    "        score = _cosine(qvec, emb)\n",
    "        items.append({\n",
    "            'id': r['id'], 'score': score,\n",
    "            'document': r['document'], 'metadata': r['cmetadata'], 'collection': r['collection']\n",
    "        })\n",
    "    items.sort(key=lambda x: x['score'], reverse=True)\n",
    "    return items[:top_k]\n",
    "\n",
    "def _pg_snippets(query: str, collection: str, k: int = 8, max_chars: int = 1200) -> List[str]:\n",
    "    hits = _pg_retrieve(query, collection, pre_k=max(24, k*6), top_k=k)\n",
    "    out = []\n",
    "    for h in hits:\n",
    "        doc = h['document'] or ''\n",
    "        out.append(doc if len(doc) <= max_chars else doc[:max_chars] + '…')\n",
    "    return out\n",
    "\n",
    "# ---------- AWS KB retrieval ----------\n",
    "def _kb_retrieve(kb_id: str, query: str, k: int = 6) -> List[str]:\n",
    "    try:\n",
    "        resp = _bart.retrieve(\n",
    "            knowledgeBaseId=kb_id,\n",
    "            retrievalQuery={'text': query},\n",
    "            retrievalConfiguration={'vectorSearchConfiguration': {'numberOfResults': k}},\n",
    "        )\n",
    "        out = []\n",
    "        for r in resp.get('retrievalResults', []):\n",
    "            content = r.get('content') or {}\n",
    "            txt = content.get('text') or ''\n",
    "            if not txt and 'metadata' in r and isinstance(r['metadata'], dict):\n",
    "                txt = r['metadata'].get('text') or r['metadata'].get('excerpt') or ''\n",
    "            if txt:\n",
    "                out.append(txt if len(txt) <= 1200 else txt[:1200] + '…')\n",
    "        return out\n",
    "    except Exception as e:\n",
    "        print(f'⚠️ KB({kb_id}) retrieval error: {e}')\n",
    "        return []\n",
    "\n",
    "# ---------- Profile fields (best-effort) ----------\n",
    "def _get_profile_fields(email=None, name=None, division=None) -> Dict[str, Any]:\n",
    "    # Prefer your existing helper if present\n",
    "    try:\n",
    "        return get_profile_fields(email=email, name=name, division=division)  # type: ignore[name-defined]\n",
    "    except Exception:\n",
    "        return {'name': name, 'title': None, 'skills': [], 'topics': []}\n",
    "\n",
    "def _detect_intents(user_text: str):\n",
    "    try:\n",
    "        return detect_intents(user_text)  # type: ignore[name-defined]\n",
    "    except Exception:\n",
    "        t = user_text.lower()\n",
    "        out = []\n",
    "        if 'job' in t: out.append('jobs')\n",
    "        if 'course' in t: out.append('courses')\n",
    "        if 'skill' in t: out.append('skills')\n",
    "        if not out: out = ['general']\n",
    "        return out\n",
    "\n",
    "# ---------- run_workflow (override) ----------\n",
    "def run_workflow(\n",
    "    user_text: str,\n",
    "    email=None, name=None, division=None,\n",
    "    override_is_manager=None,\n",
    "    stream: bool = False,\n",
    "):\n",
    "    # 1) profile + state\n",
    "    profile_fields = _get_profile_fields(email=email, name=name, division=division)\n",
    "    is_manager = bool(override_is_manager) if override_is_manager is not None else bool(profile_fields.get('is_manager', False))\n",
    "    state = SimpleNamespace(is_manager=is_manager)\n",
    "    intents = _detect_intents(user_text)\n",
    "\n",
    "    # 2) retrieve from all sources (Prod)\n",
    "    informa_ctx  = _pg_snippets(user_text, collection=COLL_INFORMA,  k=8)\n",
    "    profile_ctx  = _pg_snippets(user_text, collection=COLL_PROFILE,  k=6)\n",
    "    jobs_ctx     = _kb_retrieve(JOB_KB_ID,     user_text, k=6) if JOB_KB_ID else []\n",
    "    courses_ctx  = _kb_retrieve(COURSES_KB_ID, user_text, k=6) if COURSES_KB_ID else []\n",
    "\n",
    "    # 3) build sections (merge all sources)\n",
    "    sections = {\n",
    "        'informa_strategy': [{'title': f'informa_ctx#{i+1}', 'snippet': s} for i, s in enumerate(informa_ctx)],\n",
    "        'employee_profile': [{'title': f'profile_ctx#{i+1}', 'snippet': s} for i, s in enumerate(profile_ctx)],\n",
    "        'jobs':             [{'title': f'job_ctx#{i+1}',     'snippet': s} for i, s in enumerate(jobs_ctx)],\n",
    "        'courses':          [{'title': f'course_ctx#{i+1}',  'snippet': s} for i, s in enumerate(courses_ctx)],\n",
    "    }\n",
    "\n",
    "    # 4) stream vs non-stream using your existing generation functions\n",
    "    if stream:\n",
    "        gen = synthesize_answer_llm_stream(   # uses your existing streaming function as-is\n",
    "            user_text=user_text,\n",
    "            intents=intents,\n",
    "            is_manager=is_manager,\n",
    "            profile_fields=profile_fields,\n",
    "            sections=sections,\n",
    "        )\n",
    "        return {\n",
    "            'stream': gen,\n",
    "            'blocked': False,\n",
    "            'gate': {'intents': intents},\n",
    "            'state': state,\n",
    "            'profile_found': bool(profile_fields.get('name') or profile_fields.get('title') or profile_fields.get('skills')),\n",
    "            'profile_fields': profile_fields,\n",
    "            'sections': sections,\n",
    "        }\n",
    "\n",
    "    # non-stream fallback: use your non-stream function if present, else join streamed text\n",
    "    try:\n",
    "        answer = synthesize_answer_llm(  # type: ignore[name-defined]\n",
    "            user_text=user_text,\n",
    "            intents=intents,\n",
    "            is_manager=is_manager,\n",
    "            profile_fields=profile_fields,\n",
    "            sections=sections,\n",
    "        )\n",
    "    except Exception:\n",
    "        answer = ''.join(list(synthesize_answer_llm_stream(\n",
    "            user_text=user_text,\n",
    "            intents=intents,\n",
    "            is_manager=is_manager,\n",
    "            profile_fields=profile_fields,\n",
    "            sections=sections,\n",
    "        )))\n",
    "\n",
    "    return {\n",
    "        'answer': answer,\n",
    "        'blocked': False,\n",
    "        'gate': {'intents': intents},\n",
    "        'state': state,\n",
    "        'profile_found': bool(profile_fields.get('name') or profile_fields.get('title') or profile_fields.get('skills')),\n",
    "        'profile_fields': profile_fields,\n",
    "        'sections': sections,\n",
    "    }\n",
    "\n",
    "print('✅ Unified run_workflow() defined: Profile + Informa PG + AWS KBs -> your existing streaming LLM.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elysia-dev-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
