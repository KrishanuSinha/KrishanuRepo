{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afb7fa1e",
   "metadata": {},
   "source": [
    "\n",
    "# Informa Career Advisor — Agentic Notebook (PG Vector + AWS KB + Streaming)\n",
    "\n",
    "**Generated:** 2025-08-13 16:32:55  \n",
    "**Purpose:** End‑to‑end **agentic** workflow that:\n",
    "- Connects to **Postgres pgvector** (2 tables):\n",
    "  - `internal_curated_informa_vectorstore` (Prod snippets)\n",
    "  - `internal_private_employee_profiles_vectorstore` (Dev employee profiles)\n",
    "- Connects to **AWS Knowledge Bases** (2 KBs via Bedrock Agent Runtime):\n",
    "  - Internal Jobs (`JOB_KB_ID`)\n",
    "  - Courses (`COURSES_KB_ID`)\n",
    "- Streams the final answer from an LLM (via **Bedrock Converse** streaming API)\n",
    "- Single `run_workflow()` pipeline: **profile + existing tools (+ Prod snippets) → streamed answer**\n",
    "\n",
    "> This notebook expects your secrets in a local `.env` file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb5c236",
   "metadata": {},
   "source": [
    "\n",
    "## 0) .env quick start (required)\n",
    "\n",
    "Create a `.env` in the same folder as this notebook with **your** values (you already shared some below):\n",
    "\n",
    "```bash\n",
    "PG_DSN=\"postgresql://v_svc_usr_aidb:j%3CpW%40qNsFIc%21%28OR@postgres.dev.iris.informa.com:5432/aidb?sslmode=require\"\n",
    "AWS_REGION=\"us-west-2\"\n",
    "\n",
    "# Knowledge Bases\n",
    "JOB_KB_ID=\"9PFZZ5FEIF\"\n",
    "COURSES_KB_ID=\"DENPFPR7CR\"\n",
    "\n",
    "# Embeddings & Chat models\n",
    "BEDROCK_EMBEDDING_MODEL=\"us.amazon.titan-embed-text-v2:0\"\n",
    "# Choose a Bedrock chat model you have access to (examples):\n",
    "# BEDROCK_CHAT_MODEL_ID=\"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "# BEDROCK_CHAT_MODEL_ID=\"cohere.command-r-plus-v1:0\"\n",
    "# BEDROCK_CHAT_MODEL_ID=\"meta.llama3-70b-instruct-v1:0\"\n",
    "```\n",
    "\n",
    "> **Note:** Keep the DSN and AWS creds secure. This notebook only **reads** from your env at runtime.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263e805e",
   "metadata": {},
   "source": [
    "## 1) Install dependencies (run locally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e12960",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If running for the first time locally, uncomment the next lines:\n",
    "# %pip install -U boto3 botocore python-dotenv pydantic tenacity psycopg2-binary pgvector\n",
    "# %pip install -U tiktoken  # optional; for chunking heuristics\n",
    "# %pip install -U ipywidgets  # for nicer streaming display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab09244",
   "metadata": {},
   "source": [
    "## 2) Imports & configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81844767",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import uuid\n",
    "import textwrap\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "from dotenv import load_dotenv\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "from pydantic import BaseModel, Field\n",
    "from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type\n",
    "\n",
    "# Load .env\n",
    "load_dotenv()\n",
    "\n",
    "AWS_REGION = os.getenv(\"AWS_REGION\", \"us-west-2\")\n",
    "BEDROCK_EMBEDDING_MODEL = os.getenv(\"BEDROCK_EMBEDDING_MODEL\", \"us.amazon.titan-embed-text-v2:0\")\n",
    "BEDROCK_CHAT_MODEL_ID = os.getenv(\"BEDROCK_CHAT_MODEL_ID\", \"anthropic.claude-3-5-sonnet-20240620-v1:0\")\n",
    "\n",
    "JOB_KB_ID = os.getenv(\"JOB_KB_ID\", \"\")\n",
    "COURSES_KB_ID = os.getenv(\"COURSES_KB_ID\", \"\")\n",
    "\n",
    "PG_DSN = os.getenv(\"PG_DSN\", \"\")\n",
    "\n",
    "# Tables\n",
    "PROD_SNIPPETS_TABLE = os.getenv(\"PROD_SNIPPETS_TABLE\", \"internal_curated_informa_vectorstore\")\n",
    "DEV_PROFILE_TABLE = os.getenv(\"DEV_PROFILE_TABLE\", \"internal_private_employee_profiles_vectorstore\")\n",
    "\n",
    "# Boto3 clients\n",
    "boto_cfg = Config(retries={'max_attempts': 10, 'mode': 'adaptive'})\n",
    "bedrock_rt = boto3.client(\"bedrock-runtime\", region_name=AWS_REGION, config=boto_cfg)\n",
    "bedrock_agent_rt = boto3.client(\"bedrock-agent-runtime\", region_name=AWS_REGION, config=boto_cfg)\n",
    "\n",
    "print(\"Config loaded:\")\n",
    "print(\"  AWS_REGION:\", AWS_REGION)\n",
    "print(\"  CHAT_MODEL:\", BEDROCK_CHAT_MODEL_ID)\n",
    "print(\"  EMB_MODEL :\", BEDROCK_EMBEDDING_MODEL)\n",
    "print(\"  JOB_KB_ID :\", JOB_KB_ID)\n",
    "print(\"  COURSES_KB_ID:\", COURSES_KB_ID)\n",
    "print(\"  PG_DSN    :\", \"set\" if PG_DSN else \"NOT SET\")\n",
    "print(\"  PROD_SNIPPETS_TABLE:\", PROD_SNIPPETS_TABLE)\n",
    "print(\"  DEV_PROFILE_TABLE   :\", DEV_PROFILE_TABLE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862b2e2c",
   "metadata": {},
   "source": [
    "## 3) Data models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c3e2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RetrievedChunk(BaseModel):\n",
    "    source: str = Field(..., description=\"Where the chunk came from (prod_snippets/dev_profile/jobs_kb/courses_kb)\")\n",
    "    text: str\n",
    "    meta: Dict[str, Any] = Field(default_factory=dict)\n",
    "    score: Optional[float] = None\n",
    "\n",
    "class ProfileSummary(BaseModel):\n",
    "    found: bool\n",
    "    email: Optional[str] = None\n",
    "    text: str = \"\"\n",
    "    meta: Dict[str, Any] = Field(default_factory=dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91da25d1",
   "metadata": {},
   "source": [
    "## 4) Embeddings helper (Titan v2 over Bedrock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a054db56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def embed_texts(texts: List[str]) -> List[List[float]]:\n",
    "    \"\"\"Return embeddings for a list of texts using the Titan v2 embedding model.\"\"\"\n",
    "    if not texts:\n",
    "        return []\n",
    "    body = {\n",
    "        \"inputText\": texts if len(texts) > 1 else texts[0]\n",
    "    }\n",
    "    resp = bedrock_rt.invoke_model(\n",
    "        modelId=BEDROCK_EMBEDDING_MODEL,\n",
    "        body=json.dumps(body)\n",
    "    )\n",
    "    payload = json.loads(resp[\"body\"].read())\n",
    "    # Titan returns either a list of embeddings or a single embedding; normalize\n",
    "    vectors = payload.get(\"embeddings\") or payload.get(\"vector\") or []\n",
    "    if isinstance(vectors, dict) and \"embedding\" in vectors:\n",
    "        return [vectors[\"embedding\"]]\n",
    "    if vectors and isinstance(vectors, list) and isinstance(vectors[0], dict) and \"embedding\" in vectors[0]:\n",
    "        return [v[\"embedding\"] for v in vectors]\n",
    "    # If it's already a list[list[float]]\n",
    "    return vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28b2854",
   "metadata": {},
   "source": [
    "## 5) Postgres (pgvector) search helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e934a4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@retry(wait=wait_exponential(multiplier=1, min=2, max=10),\n",
    "       stop=stop_after_attempt(3),\n",
    "       retry=retry_if_exception_type(psycopg2.OperationalError))\n",
    "def _pg_conn(dsn: str):\n",
    "    return psycopg2.connect(dsn, cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "\n",
    "def _pg_select(conn, sql: str, params: tuple = ()):\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(sql, params)\n",
    "        return cur.fetchall()\n",
    "\n",
    "def pg_semantic_search(\n",
    "    dsn: str, table: str, query: str, k: int = 5,\n",
    "    content_col: str = \"content\", meta_col: str = \"metadata\", embed_col: str = \"embedding\"\n",
    ") -> List[RetrievedChunk]:\n",
    "    \"\"\"Perform a simple vector similarity search using pgvector (cosine distance).\"\"\"\n",
    "    qvec = embed_texts([query])[0]\n",
    "    conn = _pg_conn(dsn)\n",
    "    try:\n",
    "        sql = f\"\"\"\n",
    "        SELECT\n",
    "            {content_col} AS content,\n",
    "            {meta_col}    AS metadata,\n",
    "            1.0 - ({embed_col} <=> %(qvec)s::vector) AS score\n",
    "        FROM {table}\n",
    "        ORDER BY {embed_col} <=> %(qvec)s::vector\n",
    "        LIMIT %(k)s;\n",
    "        \"\"\"\n",
    "        rows = _pg_select(conn, sql, params={\"qvec\": qvec, \"k\": k})\n",
    "        out = []\n",
    "        for r in rows:\n",
    "            out.append(RetrievedChunk(\n",
    "                source=table,\n",
    "                text=r.get(\"content\", \"\"),\n",
    "                meta=r.get(\"metadata\") or {},\n",
    "                score=float(r.get(\"score\") or 0.0),\n",
    "            ))\n",
    "        return out\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "def pg_lookup_profile_by_email(\n",
    "    dsn: str, table: str, email: Optional[str], k: int = 3,\n",
    "    content_col: str = \"content\", meta_col: str = \"metadata\", embed_col: str = \"embedding\"\n",
    ") -> ProfileSummary:\n",
    "    \"\"\"Try to find a profile record related to an email. Falls back to semantic search over the email string.\"\"\"\n",
    "    if not email:\n",
    "        return ProfileSummary(found=False, email=None, text=\"\", meta={})\n",
    "    conn = _pg_conn(dsn)\n",
    "    try:\n",
    "        # First try a direct metadata match if available\n",
    "        try:\n",
    "            sql = f\"\"\"\n",
    "            SELECT {content_col} AS content, {meta_col} AS metadata\n",
    "            FROM {table}\n",
    "            WHERE ({meta_col} ->> 'email') = %(email)s\n",
    "            LIMIT 1;\n",
    "            \"\"\"\n",
    "            rows = _pg_select(conn, sql, params={\"email\": email})\n",
    "            if rows:\n",
    "                r = rows[0]\n",
    "                return ProfileSummary(found=True, email=email, text=r.get(\"content\", \"\"), meta=r.get(\"metadata\") or {})\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Fallback: semantic search using the email as the query\n",
    "        chunks = pg_semantic_search(dsn, table, email, k=k, content_col=content_col, meta_col=meta_col, embed_col=embed_col)\n",
    "        if chunks:\n",
    "            best = chunks[0]\n",
    "            best.meta = best.meta or {}\n",
    "            best.meta[\"matched_via\"] = \"semantic_email_fallback\"\n",
    "            return ProfileSummary(found=True, email=email, text=best.text, meta=best.meta)\n",
    "        return ProfileSummary(found=False, email=email, text=\"\", meta={})\n",
    "    finally:\n",
    "        conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cb6c54",
   "metadata": {},
   "source": [
    "## 6) AWS Knowledge Bases (retrieve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594907d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def kb_retrieve(kb_id: str, query: str, top_k: int = 5) -> List[RetrievedChunk]:\n",
    "    if not kb_id:\n",
    "        return []\n",
    "    resp = bedrock_agent_rt.retrieve(\n",
    "        knowledgeBaseId=kb_id,\n",
    "        retrievalConfiguration={\n",
    "            \"vectorSearchConfiguration\": {\n",
    "                \"numberOfResults\": top_k\n",
    "            }\n",
    "        },\n",
    "        retrievalQuery={\"text\": query}\n",
    "    )\n",
    "    results = []\n",
    "    for item in resp.get(\"retrievalResults\", []):\n",
    "        text = item.get(\"content\", {}).get(\"text\", \"\")\n",
    "        score = item.get(\"score\")\n",
    "        meta = item.get(\"metadata\") or {}\n",
    "        # Resolve source URI if present:\n",
    "        src = item.get(\"location\", {}).get(\"s3Location\", {}).get(\"uri\") or meta.get(\"source\") or \"kb\"\n",
    "        results.append(RetrievedChunk(source=f\"kb:{kb_id}\", text=text, meta={\"kb_id\": kb_id, **meta, \"source_uri\": src}, score=score))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf606e5",
   "metadata": {},
   "source": [
    "## 7) Retrieval orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fd4359",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def retrieve_text_snippets(query: str, k: int = 5) -> List[RetrievedChunk]:\n",
    "    \"\"\"Aggregate from Prod snippets + Jobs KB + Courses KB.\"\"\"\n",
    "    out: List[RetrievedChunk] = []\n",
    "    # Prod snippets (PG)\n",
    "    try:\n",
    "        out += pg_semantic_search(PG_DSN, PROD_SNIPPETS_TABLE, query, k=k)\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] Prod snippets retrieval failed:\", e)\n",
    "\n",
    "    # Jobs KB\n",
    "    try:\n",
    "        out += kb_retrieve(JOB_KB_ID, query, top_k=min(k, 5))\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] Jobs KB retrieval failed:\", e)\n",
    "\n",
    "    # Courses KB\n",
    "    try:\n",
    "        out += kb_retrieve(COURSES_KB_ID, query, top_k=min(k, 5))\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] Courses KB retrieval failed:\", e)\n",
    "\n",
    "    # Sort by score descending when available\n",
    "    out_sorted = sorted(out, key=lambda r: (r.score if r.score is not None else 0.0), reverse=True)\n",
    "    return out_sorted[: (k * 3)]  # keep a modest pool\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c45544f",
   "metadata": {},
   "source": [
    "## 8) Prompt & context construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e807f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_system_prompt() -> str:\n",
    "    return textwrap.dedent(\"\"\"\n",
    "    You are the **Informa Career Advisor**. Your job:\n",
    "    - Analyze an employee's current skillset vs. Informa's digital transformation needs.\n",
    "    - Recommend specific upskilling actions (courses, internal job postings, mentors) with practical next steps.\n",
    "    - When helpful, cite sources inline like [S1], [S2] that refer to the Sources list provided.\n",
    "    - Prefer concise, structured, **actionable** responses (bullets, headings).\n",
    "    - If the profile is missing, infer politely, and state assumptions explicitly.\n",
    "    - Avoid hallucinations; if info is unavailable, say what to provide.\n",
    "    - Audience expects enterprise-grade clarity and brevity.\n",
    "    \"\"\")\n",
    "\n",
    "def format_sources(snippets: List[RetrievedChunk]) -> str:\n",
    "    lines = []\n",
    "    for i, s in enumerate(snippets, start=1):\n",
    "        label = f\"S{i}\"\n",
    "        origin = s.source\n",
    "        uri = s.meta.get(\"source_uri\") or \"\"\n",
    "        title = s.meta.get(\"title\") or s.meta.get(\"doc_title\") or \"\"\n",
    "        extra = f\" | {title}\" if title else \"\"\n",
    "        if uri:\n",
    "            lines.append(f\"- [{label}] {origin}{extra} — {uri}\")\n",
    "        else:\n",
    "            lines.append(f\"- [{label}] {origin}{extra}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def compose_user_message(query: str, profile: ProfileSummary, top_snips: List[RetrievedChunk]) -> str:\n",
    "    profile_block = profile.text.strip() if profile and profile.text else \"Profile not found.\"\n",
    "    snip_texts = []\n",
    "    for i, s in enumerate(top_snips, start=1):\n",
    "        # Keep chunks compact\n",
    "        snippet = s.text.strip()\n",
    "        if len(snippet) > 1200:\n",
    "            snippet = snippet[:1200] + \"...\"\n",
    "        snip_texts.append(f\"[S{i}]\\n{snippet}\")\n",
    "    sources_list = format_sources(top_snips)\n",
    "    return textwrap.dedent(f\"\"\"\n",
    "    # Query\n",
    "    {query}\n",
    "\n",
    "    # Employee Profile\n",
    "    {profile_block}\n",
    "\n",
    "    # Context Snippets\n",
    "    {'\\n\\n'.join(snip_texts) if snip_texts else 'No snippets available.'}\n",
    "\n",
    "    # Sources\n",
    "    {sources_list if sources_list else 'No sources available.'}\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbf6fe3",
   "metadata": {},
   "source": [
    "## 9) Bedrock chat (Converse) — with streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b92d2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _to_content_block(text: str) -> dict:\n",
    "    return {\"text\": {\"text\": text}}\n",
    "\n",
    "def converse_stream(system_prompt: str, user_text: str, model_id: str = None, temperature: float = 0.2):\n",
    "    \"\"\"Yield text deltas from Bedrock's converse_stream API.\"\"\"\n",
    "    model_id = model_id or BEDROCK_CHAT_MODEL_ID\n",
    "    try:\n",
    "        stream = bedrock_rt.converse_stream(\n",
    "            modelId=model_id,\n",
    "            system=[_to_content_block(system_prompt)],\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": [_to_content_block(user_text)]}\n",
    "            ],\n",
    "            inferenceConfig={\n",
    "                \"temperature\": temperature,\n",
    "                \"maxTokens\": 1500,\n",
    "                \"topP\": 0.9\n",
    "            },\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # Helpful failure mode if model doesn't support converse_stream\n",
    "        raise RuntimeError(f\"Converse stream failed for model '{model_id}': {e}\")\n",
    "\n",
    "    # The SDK returns a generator of events\n",
    "    for event in stream.get(\"stream\"):\n",
    "        # Text deltas\n",
    "        if \"contentBlockDelta\" in event:\n",
    "            delta = event[\"contentBlockDelta\"][\"delta\"]\n",
    "            if \"text\" in delta:\n",
    "                yield delta[\"text\"]\n",
    "        # Final message\n",
    "        if \"messageStop\" in event:\n",
    "            break\n",
    "\n",
    "def converse_once(system_prompt: str, user_text: str, model_id: str = None, temperature: float = 0.2) -> str:\n",
    "    model_id = model_id or BEDROCK_CHAT_MODEL_ID\n",
    "    resp = bedrock_rt.converse(\n",
    "        modelId=model_id,\n",
    "        system=[_to_content_block(system_prompt)],\n",
    "        messages=[{\"role\": \"user\", \"content\": [_to_content_block(user_text)]}],\n",
    "        inferenceConfig={\"temperature\": temperature, \"maxTokens\": 1500, \"topP\": 0.9},\n",
    "    )\n",
    "    # Extract assistant text\n",
    "    msg = resp.get(\"output\", {}).get(\"message\", {})\n",
    "    parts = msg.get(\"content\", [])\n",
    "    text_out = []\n",
    "    for p in parts:\n",
    "        t = p.get(\"text\", {}).get(\"text\")\n",
    "        if t:\n",
    "            text_out.append(t)\n",
    "    return \"\".join(text_out) if text_out else json.dumps(resp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d39036",
   "metadata": {},
   "source": [
    "## 10) `run_workflow()` — one function to rule them all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cea724",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_workflow(query: str, email: Optional[str] = None, k: int = 5, stream: bool = True) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    1) Get profile (Dev PG vector) — best-effort\n",
    "    2) Retrieve snippets: Prod PG + Jobs KB + Courses KB\n",
    "    3) Build system + user messages\n",
    "    4) Stream (or not) final LLM answer\n",
    "    \"\"\"\n",
    "    profile = ProfileSummary(found=False, email=email, text=\"\", meta={})\n",
    "    try:\n",
    "        if PG_DSN and DEV_PROFILE_TABLE:\n",
    "            profile = pg_lookup_profile_by_email(PG_DSN, DEV_PROFILE_TABLE, email=email, k=3)\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] Profile lookup failed:\", e)\n",
    "\n",
    "    snippets = retrieve_text_snippets(query, k=k)\n",
    "\n",
    "    sys_prompt = build_system_prompt()\n",
    "    user_msg = compose_user_message(query, profile, snippets)\n",
    "\n",
    "    if not stream:\n",
    "        final_text = converse_once(sys_prompt, user_msg, model_id=BEDROCK_CHAT_MODEL_ID)\n",
    "        return {\n",
    "            \"profile\": profile.dict(),\n",
    "            \"snippets\": [s.dict() for s in snippets],\n",
    "            \"streamed\": False,\n",
    "            \"text\": final_text,\n",
    "        }\n",
    "\n",
    "    # Streaming\n",
    "    stream_out = []\n",
    "    try:\n",
    "        for delta in converse_stream(sys_prompt, user_msg, model_id=BEDROCK_CHAT_MODEL_ID):\n",
    "            print(delta, end=\"\", flush=True)\n",
    "            stream_out.append(delta)\n",
    "    except Exception as e:\n",
    "        print(\"\\n[WARN] Streaming failed, falling back to single-shot.\\n\", e)\n",
    "        final_text = converse_once(sys_prompt, user_msg, model_id=BEDROCK_CHAT_MODEL_ID)\n",
    "        print(final_text)\n",
    "        stream_out = [final_text]\n",
    "\n",
    "    return {\n",
    "        \"profile\": profile.dict(),\n",
    "        \"snippets\": [s.dict() for s in snippets],\n",
    "        \"streamed\": True,\n",
    "        \"text\": \"\".join(stream_out),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d6a09b",
   "metadata": {},
   "source": [
    "## 11) Smoke tests (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17598567",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 11a) PG connectivity test (uncomment to run)\n",
    "# with _pg_conn(PG_DSN) as conn:\n",
    "#     print(\"PG NOW():\", _pg_select(conn, \"SELECT NOW() AS now;\"))\n",
    "#     print(\"Prod table sample:\", _pg_select(conn, f\"SELECT COUNT(*) FROM {PROD_SNIPPETS_TABLE};\"))\n",
    "#     print(\"Dev profile table sample:\", _pg_select(conn, f\"SELECT COUNT(*) FROM {DEV_PROFILE_TABLE};\"))\n",
    "\n",
    "# 11b) AWS KB quick checks (uncomment to run)\n",
    "# print(\"Jobs KB sample:\", kb_retrieve(JOB_KB_ID, \"software engineer role\", top_k=1)[:1])\n",
    "# print(\"Courses KB sample:\", kb_retrieve(COURSES_KB_ID, \"machine learning upskilling\", top_k=1)[:1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ec3b7f",
   "metadata": {},
   "source": [
    "## 12) Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa0c200",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "example_query = \"Analyze my current skillset against Informa's digital transformation needs and recommend 5 specific learning opportunities to close these gaps.\"\n",
    "example_email = os.getenv(\"DEFAULT_USER_EMAIL\")  # optionally set in .env\n",
    "\n",
    "# Run end-to-end (streaming)\n",
    "out = run_workflow(example_query, email=example_email, stream=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a14fbc",
   "metadata": {},
   "source": [
    "\n",
    "## 13) Troubleshooting\n",
    "\n",
    "- **`Unexpected role \"system\"`**: This notebook uses Bedrock **Converse** APIs correctly by passing the system prompt via the **top-level** `system=[...]` parameter, not as a message. If you see this error, double‑check the `BEDROCK_CHAT_MODEL_ID`; some non‑Converse models may only support legacy `invoke_model`. Try another chat model you have access to (e.g., an Anthropic or Cohere chat model on Bedrock).\n",
    "\n",
    "- **Streaming not supported**: If the selected model doesn't support `converse_stream`, the code will **fall back** to a single-shot `converse` call.\n",
    "\n",
    "- **PG schema**: This expects `pgvector` installed and an `embedding` column compatible with your embedding dimension (Titan v2: typically 1024). If your column name or dimension differs, update `embed_col` or adjust the SQL. Content column assumed `content` and `metadata` (`jsonb`).\n",
    "\n",
    "- **Empty results**:\n",
    "  - If `retrieve_text_snippets()` returns `[]`, verify table names and KB IDs.\n",
    "  - If `profile` not found, the agent will still answer and clearly state assumptions.\n",
    "\n",
    "- **Security**: Keep `.env` out of version control.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca79079",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "**© Informa / Internal Use** — This notebook contains example integrations and should be reviewed for compliance and data governance before production use.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}