{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Machine learning libraries:# Machi \n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report, roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>trending_date</th>\n",
       "      <th>title_description</th>\n",
       "      <th>channel_title</th>\n",
       "      <th>category_id</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>tags</th>\n",
       "      <th>views</th>\n",
       "      <th>likes</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>thumbnail_link</th>\n",
       "      <th>comments_disabled</th>\n",
       "      <th>ratings_disabled</th>\n",
       "      <th>video_error_or_removed</th>\n",
       "      <th>description</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2kyS6SvSYSE</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>WE WANT TO TALK ABOUT OUR MARRIAGE</td>\n",
       "      <td>CaseyNeistat</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2017-11-13T17:13:01.000Z</td>\n",
       "      <td>SHANtell martin</td>\n",
       "      <td>748374.0</td>\n",
       "      <td>57527.0</td>\n",
       "      <td>2966.0</td>\n",
       "      <td>15954.0</td>\n",
       "      <td>https://i.ytimg.com/vi/2kyS6SvSYSE/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>SHANTELL'S CHANNEL - https://www.youtube.com/s...</td>\n",
       "      <td>People &amp; Blogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0mlNzVSJrT0</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>Me-O Cats Commercial</td>\n",
       "      <td>Nobrand</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2017-04-21T06:47:32.000Z</td>\n",
       "      <td>cute|\"cats\"|\"thai\"|\"eggs\"</td>\n",
       "      <td>98966.0</td>\n",
       "      <td>2486.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>532.0</td>\n",
       "      <td>https://i.ytimg.com/vi/0mlNzVSJrT0/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Kittens come out of the eggs in a Thai commerc...</td>\n",
       "      <td>People &amp; Blogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>STI2fI7sKMo</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>AFFAIRS, EX BOYFRIENDS, $18MILLION NET WORTH -...</td>\n",
       "      <td>Shawn Johnson East</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2017-11-11T15:00:03.000Z</td>\n",
       "      <td>shawn johnson|\"andrew east\"|\"shawn east\"|\"shaw...</td>\n",
       "      <td>321053.0</td>\n",
       "      <td>4451.0</td>\n",
       "      <td>1772.0</td>\n",
       "      <td>895.0</td>\n",
       "      <td>https://i.ytimg.com/vi/STI2fI7sKMo/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Subscribe for weekly videos ▶ http://bit.ly/sj...</td>\n",
       "      <td>People &amp; Blogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KODzih-pYlU</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>BLIND(folded) CAKE DECORATING CONTEST (with Mo...</td>\n",
       "      <td>Grace Helbig</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2017-11-11T18:08:04.000Z</td>\n",
       "      <td>itsgrace|\"funny\"|\"comedy\"|\"vlog\"|\"grace\"|\"helb...</td>\n",
       "      <td>197062.0</td>\n",
       "      <td>7250.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>456.0</td>\n",
       "      <td>https://i.ytimg.com/vi/KODzih-pYlU/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Molly is an god damn amazing human and she cha...</td>\n",
       "      <td>People &amp; Blogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8mhTWqWlQzU</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>Wearing Online Dollar Store Makeup For A Week</td>\n",
       "      <td>Safiya Nygaard</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2017-11-11T01:19:33.000Z</td>\n",
       "      <td>wearing online dollar store makeup for a week|...</td>\n",
       "      <td>2744430.0</td>\n",
       "      <td>115426.0</td>\n",
       "      <td>1110.0</td>\n",
       "      <td>6541.0</td>\n",
       "      <td>https://i.ytimg.com/vi/8mhTWqWlQzU/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>I found this online dollar store called ShopMi...</td>\n",
       "      <td>People &amp; Blogs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id trending_date  \\\n",
       "0  2kyS6SvSYSE      17.14.11   \n",
       "1  0mlNzVSJrT0      17.14.11   \n",
       "2  STI2fI7sKMo      17.14.11   \n",
       "3  KODzih-pYlU      17.14.11   \n",
       "4  8mhTWqWlQzU      17.14.11   \n",
       "\n",
       "                                   title_description       channel_title  \\\n",
       "0                 WE WANT TO TALK ABOUT OUR MARRIAGE        CaseyNeistat   \n",
       "1                               Me-O Cats Commercial             Nobrand   \n",
       "2  AFFAIRS, EX BOYFRIENDS, $18MILLION NET WORTH -...  Shawn Johnson East   \n",
       "3  BLIND(folded) CAKE DECORATING CONTEST (with Mo...        Grace Helbig   \n",
       "4      Wearing Online Dollar Store Makeup For A Week      Safiya Nygaard   \n",
       "\n",
       "   category_id              publish_time  \\\n",
       "0         22.0  2017-11-13T17:13:01.000Z   \n",
       "1         22.0  2017-04-21T06:47:32.000Z   \n",
       "2         22.0  2017-11-11T15:00:03.000Z   \n",
       "3         22.0  2017-11-11T18:08:04.000Z   \n",
       "4         22.0  2017-11-11T01:19:33.000Z   \n",
       "\n",
       "                                                tags      views     likes  \\\n",
       "0                                    SHANtell martin   748374.0   57527.0   \n",
       "1                          cute|\"cats\"|\"thai\"|\"eggs\"    98966.0    2486.0   \n",
       "2  shawn johnson|\"andrew east\"|\"shawn east\"|\"shaw...   321053.0    4451.0   \n",
       "3  itsgrace|\"funny\"|\"comedy\"|\"vlog\"|\"grace\"|\"helb...   197062.0    7250.0   \n",
       "4  wearing online dollar store makeup for a week|...  2744430.0  115426.0   \n",
       "\n",
       "   dislikes  comment_count                                  thumbnail_link  \\\n",
       "0    2966.0        15954.0  https://i.ytimg.com/vi/2kyS6SvSYSE/default.jpg   \n",
       "1     184.0          532.0  https://i.ytimg.com/vi/0mlNzVSJrT0/default.jpg   \n",
       "2    1772.0          895.0  https://i.ytimg.com/vi/STI2fI7sKMo/default.jpg   \n",
       "3     217.0          456.0  https://i.ytimg.com/vi/KODzih-pYlU/default.jpg   \n",
       "4    1110.0         6541.0  https://i.ytimg.com/vi/8mhTWqWlQzU/default.jpg   \n",
       "\n",
       "  comments_disabled ratings_disabled video_error_or_removed  \\\n",
       "0             False            False                  False   \n",
       "1             False            False                  False   \n",
       "2             False            False                  False   \n",
       "3             False            False                  False   \n",
       "4             False            False                  False   \n",
       "\n",
       "                                         description           title  \n",
       "0  SHANTELL'S CHANNEL - https://www.youtube.com/s...  People & Blogs  \n",
       "1  Kittens come out of the eggs in a Thai commerc...  People & Blogs  \n",
       "2  Subscribe for weekly videos ▶ http://bit.ly/sj...  People & Blogs  \n",
       "3  Molly is an god damn amazing human and she cha...  People & Blogs  \n",
       "4  I found this online dollar store called ShopMi...  People & Blogs  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading data into dataframe\n",
    "file_name = 'D:\\\\Springboard_Capstone2\\\\CapstoneDataset\\\\NewData\\\\YouTubeDataSet.csv'\n",
    "df=pd.read_csv(file_name, encoding='utf-8')\n",
    "df = df.dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Numerical Features such as Views, Comment Count, Likes and Dislikes are highly correlated as obeserved in the Visualization section . Hence in my opinion I shall use only one Numerical Variable in our Training Data i.e., views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Keeping the numerical features of the dataset on one side. This numerical feature will be concatenated with the training set once the textual data has been tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40291, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keeping the numerical features of the dataset on one side. \n",
    "# This numerical feature will be concatenated with the training set once the textual data has been tokenized.\n",
    "Statistics = df[['views']]\n",
    "Statistics.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Converting the textual data into list format in order to make it ready for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the textual data (feature data) and video title (Label) into list format in order to make it ready for tokenization \n",
    "corpus = list(df['title_description'])\n",
    "labels = list(df['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Converting feature set int array format. I shall later pass this array as an argument to a function that will extract the theme from the textual data in the form of tokenized words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting feature set int array format. I shall later pass this array as an argument to a function that will extract \n",
    "# the theme from the textual data in the form of tokenized words.\n",
    "corpus = np.array(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For the convinience of clear understanding I am representing the Feature set and Label set into a single DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WE WANT TO TALK ABOUT OUR MARRIAGE</td>\n",
       "      <td>People &amp; Blogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Me-O Cats Commercial</td>\n",
       "      <td>People &amp; Blogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFFAIRS, EX BOYFRIENDS, $18MILLION NET WORTH -...</td>\n",
       "      <td>People &amp; Blogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BLIND(folded) CAKE DECORATING CONTEST (with Mo...</td>\n",
       "      <td>People &amp; Blogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wearing Online Dollar Store Makeup For A Week</td>\n",
       "      <td>People &amp; Blogs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Document        Category\n",
       "0                 WE WANT TO TALK ABOUT OUR MARRIAGE  People & Blogs\n",
       "1                               Me-O Cats Commercial  People & Blogs\n",
       "2  AFFAIRS, EX BOYFRIENDS, $18MILLION NET WORTH -...  People & Blogs\n",
       "3  BLIND(folded) CAKE DECORATING CONTEST (with Mo...  People & Blogs\n",
       "4      Wearing Online Dollar Store Makeup For A Week  People & Blogs"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For my convinience of understanding I am representing the Feature set and Label set into a single DataFrame. \n",
    "corpus_df = pd.DataFrame({'Document': corpus, \n",
    "                          'Category': labels})\n",
    "corpus_df = corpus_df[['Document', 'Category']]\n",
    "corpus_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Following is a python function that removes unwanted characters in order to extract the theme of the textual data. Next we shall pass our textual data to this function in order to extract the theme.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['want talk marriage', 'meo cats commercial',\n",
       "       'affairs ex boyfriends million net worth google us shawn andrew',\n",
       "       ..., 'game zones se isle van gundy',\n",
       "       'game zones se isle van gundy', 'game zones se isle van gundy'],\n",
       "      dtype='<U92')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)\n",
    "\n",
    "text = normalize_corpus(corpus)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The above represents the tokenized words or themes in array format, that have been extracted from textual description. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing the tokenized words from array into Sparse Matrix format so that it can be fit into Classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing the tokenized words from array into Sparse Matrix format so that it can be fit into Classification model\n",
    "vectorizer = CountVectorizer(min_df=0)\n",
    "X = vectorizer.fit_transform(text)\n",
    "X = X.tocsc()  # some versions of sklearn return COO format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the shape of the Sparse Matrix to ensure we have not lost any data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40291, 10063)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the shape of the Sparse Matrix to ensure we have not lost any data\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Converting the Sparse Matrix into DataFrame so that we can combine it with video statistics feature set in order to build our Final Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the Sparse Matrix into DataFrame so that we can combine it with video statistics feature set in \n",
    "# order to build our Final Training Dataset\n",
    "A=pd.DataFrame(X.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating Presprocessed Textual data with video statistics feature set in order to build our Final Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating Presprocessed Textual data with video statistics feature set in order to build our Final Training Dataset\n",
    "U = pd.concat([Statistics.reset_index(drop=True), A.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recheking the shape of our Training Data in order to confirm after concatenation no data has been lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40291, 10064)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Label column into a format that is suitable for fitting into our Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the Label column into a format that is suitable for fitting into our Classification Model\n",
    "mymap = {'People & Blogs':1, 'Entertainment':2, 'Comedy':3, 'Science & Technology':4, 'Film & Animation':5,'News & Politics':6,'Sports':7,'Music':8,'Pets & Animals':9,'Education':10,'Howto & Style':11,'Autos & Vehicles':12,'Travel & Events':13,'Gaming':14,'Nonprofits & Activism':15,'Shows':16}\n",
    "\n",
    "label=df[['title']].applymap(lambda s: mymap.get(s) if s in mymap else s)\n",
    "\n",
    "y = label.values.astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying MultinomialNB Machine Learning Algorithms:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I have randomly chosen a set of Alpha values that I shall fit into my model and then see which of the values yield the highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#I have randomly chosen a set of Alpha values that I shall fit into my model and \n",
    "#then see which of the values yield the highest accuracy  \n",
    "#the grid of parameters to search over\n",
    "alphas = [.1, 1, 5, 10, 50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the Dataset into Training and Testing set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the Dataset into Training and Testing set\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(U,y,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.8385175380542687\n",
      "\n",
      "Alpha:  1\n",
      "Score:  0.689692256783587\n",
      "\n",
      "Alpha:  5\n",
      "Score:  0.4737756452680344\n",
      "\n",
      "Alpha:  10\n",
      "Score:  0.3834381204500331\n",
      "\n",
      "Alpha:  50\n",
      "Score:  0.21757114493712773\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define train_and_predict()\n",
    "def train_and_predict(alpha):\n",
    "    # Instantiate the classifier: nb_classifier\n",
    "    nb_classifier = MultinomialNB(alpha=alpha)\n",
    "    # Fit to the training data\n",
    "    nb_classifier.fit(X_train,Y_train)\n",
    "    # Predict the labels: pred\n",
    "    pred = nb_classifier.predict(X_test)\n",
    "    # Compute accuracy: score\n",
    "    score = metrics.accuracy_score(Y_test,pred)\n",
    "    return score\n",
    "\n",
    "# Iterate over the alphas and print the corresponding score\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    print('Score: ', train_and_predict(alpha))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Our model displays best accuracy of 83% at Alpha = 0.1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Also evaluating the Precision and Recall metric for a our training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB(alpha=0.1)\n",
    "# Fit to the training data\n",
    "nb_classifier.fit(X_train,Y_train)\n",
    "# Predict the labels: pred\n",
    "pred = nb_classifier.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine learning pipelines\n",
    "pipe_NB = make_pipeline(MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predicted y's (y_hat)\n",
    "predicted_NB = cross_val_predict(pipe_NB, X, y, cv=3, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.23      0.23      0.23      3061\n",
      "          2       0.55      0.48      0.51      9819\n",
      "          3       0.42      0.38      0.40      3435\n",
      "          4       0.37      0.45      0.41      2361\n",
      "          5       0.48      0.55      0.51      2340\n",
      "          6       0.53      0.51      0.52      2409\n",
      "          7       0.62      0.72      0.67      2060\n",
      "          8       0.77      0.78      0.77      6434\n",
      "          9       0.39      0.46      0.42       916\n",
      "         10       0.31      0.27      0.29      1642\n",
      "         11       0.53      0.62      0.57      4132\n",
      "         12       0.30      0.28      0.29       379\n",
      "         13       0.48      0.39      0.43       402\n",
      "         14       0.49      0.43      0.45       791\n",
      "         15       0.65      0.21      0.31        53\n",
      "         16       0.69      0.72      0.71        57\n",
      "\n",
      "avg / total       0.52      0.52      0.52     40291\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification tables\n",
    "\n",
    "print(\"Multinomial Naive Bayes:\") \n",
    "print(classification_report(y, predicted_NB)) \n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The Precision and Recall Rate give us an idea about the discriminative abitity of our classifier. In this case our Precision and Recall rate happen to be 0.52"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the accuracy score when only the tokenized words are used as feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  0.1\n",
      "Score:  0.9268696227663799\n",
      "\n",
      "Alpha:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.8899735274652548\n",
      "\n",
      "Alpha:  5\n",
      "Score:  0.7897088021178028\n",
      "\n",
      "Alpha:  10\n",
      "Score:  0.7217902051621443\n",
      "\n",
      "Alpha:  50\n",
      "Score:  0.5210953011250827\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Splitting the Dataset into Training and Testing set\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,y,test_size=0.3,random_state=42)\n",
    "\n",
    "# Define train_and_predict()\n",
    "def train_and_predict(alpha):\n",
    "    # Instantiate the classifier: nb_classifier\n",
    "    nb_classifier = MultinomialNB(alpha=alpha)\n",
    "    # Fit to the training data\n",
    "    nb_classifier.fit(X_train,Y_train)\n",
    "    # Predict the labels: pred\n",
    "    pred = nb_classifier.predict(X_test)\n",
    "    # Compute accuracy: score\n",
    "    score = metrics.accuracy_score(Y_test,pred)\n",
    "    return score\n",
    "\n",
    "# Iterate over the alphas and print the corresponding score\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    print('Score: ', train_and_predict(alpha))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB(alpha=0.1)\n",
    "# Fit to the training data\n",
    "nb_classifier.fit(X_train,Y_train)\n",
    "# Predict the labels: pred\n",
    "pred = nb_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine learning pipelines\n",
    "pipe_NB = make_pipeline(MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predicted y's (y_hat)\n",
    "predicted_NB = cross_val_predict(pipe_NB, X, y, cv=3, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.23      0.23      0.23      3061\n",
      "          2       0.55      0.48      0.51      9819\n",
      "          3       0.42      0.38      0.40      3435\n",
      "          4       0.37      0.45      0.41      2361\n",
      "          5       0.48      0.55      0.51      2340\n",
      "          6       0.53      0.51      0.52      2409\n",
      "          7       0.62      0.72      0.67      2060\n",
      "          8       0.77      0.78      0.77      6434\n",
      "          9       0.39      0.46      0.42       916\n",
      "         10       0.31      0.27      0.29      1642\n",
      "         11       0.53      0.62      0.57      4132\n",
      "         12       0.30      0.28      0.29       379\n",
      "         13       0.48      0.39      0.43       402\n",
      "         14       0.49      0.43      0.45       791\n",
      "         15       0.65      0.21      0.31        53\n",
      "         16       0.69      0.72      0.71        57\n",
      "\n",
      "avg / total       0.52      0.52      0.52     40291\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification tables\n",
    "\n",
    "print(\"Multinomial Naive Bayes:\") \n",
    "print(classification_report(y, predicted_NB)) \n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Even after the accuracy has increased, the Precision and Recall value seem to have remained the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15388030084564697"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(Y_test, pred, pos_label=2)\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### This particular model where no numerical features have not been utilized displays its best accuracy of 92% when Alpha = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the performance of other Classification Machine Learning Algorithms:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying out Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# split X and y into training and testing sets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train a logistic regression model on the training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# instantiate model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# fit model\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels: pred\n",
    "pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.965452198947682\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Story:\n",
    "\n",
    "Continuous Numerical features such as Views, Likes, Dislikes, Comment Count are not very helpful features to classify videos based on their themes. The reason is because the number of views that a particular video recieves is independent of the theme or category of the video.Irrespective of its theme/category the video may get higher views based on how well it appeals to its target audience. Hence, it is not necessary that particular categories of videos always get very high views. This is the possible reason why the accuracy dropped.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
