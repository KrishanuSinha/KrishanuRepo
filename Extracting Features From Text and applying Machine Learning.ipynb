{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>trending_date</th>\n",
       "      <th>title_description</th>\n",
       "      <th>channel_title</th>\n",
       "      <th>category_id</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>tags</th>\n",
       "      <th>views</th>\n",
       "      <th>likes</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>thumbnail_link</th>\n",
       "      <th>comments_disabled</th>\n",
       "      <th>ratings_disabled</th>\n",
       "      <th>video_error_or_removed</th>\n",
       "      <th>description</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2kyS6SvSYSE</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>WE WANT TO TALK ABOUT OUR MARRIAGE</td>\n",
       "      <td>CaseyNeistat</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2017-11-13T17:13:01.000Z</td>\n",
       "      <td>SHANtell martin</td>\n",
       "      <td>748374.0</td>\n",
       "      <td>57527.0</td>\n",
       "      <td>2966.0</td>\n",
       "      <td>15954.0</td>\n",
       "      <td>https://i.ytimg.com/vi/2kyS6SvSYSE/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>SHANTELL'S CHANNEL - https://www.youtube.com/s...</td>\n",
       "      <td>People &amp; Blogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0mlNzVSJrT0</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>Me-O Cats Commercial</td>\n",
       "      <td>Nobrand</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2017-04-21T06:47:32.000Z</td>\n",
       "      <td>cute|\"cats\"|\"thai\"|\"eggs\"</td>\n",
       "      <td>98966.0</td>\n",
       "      <td>2486.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>532.0</td>\n",
       "      <td>https://i.ytimg.com/vi/0mlNzVSJrT0/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Kittens come out of the eggs in a Thai commerc...</td>\n",
       "      <td>People &amp; Blogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>STI2fI7sKMo</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>AFFAIRS, EX BOYFRIENDS, $18MILLION NET WORTH -...</td>\n",
       "      <td>Shawn Johnson East</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2017-11-11T15:00:03.000Z</td>\n",
       "      <td>shawn johnson|\"andrew east\"|\"shawn east\"|\"shaw...</td>\n",
       "      <td>321053.0</td>\n",
       "      <td>4451.0</td>\n",
       "      <td>1772.0</td>\n",
       "      <td>895.0</td>\n",
       "      <td>https://i.ytimg.com/vi/STI2fI7sKMo/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Subscribe for weekly videos ▶ http://bit.ly/sj...</td>\n",
       "      <td>People &amp; Blogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KODzih-pYlU</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>BLIND(folded) CAKE DECORATING CONTEST (with Mo...</td>\n",
       "      <td>Grace Helbig</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2017-11-11T18:08:04.000Z</td>\n",
       "      <td>itsgrace|\"funny\"|\"comedy\"|\"vlog\"|\"grace\"|\"helb...</td>\n",
       "      <td>197062.0</td>\n",
       "      <td>7250.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>456.0</td>\n",
       "      <td>https://i.ytimg.com/vi/KODzih-pYlU/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Molly is an god damn amazing human and she cha...</td>\n",
       "      <td>People &amp; Blogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8mhTWqWlQzU</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>Wearing Online Dollar Store Makeup For A Week</td>\n",
       "      <td>Safiya Nygaard</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2017-11-11T01:19:33.000Z</td>\n",
       "      <td>wearing online dollar store makeup for a week|...</td>\n",
       "      <td>2744430.0</td>\n",
       "      <td>115426.0</td>\n",
       "      <td>1110.0</td>\n",
       "      <td>6541.0</td>\n",
       "      <td>https://i.ytimg.com/vi/8mhTWqWlQzU/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>I found this online dollar store called ShopMi...</td>\n",
       "      <td>People &amp; Blogs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id trending_date  \\\n",
       "0  2kyS6SvSYSE      17.14.11   \n",
       "1  0mlNzVSJrT0      17.14.11   \n",
       "2  STI2fI7sKMo      17.14.11   \n",
       "3  KODzih-pYlU      17.14.11   \n",
       "4  8mhTWqWlQzU      17.14.11   \n",
       "\n",
       "                                   title_description       channel_title  \\\n",
       "0                 WE WANT TO TALK ABOUT OUR MARRIAGE        CaseyNeistat   \n",
       "1                               Me-O Cats Commercial             Nobrand   \n",
       "2  AFFAIRS, EX BOYFRIENDS, $18MILLION NET WORTH -...  Shawn Johnson East   \n",
       "3  BLIND(folded) CAKE DECORATING CONTEST (with Mo...        Grace Helbig   \n",
       "4      Wearing Online Dollar Store Makeup For A Week      Safiya Nygaard   \n",
       "\n",
       "   category_id              publish_time  \\\n",
       "0         22.0  2017-11-13T17:13:01.000Z   \n",
       "1         22.0  2017-04-21T06:47:32.000Z   \n",
       "2         22.0  2017-11-11T15:00:03.000Z   \n",
       "3         22.0  2017-11-11T18:08:04.000Z   \n",
       "4         22.0  2017-11-11T01:19:33.000Z   \n",
       "\n",
       "                                                tags      views     likes  \\\n",
       "0                                    SHANtell martin   748374.0   57527.0   \n",
       "1                          cute|\"cats\"|\"thai\"|\"eggs\"    98966.0    2486.0   \n",
       "2  shawn johnson|\"andrew east\"|\"shawn east\"|\"shaw...   321053.0    4451.0   \n",
       "3  itsgrace|\"funny\"|\"comedy\"|\"vlog\"|\"grace\"|\"helb...   197062.0    7250.0   \n",
       "4  wearing online dollar store makeup for a week|...  2744430.0  115426.0   \n",
       "\n",
       "   dislikes  comment_count                                  thumbnail_link  \\\n",
       "0    2966.0        15954.0  https://i.ytimg.com/vi/2kyS6SvSYSE/default.jpg   \n",
       "1     184.0          532.0  https://i.ytimg.com/vi/0mlNzVSJrT0/default.jpg   \n",
       "2    1772.0          895.0  https://i.ytimg.com/vi/STI2fI7sKMo/default.jpg   \n",
       "3     217.0          456.0  https://i.ytimg.com/vi/KODzih-pYlU/default.jpg   \n",
       "4    1110.0         6541.0  https://i.ytimg.com/vi/8mhTWqWlQzU/default.jpg   \n",
       "\n",
       "  comments_disabled ratings_disabled video_error_or_removed  \\\n",
       "0             False            False                  False   \n",
       "1             False            False                  False   \n",
       "2             False            False                  False   \n",
       "3             False            False                  False   \n",
       "4             False            False                  False   \n",
       "\n",
       "                                         description           title  \n",
       "0  SHANTELL'S CHANNEL - https://www.youtube.com/s...  People & Blogs  \n",
       "1  Kittens come out of the eggs in a Thai commerc...  People & Blogs  \n",
       "2  Subscribe for weekly videos ▶ http://bit.ly/sj...  People & Blogs  \n",
       "3  Molly is an god damn amazing human and she cha...  People & Blogs  \n",
       "4  I found this online dollar store called ShopMi...  People & Blogs  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading data into dataframe\n",
    "file_name = 'D:\\\\Springboard_Capstone2\\\\CapstoneDataset\\\\NewData\\\\YouTubeDataSet.csv'\n",
    "df=pd.read_csv(file_name, encoding='utf-8')\n",
    "df = df.dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Numerical Features such as Views, Comment Count, Likes and Dislikes are highly correlated as obeserved in the Visualization section . Hence in my opinion I shall use only one Numerical Variable in our Training Data i.e., views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Keeping the numerical features of the dataset on one side. This numerical feature will be concatenated with the training set once the textual data has been tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40291, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keeping the numerical features of the dataset on one side. \n",
    "# This numerical feature will be concatenated with the training set once the textual data has been tokenized.\n",
    "Statistics = df[['views']]\n",
    "Statistics.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Converting the textual data into list format in order to make it ready for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the textual data (feature data) and video title (Label) into list format in order to make it ready for tokenization \n",
    "corpus = list(df['title_description'])\n",
    "labels = list(df['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Converting feature set int array format. I shall later pass this array as an argument to a function that will extract the theme from the textual data in the form of tokenized words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting feature set int array format. I shall later pass this array as an argument to a function that will extract \n",
    "# the theme from the textual data in the form of tokenized words.\n",
    "corpus = np.array(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For the convinience of clear understanding I am representing the Feature set and Label set into a single DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WE WANT TO TALK ABOUT OUR MARRIAGE</td>\n",
       "      <td>People &amp; Blogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Me-O Cats Commercial</td>\n",
       "      <td>People &amp; Blogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFFAIRS, EX BOYFRIENDS, $18MILLION NET WORTH -...</td>\n",
       "      <td>People &amp; Blogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BLIND(folded) CAKE DECORATING CONTEST (with Mo...</td>\n",
       "      <td>People &amp; Blogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wearing Online Dollar Store Makeup For A Week</td>\n",
       "      <td>People &amp; Blogs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Document        Category\n",
       "0                 WE WANT TO TALK ABOUT OUR MARRIAGE  People & Blogs\n",
       "1                               Me-O Cats Commercial  People & Blogs\n",
       "2  AFFAIRS, EX BOYFRIENDS, $18MILLION NET WORTH -...  People & Blogs\n",
       "3  BLIND(folded) CAKE DECORATING CONTEST (with Mo...  People & Blogs\n",
       "4      Wearing Online Dollar Store Makeup For A Week  People & Blogs"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For my convinience of understanding I am representing the Feature set and Label set into a single DataFrame. \n",
    "corpus_df = pd.DataFrame({'Document': corpus, \n",
    "                          'Category': labels})\n",
    "corpus_df = corpus_df[['Document', 'Category']]\n",
    "corpus_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Following is a python function that removes unwanted characters in order to extract the theme of the textual data. Next we shall pass our textual data to this function in order to extract the theme.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['want talk marriage', 'meo cats commercial',\n",
       "       'affairs ex boyfriends million net worth google us shawn andrew',\n",
       "       ..., 'game zones se isle van gundy',\n",
       "       'game zones se isle van gundy', 'game zones se isle van gundy'],\n",
       "      dtype='<U92')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)\n",
    "\n",
    "text = normalize_corpus(corpus)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The above represents the tokenized words or themes in array format, that have been extracted from textual description. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing the tokenized words from array into Sparse Matrix format so that it can be fit into Classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing the tokenized words from array into Sparse Matrix format so that it can be fit into Classification model\n",
    "vectorizer = CountVectorizer(min_df=0)\n",
    "X = vectorizer.fit_transform(text)\n",
    "X = X.tocsc()  # some versions of sklearn return COO format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the shape of the Sparse Matrix to ensure we have not lost any data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40291, 10063)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the shape of the Sparse Matrix to ensure we have not lost any data\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Converting the Sparse Matrix into DataFrame so that we can combine it with video statistics feature set in order to build our Final Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the Sparse Matrix into DataFrame so that we can combine it with video statistics feature set in \n",
    "# order to build our Final Training Dataset\n",
    "A=pd.DataFrame(X.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating Presprocessed Textual data with video statistics feature set in order to build our Final Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating Presprocessed Textual data with video statistics feature set in order to build our Final Training Dataset\n",
    "U = pd.concat([Statistics.reset_index(drop=True), A.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recheking the shape of our Training Data in order to confirm after concatenation no data has been lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40291, 10064)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Label column into a format that is suitable for fitting into our Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the Label column into a format that is suitable for fitting into our Classification Model\n",
    "mymap = {'People & Blogs':1, 'Entertainment':2, 'Comedy':3, 'Science & Technology':4, 'Film & Animation':5,'News & Politics':6,'Sports':7,'Music':8,'Pets & Animals':9,'Education':10,'Howto & Style':11,'Autos & Vehicles':12,'Travel & Events':13,'Gaming':14,'Nonprofits & Activism':15,'Shows':16}\n",
    "\n",
    "label=df[['title']].applymap(lambda s: mymap.get(s) if s in mymap else s)\n",
    "\n",
    "y = label.values.astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying MultinomialNB Machine Learning Algorithms:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I have randomly chosen a set of Alpha values that I shall fit into my model and then see which of the values yield the highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#I have randomly chosen a set of Alpha values that I shall fit into my model and \n",
    "#then see which of the values yield the highest accuracy  \n",
    "#the grid of parameters to search over\n",
    "alphas = [.1, 1, 5, 10, 50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the Dataset into Training and Testing set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the Dataset into Training and Testing set\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(U,y,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.8385175380542687\n",
      "\n",
      "Alpha:  1\n",
      "Score:  0.689692256783587\n",
      "\n",
      "Alpha:  5\n",
      "Score:  0.4737756452680344\n",
      "\n",
      "Alpha:  10\n",
      "Score:  0.3834381204500331\n",
      "\n",
      "Alpha:  50\n",
      "Score:  0.21757114493712773\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define train_and_predict()\n",
    "def train_and_predict(alpha):\n",
    "    # Instantiate the classifier: nb_classifier\n",
    "    nb_classifier = MultinomialNB(alpha=alpha)\n",
    "    # Fit to the training data\n",
    "    nb_classifier.fit(X_train,Y_train)\n",
    "    # Predict the labels: pred\n",
    "    pred = nb_classifier.predict(X_test)\n",
    "    # Compute accuracy: score\n",
    "    score = metrics.accuracy_score(Y_test,pred)\n",
    "    return score\n",
    "\n",
    "# Iterate over the alphas and print the corresponding score\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    print('Score: ', train_and_predict(alpha))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Our model displays best accuracy of 83% at Alpha = 0.1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the accuracy score when only the tokenized words are used as feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  0.1\n",
      "Score:  0.9268696227663799\n",
      "\n",
      "Alpha:  1\n",
      "Score:  0.8899735274652548\n",
      "\n",
      "Alpha:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.7897088021178028\n",
      "\n",
      "Alpha:  10\n",
      "Score:  0.7217902051621443\n",
      "\n",
      "Alpha:  50\n",
      "Score:  0.5210953011250827\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Splitting the Dataset into Training and Testing set\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,y,test_size=0.3,random_state=42)\n",
    "\n",
    "# Define train_and_predict()\n",
    "def train_and_predict(alpha):\n",
    "    # Instantiate the classifier: nb_classifier\n",
    "    nb_classifier = MultinomialNB(alpha=alpha)\n",
    "    # Fit to the training data\n",
    "    nb_classifier.fit(X_train,Y_train)\n",
    "    # Predict the labels: pred\n",
    "    pred = nb_classifier.predict(X_test)\n",
    "    # Compute accuracy: score\n",
    "    score = metrics.accuracy_score(Y_test,pred)\n",
    "    return score\n",
    "\n",
    "# Iterate over the alphas and print the corresponding score\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    print('Score: ', train_and_predict(alpha))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### This particular model where no numerical features have not been utilized displays its best accuracy of 92% when Alpha = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Story:\n",
    "##### Continuous Numerical features such as Views, Likes, Dislikes, Comment Count are not very helpful features to classify videos based on their themes. This also proves another point that it is not necessary that particular categories of videos always get very high views. Irrespective of its theme the video may get higher views based on how well it appeals to its target audience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
